{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from src.network.network_features import cal_betweenness_centrality\n",
    "import igraph as ig\n",
    "import json\n",
    "from src.data.dataset_info import datasets\n",
    "from src.network.network_features import separate_graph, cal_betweenness_centrality, cal_k_core, cal_k_truss\n",
    "from src.network.CommCentralityCode import comm_centreality\n",
    "from src.network.modularity_vitality import modularity_vitality\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "with_sort_timestamp = False\n",
    "undersample_classes = True\n",
    "folder_path = \"fl_from_2_datasets/\"\n",
    "folder_path_prep = \"fl_from_2_datasets/preprocessed/\"\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"./datasets/cic_ton_iot.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.weak_columns:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'mitm' 'scanning' 'dos' 'ddos' 'injection' 'password' 'backdoor'\n",
      " 'ransomware' 'xss']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854b4573-67e2-4691-be7a-3926173792c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset2.name: cic_ids_2017\n"
     ]
    }
   ],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"./datasets/cic_ids_2017.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.weak_columns:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'ddos' 'PortScan' 'Bot' 'Infiltration' 'bruteforce' 'xss'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'DoS slowloris', 'DoS Hulk', 'Infiltration', 'dos', 'injection', 'ransomware', 'SSH-Patator', 'Benign', 'Bot', 'DoS GoldenEye', 'password', 'ddos', 'DoS Slowhttptest', 'PortScan', 'Heartbleed', 'FTP-Patator', 'scanning', 'Web Attack � Sql Injection', 'bruteforce', 'mitm', 'xss', 'backdoor'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'Heartbleed', 8: 'Infiltration', 9: 'PortScan', 10: 'SSH-Patator', 11: 'Web Attack � Sql Injection', 12: 'backdoor', 13: 'bruteforce', 14: 'ddos', 15: 'dos', 16: 'injection', 17: 'mitm', 18: 'password', 19: 'ransomware', 20: 'scanning', 21: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        2514059\n",
      "xss           2149308\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: ddos\n",
      "==>> class_label: dos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        1257030\n",
      "xss           1074654\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        2265910\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        1132955\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> number_of_nodes: 109166\n",
      "==>> number_of_edges: 209615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 3018900,\n",
       " 'num_benign': 1257030,\n",
       " 'percentage_of_benign_records': 41.6386763390639,\n",
       " 'num_attack': 1761870,\n",
       " 'percentage_of_attack_records': 58.3613236609361,\n",
       " 'attacks': ['xss',\n",
       "  'Benign',\n",
       "  'injection',\n",
       "  'backdoor',\n",
       "  'password',\n",
       "  'ransomware',\n",
       "  'scanning',\n",
       "  'ddos',\n",
       "  'mitm',\n",
       "  'dos']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> number_of_nodes: 18405\n",
      "==>> number_of_edges: 93329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 1681923,\n",
       " 'num_benign': 1132955,\n",
       " 'percentage_of_benign_records': 67.36069368217213,\n",
       " 'num_attack': 548968,\n",
       " 'percentage_of_attack_records': 32.63930631782787,\n",
       " 'attacks': ['Benign',\n",
       "  'ddos',\n",
       "  'PortScan',\n",
       "  'DoS Hulk',\n",
       "  'DoS slowloris',\n",
       "  'DoS Slowhttptest',\n",
       "  'Bot',\n",
       "  'DoS GoldenEye',\n",
       "  'FTP-Patator',\n",
       "  'SSH-Patator',\n",
       "  'bruteforce',\n",
       "  'xss',\n",
       "  'Web Attack � Sql Injection',\n",
       "  'Infiltration',\n",
       "  'Heartbleed']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "092beef1-914d-4a3a-8093-a57411c3a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_properties(G, name):\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['label'] = node\n",
    "    G1 = ig.Graph.from_networkx(G)\n",
    "    \n",
    "    labels = [G.nodes[node]['label'] for node in G.nodes()]\n",
    "    G1.vs['label'] = labels\n",
    "    \n",
    "    part = G1.community_infomap()\n",
    "    \n",
    "    communities = []\n",
    "    for com in part:\n",
    "        communities.append([G1.vs[node_index]['label'] for node_index in com])\n",
    "    \n",
    "    properties = {}\n",
    "    properties[\"number_of_nodes\"] = G.number_of_nodes()\n",
    "    properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "    \n",
    "    degrees = [degree for _, degree in G.degree()]\n",
    "    properties[\"max_degree\"] = max(degrees)\n",
    "    properties[\"avg_degree\"] = sum(degrees) / len(degrees)\n",
    "    properties[\"transitivity\"] = nx.transitivity(G)\n",
    "    properties[\"density\"] = nx.density(G)\n",
    "    \n",
    "    node_to_community = {}\n",
    "    for community_index, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            node_to_community[node] = community_index\n",
    "    \n",
    "    inter_cluster_edges = 0\n",
    "    for u, v in G.edges():\n",
    "        if node_to_community[u] != node_to_community[v]:\n",
    "            inter_cluster_edges += 1\n",
    "    \n",
    "    properties[\"mixing_parameter\"] = inter_cluster_edges / G.number_of_edges()\n",
    "    \n",
    "    filename = os.path.join('fl_from_2_datasets', 'datasets_properties', f\"graph_{name}.json\")\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(properties, outfile)\n",
    "    \n",
    "    return properties\n",
    "\n",
    "def process_and_compute_graph_properties(train1, train2, test1, test2, folder_path):\n",
    "    client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "    \n",
    "    for cid, data_partition in enumerate(client_data):\n",
    "        data_partition.to_parquet(folder_path + \"client_{}.parquet\".format(cid))\n",
    "        \n",
    "        G = nx.from_pandas_edgelist(data_partition, source='Src IP', target='Dst IP', create_using=nx.Graph())\n",
    "        properties = compute_graph_properties(G, f\"client_{cid}\")\n",
    "        print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "    \n",
    "    test = pd.concat([test1, test2])\n",
    "    test.to_parquet(folder_path + \"test.parquet\")\n",
    "    \n",
    "    G_test = nx.from_pandas_edgelist(test, source='Src IP', target='Dst IP', create_using=nx.Graph())\n",
    "    test_properties = compute_graph_properties(G_test, \"test\")\n",
    "    print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f63ffd39-399a-4ae2-bcd2-4be692bc8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed properties for client_0: {'number_of_nodes': 37771, 'number_of_edges': 44465, 'max_degree': 9884, 'avg_degree': 2.3544518281221043, 'transitivity': 6.354718836380373e-06, 'density': 6.233655885946795e-05, 'mixing_parameter': 0.15236703024851006}\n",
      "Computed properties for client_1: {'number_of_nodes': 37930, 'number_of_edges': 44742, 'max_degree': 9983, 'avg_degree': 2.3591879778539413, 'transitivity': 3.659587582103573e-05, 'density': 6.220011014933011e-05, 'mixing_parameter': 0.15408341155960842}\n",
      "Computed properties for client_2: {'number_of_nodes': 37865, 'number_of_edges': 44548, 'max_degree': 9947, 'avg_degree': 2.3529908886834807, 'transitivity': 2.9297702829502443e-05, 'density': 6.214322017439998e-05, 'mixing_parameter': 0.15192601239112866}\n",
      "Computed properties for client_3: {'number_of_nodes': 37825, 'number_of_edges': 44617, 'max_degree': 9984, 'avg_degree': 2.3591275611368143, 'transitivity': 8.459053200490562e-06, 'density': 6.237118129063066e-05, 'mixing_parameter': 0.15399959656633122}\n",
      "Computed properties for client_4: {'number_of_nodes': 37813, 'number_of_edges': 44429, 'max_degree': 9797, 'avg_degree': 2.3499325628752015, 'transitivity': 3.938911646850556e-06, 'density': 6.21477986585e-05, 'mixing_parameter': 0.15059983344212113}\n",
      "Computed properties for client_5: {'number_of_nodes': 14868, 'number_of_edges': 39433, 'max_degree': 4279, 'avg_degree': 5.304412160344364, 'transitivity': 0.0010324553807150771, 'density': 0.0003567910244396559, 'mixing_parameter': 0.0016737250526209014}\n",
      "Computed properties for client_6: {'number_of_nodes': 14730, 'number_of_edges': 39347, 'max_degree': 4313, 'avg_degree': 5.342430414120842, 'transitivity': 0.000985822721883367, 'density': 0.00036271508005437177, 'mixing_parameter': 0.0015248938927999594}\n",
      "Computed properties for client_7: {'number_of_nodes': 14865, 'number_of_edges': 39483, 'max_degree': 4359, 'avg_degree': 5.312209889001009, 'transitivity': 0.0009227436853296424, 'density': 0.00035738764054097207, 'mixing_parameter': 0.001696932857179039}\n",
      "Computed properties for test dataset: {'number_of_nodes': 33150, 'number_of_edges': 47450, 'max_degree': 5554, 'avg_degree': 2.8627450980392157, 'transitivity': 0.0005802227906806432, 'density': 8.635992331712014e-05, 'mixing_parameter': 0.07610115911485775}\n"
     ]
    }
   ],
   "source": [
    "process_and_compute_graph_properties(train1, train2, test1, test2, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed61782e-62b6-4048-823a-6902168e59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(name, path, dataset, cn_measures):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(\"fl_from_2_datasets\", \"preprocessed\", \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(\"fl_from_2_datasets\", \"preprocessed\",\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(axis=0, how='any', inplace=True)\n",
    "    df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(df, source=dataset.src_ip_col, target=dataset.dst_ip_col, create_using=nx.DiGraph())\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['label'] = node\n",
    "    \n",
    "    G1 = ig.Graph.from_networkx(G)\n",
    "    labels = [G.nodes[node]['label'] for node in G.nodes()]\n",
    "    G1.vs['label'] = labels\n",
    "    \n",
    "    part = G1.community_infomap()\n",
    "    communities = []\n",
    "    for com in part:\n",
    "        communities.append([G1.vs[node_index]['label'] for node_index in com])\n",
    "    \n",
    "    community_labels = {}\n",
    "    for i, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            community_labels[node] = i\n",
    "    \n",
    "    nx.set_node_attributes(G, community_labels, \"new_community\")\n",
    "    \n",
    "    intra_graph, inter_graph = separate_graph(G, communities)\n",
    "    \n",
    "    if \"betweenness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, cal_betweenness_centrality(G), \"betweenness\")\n",
    "        print(\"calculated betweenness\")\n",
    "    if \"local_betweenness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, cal_betweenness_centrality(intra_graph), \"local_betweenness\")\n",
    "        print(\"calculated local_betweenness\")\n",
    "    if \"global_betweenness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, cal_betweenness_centrality(inter_graph), \"global_betweenness\")\n",
    "        print(\"calculated global_betweenness\")\n",
    "    if \"degree\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.degree_centrality(G), \"degree\")\n",
    "        print(\"calculated degree\")\n",
    "    if \"local_degree\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.degree_centrality(intra_graph), \"local_degree\")\n",
    "        print(\"calculated local_degree\")\n",
    "    if \"global_degree\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.degree_centrality(inter_graph), \"global_degree\")\n",
    "        print(\"calculated global_degree\")\n",
    "    if \"eigenvector\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.eigenvector_centrality(G, max_iter=600), \"eigenvector\")\n",
    "        print(\"calculated eigenvector\")\n",
    "    if \"local_eigenvector\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.eigenvector_centrality(intra_graph), \"local_eigenvector\")\n",
    "        print(\"calculated local_eigenvector\")\n",
    "    if \"global_eigenvector\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.eigenvector_centrality(inter_graph), \"global_eigenvector\")\n",
    "        print(\"calculated global_eigenvector\")\n",
    "    if \"closeness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.closeness_centrality(G), \"closeness\")\n",
    "        print(\"calculated closeness\")\n",
    "    if \"local_closeness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.closeness_centrality(intra_graph), \"local_closeness\")\n",
    "        print(\"calculated local_closeness\")\n",
    "    if \"global_closeness\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.closeness_centrality(inter_graph), \"global_closeness\")\n",
    "        print(\"calculated global_closeness\")\n",
    "    if \"pagerank\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.pagerank(G, alpha=0.85), \"pagerank\")\n",
    "        print(\"calculated pagerank\")\n",
    "    if \"local_pagerank\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.pagerank(intra_graph, alpha=0.85), \"local_pagerank\")\n",
    "        print(\"calculated local_pagerank\")\n",
    "    if \"global_pagerank\" in cn_measures:\n",
    "        nx.set_node_attributes(G, nx.pagerank(inter_graph, alpha=0.85), \"global_pagerank\")\n",
    "        print(\"calculated global_pagerank\")\n",
    "    if \"k_core\" in cn_measures:\n",
    "        nx.set_node_attributes(G, cal_k_core(G), \"k_core\")\n",
    "        print(\"calculated k_core\")\n",
    "    if \"k_truss\" in cn_measures:\n",
    "        nx.set_node_attributes(G, cal_k_truss(G), \"k_truss\")\n",
    "        print(\"calculated k_truss\")\n",
    "    if \"Comm\" in cn_measures:\n",
    "        nx.set_node_attributes(G, comm_centreality(G, community_labels), \"Comm\")\n",
    "        print(\"calculated Comm\")\n",
    "    if \"mv\" in cn_measures:\n",
    "        nx.set_node_attributes(G, modularity_vitality(G1, part), \"mv\")\n",
    "        print(\"calculated mv\")\n",
    "    \n",
    "    nx.write_gexf(G, graph_path)\n",
    "    \n",
    "    features_dicts = {}\n",
    "    for measure in cn_measures:\n",
    "        features_dicts[measure] = nx.get_node_attributes(G, measure)\n",
    "        print(f\"==>> features_dicts: {measure , len(features_dicts[measure])}\")\n",
    "    \n",
    "    for feature in dataset.network_features:\n",
    "        if feature[:3] == \"src\":\n",
    "            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(row[dataset.src_ip_col], -1), axis=1)\n",
    "        if feature[:3] == \"dst\":\n",
    "            df[feature] = df.apply(lambda row: features_dicts[feature[4:]].get(row[dataset.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df.to_parquet(new_path)\n",
    "    print(f\"DataFrame written to {new_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fed61f5-1abe-4226-b02d-971662c83243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: client_0\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 37771)\n",
      "==>> features_dicts: ('global_betweenness', 37771)\n",
      "==>> features_dicts: ('degree', 37771)\n",
      "==>> features_dicts: ('global_degree', 37771)\n",
      "==>> features_dicts: ('eigenvector', 37771)\n",
      "==>> features_dicts: ('closeness', 37771)\n",
      "==>> features_dicts: ('pagerank', 37771)\n",
      "==>> features_dicts: ('global_pagerank', 37771)\n",
      "==>> features_dicts: ('k_core', 37771)\n",
      "==>> features_dicts: ('k_truss', 37771)\n",
      "==>> features_dicts: ('mv', 37771)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_0.parquet\n",
      "Processing dataset: client_1\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 37930)\n",
      "==>> features_dicts: ('global_betweenness', 37930)\n",
      "==>> features_dicts: ('degree', 37930)\n",
      "==>> features_dicts: ('global_degree', 37930)\n",
      "==>> features_dicts: ('eigenvector', 37930)\n",
      "==>> features_dicts: ('closeness', 37930)\n",
      "==>> features_dicts: ('pagerank', 37930)\n",
      "==>> features_dicts: ('global_pagerank', 37930)\n",
      "==>> features_dicts: ('k_core', 37930)\n",
      "==>> features_dicts: ('k_truss', 37930)\n",
      "==>> features_dicts: ('mv', 37930)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_1.parquet\n",
      "Processing dataset: client_2\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 37865)\n",
      "==>> features_dicts: ('global_betweenness', 37865)\n",
      "==>> features_dicts: ('degree', 37865)\n",
      "==>> features_dicts: ('global_degree', 37865)\n",
      "==>> features_dicts: ('eigenvector', 37865)\n",
      "==>> features_dicts: ('closeness', 37865)\n",
      "==>> features_dicts: ('pagerank', 37865)\n",
      "==>> features_dicts: ('global_pagerank', 37865)\n",
      "==>> features_dicts: ('k_core', 37865)\n",
      "==>> features_dicts: ('k_truss', 37865)\n",
      "==>> features_dicts: ('mv', 37865)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_2.parquet\n",
      "Processing dataset: client_3\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 37825)\n",
      "==>> features_dicts: ('global_betweenness', 37825)\n",
      "==>> features_dicts: ('degree', 37825)\n",
      "==>> features_dicts: ('global_degree', 37825)\n",
      "==>> features_dicts: ('eigenvector', 37825)\n",
      "==>> features_dicts: ('closeness', 37825)\n",
      "==>> features_dicts: ('pagerank', 37825)\n",
      "==>> features_dicts: ('global_pagerank', 37825)\n",
      "==>> features_dicts: ('k_core', 37825)\n",
      "==>> features_dicts: ('k_truss', 37825)\n",
      "==>> features_dicts: ('mv', 37825)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_3.parquet\n",
      "Processing dataset: client_4\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 37813)\n",
      "==>> features_dicts: ('global_betweenness', 37813)\n",
      "==>> features_dicts: ('degree', 37813)\n",
      "==>> features_dicts: ('global_degree', 37813)\n",
      "==>> features_dicts: ('eigenvector', 37813)\n",
      "==>> features_dicts: ('closeness', 37813)\n",
      "==>> features_dicts: ('pagerank', 37813)\n",
      "==>> features_dicts: ('global_pagerank', 37813)\n",
      "==>> features_dicts: ('k_core', 37813)\n",
      "==>> features_dicts: ('k_truss', 37813)\n",
      "==>> features_dicts: ('mv', 37813)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_4.parquet\n",
      "Processing dataset: client_5\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 14868)\n",
      "==>> features_dicts: ('local_betweenness', 14868)\n",
      "==>> features_dicts: ('degree', 14868)\n",
      "==>> features_dicts: ('local_degree', 14868)\n",
      "==>> features_dicts: ('eigenvector', 14868)\n",
      "==>> features_dicts: ('closeness', 14868)\n",
      "==>> features_dicts: ('pagerank', 14868)\n",
      "==>> features_dicts: ('local_pagerank', 14868)\n",
      "==>> features_dicts: ('k_core', 14868)\n",
      "==>> features_dicts: ('k_truss', 14868)\n",
      "==>> features_dicts: ('Comm', 14868)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_5.parquet\n",
      "Processing dataset: client_6\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 14730)\n",
      "==>> features_dicts: ('local_betweenness', 14730)\n",
      "==>> features_dicts: ('degree', 14730)\n",
      "==>> features_dicts: ('local_degree', 14730)\n",
      "==>> features_dicts: ('eigenvector', 14730)\n",
      "==>> features_dicts: ('closeness', 14730)\n",
      "==>> features_dicts: ('pagerank', 14730)\n",
      "==>> features_dicts: ('local_pagerank', 14730)\n",
      "==>> features_dicts: ('k_core', 14730)\n",
      "==>> features_dicts: ('k_truss', 14730)\n",
      "==>> features_dicts: ('Comm', 14730)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_6.parquet\n",
      "Processing dataset: client_7\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 14865)\n",
      "==>> features_dicts: ('local_betweenness', 14865)\n",
      "==>> features_dicts: ('degree', 14865)\n",
      "==>> features_dicts: ('local_degree', 14865)\n",
      "==>> features_dicts: ('eigenvector', 14865)\n",
      "==>> features_dicts: ('closeness', 14865)\n",
      "==>> features_dicts: ('pagerank', 14865)\n",
      "==>> features_dicts: ('local_pagerank', 14865)\n",
      "==>> features_dicts: ('k_core', 14865)\n",
      "==>> features_dicts: ('k_truss', 14865)\n",
      "==>> features_dicts: ('Comm', 14865)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\client_7.parquet\n",
      "Processing dataset: test\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 33150)\n",
      "==>> features_dicts: ('local_betweenness', 33150)\n",
      "==>> features_dicts: ('degree', 33150)\n",
      "==>> features_dicts: ('local_degree', 33150)\n",
      "==>> features_dicts: ('eigenvector', 33150)\n",
      "==>> features_dicts: ('closeness', 33150)\n",
      "==>> features_dicts: ('pagerank', 33150)\n",
      "==>> features_dicts: ('local_pagerank', 33150)\n",
      "==>> features_dicts: ('k_core', 33150)\n",
      "==>> features_dicts: ('k_truss', 33150)\n",
      "==>> features_dicts: ('Comm', 33150)\n",
      "DataFrame written to fl_from_2_datasets\\preprocessed\\test.parquet\n"
     ]
    }
   ],
   "source": [
    "def process_all_clients(clients_paths, folder_path_prep):\n",
    "    for i, path in enumerate(clients_paths):\n",
    "        name = f\"client_{i}\" if i < len(clients_paths) - 1 else \"test\"\n",
    "        dataset = datasets[0] if i < 5 else datasets[1]\n",
    "        process_dataset(name, clients_paths[i], dataset, dataset.cn_measures)\n",
    "\n",
    "\n",
    "client_names = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_names]\n",
    "\n",
    "\n",
    "process_all_clients(clients_paths, folder_path_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b271faf2-c3b0-47d6-8288-04d60c3ed90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clients_with_pca(clients_paths, cn_measures_type_1, cn_measures_type_2, output_folder):\n",
    "    def calculate_local_covariance(df, cn_measures):\n",
    "        centrality_data = df[cn_measures].fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        centrality_data_std = scaler.fit_transform(centrality_data)\n",
    "        covariance_matrix = np.cov(centrality_data_std, rowvar=False)\n",
    "        return covariance_matrix, scaler.mean_, scaler.scale_\n",
    "\n",
    "    def apply_local_pca(df, cn_measures, n_components=2):\n",
    "        centrality_data = df[cn_measures].fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        centrality_data_std = scaler.fit_transform(centrality_data)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        centrality_data_pca = pca.fit_transform(centrality_data_std)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        return explained_variance\n",
    "\n",
    "    def apply_global_pca(df, cn_measures, mean, scale, principal_components):\n",
    "        centrality_data = df[cn_measures].fillna(0)\n",
    "        centrality_data_std = (centrality_data - mean) / scale\n",
    "        centrality_data_pca = np.dot(centrality_data_std, principal_components)\n",
    "        pca_columns = [f'pca_{i+1}' for i in range(principal_components.shape[1])]\n",
    "        centrality_data_pca_df = pd.DataFrame(centrality_data_pca, columns=pca_columns, index=df.index)\n",
    "        return centrality_data_pca_df\n",
    "\n",
    "    local_covariances = []\n",
    "    local_means = []\n",
    "    local_scales = []\n",
    "    local_explained_variances = []\n",
    "\n",
    "    for i, client_path in enumerate(clients_paths):\n",
    "        df = pd.read_parquet(client_path)\n",
    "        cn_measures = cn_measures_type_2 if i < 5 else cn_measures_type_1\n",
    "        client_cn_measures = [measure for measure in cn_measures if measure in df.columns]\n",
    "        if len(client_cn_measures) == 0:\n",
    "            print(f\"No centrality measures found in client {i} data.\")\n",
    "            continue\n",
    "        covariance_matrix, mean, scale = calculate_local_covariance(df, client_cn_measures)\n",
    "        local_covariances.append(covariance_matrix)\n",
    "        local_means.append(mean)\n",
    "        local_scales.append(scale)\n",
    "        explained_variance = apply_local_pca(df, client_cn_measures)\n",
    "        local_explained_variances.append(explained_variance)\n",
    "\n",
    "    global_covariance_matrix = np.mean(local_covariances, axis=0)\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(global_covariance_matrix)\n",
    "    sorted_indices = np.argsort(eigen_values)[::-1]\n",
    "    eigen_values = eigen_values[sorted_indices]\n",
    "    eigen_vectors = eigen_vectors[:, sorted_indices]\n",
    "    n_components = 2\n",
    "    principal_components = eigen_vectors[:, :n_components]\n",
    "    explained_variances = eigen_values[:n_components] / np.sum(eigen_values)\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    global_explained_variances = []\n",
    "\n",
    "    for i, client_path in enumerate(clients_paths):\n",
    "        df = pd.read_parquet(client_path)\n",
    "        cn_measures = cn_measures_type_2 if i < 5 else cn_measures_type_1\n",
    "        client_cn_measures = [measure for measure in cn_measures if measure in df.columns]\n",
    "        if len(client_cn_measures) == 0:\n",
    "            print(f\"No centrality measures found in client {i} data.\")\n",
    "            continue\n",
    "        centrality_data_pca_df = apply_global_pca(df, client_cn_measures, local_means[i], local_scales[i], principal_components)\n",
    "        df_pca = pd.concat([df.drop(columns=client_cn_measures), centrality_data_pca_df], axis=1)\n",
    "        if i < len(clients_paths) - 1:\n",
    "            output_path = os.path.join(output_folder, f'client_{i}_pca.parquet')\n",
    "        else:\n",
    "            output_path = os.path.join(output_folder, 'test_pca.parquet')\n",
    "        df_pca.to_parquet(output_path)\n",
    "        print(f'Processed PCA for {\"client\" if i < len(clients_paths) - 1 else \"test\"} {i}, saved to {output_path}')\n",
    "        \n",
    "        centrality_data_std = (df[client_cn_measures].fillna(0) - local_means[i]) / local_scales[i]\n",
    "        global_pca_transformed = np.dot(centrality_data_std, principal_components)\n",
    "        global_explained_variance = np.var(global_pca_transformed, axis=0) / np.var(centrality_data_std, axis=0).sum()\n",
    "        global_explained_variances.append(global_explained_variance)\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    for i, explained_variance in enumerate(local_explained_variances):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Local PCA')\n",
    "        plt.bar(range(1, len(global_explained_variances[i]) + 1), global_explained_variances[i], alpha=0.7, label='Global PCA')\n",
    "        plt.title(f'Client {i} Explained Variance')\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.ylabel('Variance Explained')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ff46b0d-326b-4eb9-9125-bde81b5d7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_clients_with_grouped_pca(feature_groups, output_folder, n_components=2):\n",
    "    def calculate_local_pca(df, cn_measures, n_components=2):\n",
    "        centrality_data = df[cn_measures].fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        centrality_data_std = scaler.fit_transform(centrality_data)\n",
    "        pca = PCA(n_components=n_components)\n",
    "        centrality_data_pca = pca.fit_transform(centrality_data_std)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        return centrality_data_pca, explained_variance, scaler.mean_, scaler.scale_, pca.components_\n",
    "\n",
    "    def calculate_local_covariance(pca_results):\n",
    "        return np.cov(pca_results, rowvar=False)\n",
    "\n",
    "    def apply_global_pca(local_covariances):\n",
    "        global_covariance_matrix = np.mean(local_covariances, axis=0)\n",
    "        eigen_values, eigen_vectors = np.linalg.eigh(global_covariance_matrix)\n",
    "        sorted_indices = np.argsort(eigen_values)[::-1]\n",
    "        global_principal_components = eigen_vectors[:, sorted_indices][:, :n_components]\n",
    "        return global_principal_components\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    all_local_pca_results = []  \n",
    "    local_covariances = []\n",
    "    client_dfs = {}  \n",
    "    local_explained_variances = {}  \n",
    "\n",
    "    reconstruction_errors_local = {}\n",
    "    reconstruction_errors_federated = {}\n",
    "\n",
    "    for group_id, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "        for client_path in clients:\n",
    "            df = pd.read_parquet(client_path)\n",
    "            client_cn_measures = [measure for measure in unique_feature_set if measure in df.columns]\n",
    "            if len(client_cn_measures) == 0:\n",
    "                print(f\"No centrality measures found in client {client_path}.\")\n",
    "                continue\n",
    "            \n",
    "            centrality_data_pca, explained_variance, mean, scale, pca_components = calculate_local_pca(df, client_cn_measures, n_components)\n",
    "            all_local_pca_results.append(centrality_data_pca)\n",
    "            \n",
    "            local_explained_variances[client_path] = explained_variance\n",
    "            \n",
    "            local_covariance_matrix = calculate_local_covariance(centrality_data_pca)\n",
    "            local_covariances.append(local_covariance_matrix)\n",
    "            \n",
    "            pca_columns = [f'pca_{i+1}' for i in range(n_components)]\n",
    "            local_pca_df = pd.DataFrame(centrality_data_pca, columns=pca_columns, index=df.index)\n",
    "            df = pd.concat([df.drop(columns=client_cn_measures), local_pca_df], axis=1)\n",
    "            \n",
    "            client_dfs[client_path] = df\n",
    "\n",
    "    global_principal_components = apply_global_pca(local_covariances)\n",
    "\n",
    "    global_explained_variances = {}  \n",
    "\n",
    "    for client_path, df in client_dfs.items():\n",
    "        local_pca_data = df[[f'pca_{i+1}' for i in range(n_components)]].values\n",
    "        \n",
    "        global_pca_transformed = np.dot(local_pca_data, global_principal_components)\n",
    "        pca_columns = [f'global_pca_{j+1}' for j in range(n_components)]\n",
    "        global_pca_df = pd.DataFrame(global_pca_transformed, columns=pca_columns, index=df.index)\n",
    "        \n",
    "        explained_variance = np.var(global_pca_df.values, axis=0)\n",
    "        explained_variance /= np.sum(explained_variance)\n",
    "        global_explained_variances[client_path] = explained_variance\n",
    "        \n",
    "        final_df = pd.concat([df.drop(columns=[f'pca_{i+1}' for i in range(n_components)]), global_pca_df], axis=1)\n",
    "        \n",
    "        output_path = os.path.join(output_folder, f'{os.path.basename(client_path).split(\".\")[0]}_global_pca.parquet')\n",
    "        final_df.to_parquet(output_path)\n",
    "        print(f'Processed federated PCA for client {client_path}, saved to {output_path}')\n",
    "\n",
    "        client_dfs[client_path] = final_df\n",
    "\n",
    "        local_reconstructed = np.dot(local_pca_data, global_principal_components.T)  \n",
    "        reconstruction_errors_local[client_path] = mean_squared_error(local_pca_data, local_reconstructed)\n",
    "        reconstruction_errors_federated[client_path] = mean_squared_error(local_pca_data, global_pca_transformed)\n",
    "\n",
    "        print(f\"Client {client_path} Local PCA Reconstruction Error: {reconstruction_errors_local[client_path]}\")\n",
    "        print(f\"Client {client_path} Federated PCA Reconstruction Error: {reconstruction_errors_federated[client_path]}\")\n",
    "\n",
    "    return {\n",
    "        'reconstruction_errors_local': reconstruction_errors_local,\n",
    "        'reconstruction_errors_federated': reconstruction_errors_federated,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "256e2ca1-fdf8-4107-bfd1-2ec709969b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pca_results(clients_paths):\n",
    "    for client_idx, client_path in enumerate(clients_paths):\n",
    "        df = pd.read_parquet(client_path)\n",
    "        \n",
    "        if 'global_pca_1' not in df.columns or 'global_pca_2' not in df.columns or 'Label' not in df.columns:\n",
    "            print(f\"Skipping client {client_idx} as required columns are missing.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(df['global_pca_1'], df['global_pca_2'], c=df['Label'], cmap='viridis', alpha=0.5)\n",
    "        plt.colorbar(scatter, label='Label')\n",
    "        plt.title(f'PCA Plot for Client {client_idx}')\n",
    "        plt.xlabel('1st Principal Component (global_pca_1)')\n",
    "        plt.ylabel('2nd Principal Component (global_pca_2)')\n",
    "        plt.show()\n",
    "\n",
    "        kmeans = KMeans(n_clusters=len(df['Label'].unique()))\n",
    "        kmeans.fit(df[['global_pca_1', 'global_pca_2']])\n",
    "        cluster_labels = kmeans.labels_\n",
    "        sil_score = silhouette_score(df[['global_pca_1', 'global_pca_2']], cluster_labels)\n",
    "        print(f\"Silhouette Score for client {client_idx}: {sil_score}\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[['global_pca_1', 'global_pca_2']], df['Label'], test_size=0.3, random_state=42)\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"Train/Test Split Classification performance for client {client_idx}:\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "        k_folds = 5\n",
    "        skf = StratifiedKFold(n_splits=k_folds)\n",
    "        cv_accuracy = cross_val_score(clf, df[['global_pca_1', 'global_pca_2']], df['Label'], cv=skf, scoring='accuracy')\n",
    "        cv_precision = cross_val_score(clf, df[['global_pca_1', 'global_pca_2']], df['Label'], cv=skf, scoring='precision_weighted')\n",
    "        cv_recall = cross_val_score(clf, df[['global_pca_1', 'global_pca_2']], df['Label'], cv=skf, scoring='recall_weighted')\n",
    "        cv_f1 = cross_val_score(clf, df[['global_pca_1', 'global_pca_2']], df['Label'], cv=skf, scoring='f1_weighted')\n",
    "\n",
    "        print(f\"Cross-Validation (k={k_folds}) performance for client {client_idx}:\")\n",
    "        print(f\"CV Accuracy: {np.mean(cv_accuracy)} (+/- {np.std(cv_accuracy)})\")\n",
    "        print(f\"CV Precision: {np.mean(cv_precision)} (+/- {np.std(cv_precision)})\")\n",
    "        print(f\"CV Recall: {np.mean(cv_recall)} (+/- {np.std(cv_recall)})\")\n",
    "        print(f\"CV F1 Score: {np.mean(cv_f1)} (+/- {np.std(cv_f1)})\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa3df23a-e842-4ee2-b8b0-55ae6f5d2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features Across All Clients: {'Fwd Pkt Len Min', 'Flow Pkts/s', 'src_k_truss', 'Tot Bwd Pkts', 'Bwd Pkt Len Mean', 'Protocol', 'Bwd IAT Min', 'Fwd IAT Mean', 'Subflow Bwd Byts', 'Bwd Pkt Len Min', 'Bwd Pkt Len Std', 'Fwd Pkt Len Max', 'SYN Flag Cnt', 'Active Std', 'TotLen Fwd Pkts', 'Bwd Byts/b Avg', 'Bwd Blk Rate Avg', 'Bwd Pkts/b Avg', 'src_eigenvector', 'Fwd Seg Size Min', 'Fwd Pkt Len Std', 'Fwd URG Flags', 'Pkt Size Avg', 'Bwd URG Flags', 'Active Min', 'src_betweenness', 'Bwd Seg Size Avg', 'Bwd Pkts/s', 'Fwd IAT Min', 'Fwd IAT Tot', 'dst_pagerank', 'src_k_core', 'Flow IAT Max', 'Fwd Byts/b Avg', 'ECE Flag Cnt', 'Bwd Pkt Len Max', 'Flow Byts/s', 'Down/Up Ratio', 'Label', 'Attack', 'Pkt Len Max', 'dst_eigenvector', 'Active Max', 'Flow IAT Min', 'Fwd Header Len', 'Bwd PSH Flags', 'Pkt Len Var', 'Idle Std', 'Pkt Len Min', 'Fwd Seg Size Avg', 'Fwd Pkt Len Mean', 'src_closeness', 'Fwd Blk Rate Avg', 'PSH Flag Cnt', 'dst_degree', 'Subflow Fwd Byts', 'Pkt Len Mean', 'dst_closeness', 'Bwd IAT Std', 'Src Port', 'Pkt Len Std', 'Fwd PSH Flags', 'Bwd IAT Tot', 'TotLen Bwd Pkts', 'Flow Duration', 'dst_k_core', 'Fwd Pkts/s', 'Subflow Bwd Pkts', 'Flow IAT Std', 'Fwd Pkts/b Avg', 'Idle Max', 'FIN Flag Cnt', 'Timestamp', 'dst_betweenness', 'Fwd Act Data Pkts', 'Subflow Fwd Pkts', 'Idle Mean', 'Bwd IAT Mean', 'ACK Flag Cnt', 'Init Fwd Win Byts', 'Src IP', 'Flow ID', 'Init Bwd Win Byts', 'src_degree', 'Bwd Header Len', 'Active Mean', 'Dst Port', 'Class', 'src_pagerank', 'RST Flag Cnt', 'URG Flag Cnt', 'dst_k_truss', 'Tot Fwd Pkts', 'CWE Flag Count', 'Idle Min', 'Bwd IAT Max', 'Fwd IAT Max', 'Flow IAT Mean', 'Dst IP'}\n",
      "\n",
      "Unique Feature Set Group 1:\n",
      "Unique Features: {'src_global_betweenness', 'dst_mv', 'src_global_pagerank', 'src_global_degree', 'dst_global_degree', 'src_mv', 'dst_global_pagerank'}\n",
      "Clients: ['fl_from_2_datasets/preprocessed/client_0.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 2:\n",
      "Unique Features: {'src_global_pagerank', 'dst_global_betweenness', 'src_global_betweenness', 'dst_mv', 'src_global_degree', 'dst_global_degree', 'src_mv', 'dst_global_pagerank'}\n",
      "Clients: ['fl_from_2_datasets/preprocessed/client_1.parquet', 'fl_from_2_datasets/preprocessed/client_2.parquet', 'fl_from_2_datasets/preprocessed/client_3.parquet', 'fl_from_2_datasets/preprocessed/client_4.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 3:\n",
      "Unique Features: {'src_local_pagerank', 'src_local_degree', 'dst_local_degree', 'dst_Comm', 'src_Comm', 'dst_local_pagerank', 'dst_local_betweenness', 'src_local_betweenness'}\n",
      "Clients: ['fl_from_2_datasets/preprocessed/client_5.parquet', 'fl_from_2_datasets/preprocessed/client_6.parquet', 'fl_from_2_datasets/preprocessed/client_7.parquet', 'fl_from_2_datasets/preprocessed/test.parquet']\n",
      "----------\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_0.parquet, saved to datasets_pca_federated_new\\client_0_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_0.parquet Local PCA Reconstruction Error: 10.666160904809274\n",
      "Client fl_from_2_datasets/preprocessed/client_0.parquet Federated PCA Reconstruction Error: 10.666160904809274\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_1.parquet, saved to datasets_pca_federated_new\\client_1_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_1.parquet Local PCA Reconstruction Error: 12.878182307008561\n",
      "Client fl_from_2_datasets/preprocessed/client_1.parquet Federated PCA Reconstruction Error: 12.878182307008561\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_2.parquet, saved to datasets_pca_federated_new\\client_2_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_2.parquet Local PCA Reconstruction Error: 11.585766169327734\n",
      "Client fl_from_2_datasets/preprocessed/client_2.parquet Federated PCA Reconstruction Error: 11.58576616932783\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_3.parquet, saved to datasets_pca_federated_new\\client_3_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_3.parquet Local PCA Reconstruction Error: 12.325707503935485\n",
      "Client fl_from_2_datasets/preprocessed/client_3.parquet Federated PCA Reconstruction Error: 12.325707503935481\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_4.parquet, saved to datasets_pca_federated_new\\client_4_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_4.parquet Local PCA Reconstruction Error: 12.787654699091133\n",
      "Client fl_from_2_datasets/preprocessed/client_4.parquet Federated PCA Reconstruction Error: 12.787654699091135\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_5.parquet, saved to datasets_pca_federated_new\\client_5_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_5.parquet Local PCA Reconstruction Error: 14.182815480471682\n",
      "Client fl_from_2_datasets/preprocessed/client_5.parquet Federated PCA Reconstruction Error: 14.182815480471696\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_6.parquet, saved to datasets_pca_federated_new\\client_6_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_6.parquet Local PCA Reconstruction Error: 14.788265894315836\n",
      "Client fl_from_2_datasets/preprocessed/client_6.parquet Federated PCA Reconstruction Error: 14.78826589431577\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/client_7.parquet, saved to datasets_pca_federated_new\\client_7_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/client_7.parquet Local PCA Reconstruction Error: 13.754880529670974\n",
      "Client fl_from_2_datasets/preprocessed/client_7.parquet Federated PCA Reconstruction Error: 13.754880529671054\n",
      "Processed federated PCA for client fl_from_2_datasets/preprocessed/test.parquet, saved to datasets_pca_federated_new\\test_global_pca.parquet\n",
      "Client fl_from_2_datasets/preprocessed/test.parquet Local PCA Reconstruction Error: 13.235280032693229\n",
      "Client fl_from_2_datasets/preprocessed/test.parquet Federated PCA Reconstruction Error: 13.235280032693144\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'folder_copy_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets_pca_federated_new\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m pca_results \u001b[38;5;241m=\u001b[39m process_clients_with_grouped_pca(feature_groups, output_folder, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_copy_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_names.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/labels_names.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     45\u001b[0m clients_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     46\u001b[0m     output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/client_0_global_pca.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     47\u001b[0m     output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/client_1_global_pca.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'folder_copy_file' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "folder_path = 'fl_from_2_datasets/preprocessed/'\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path + \"client_0.parquet\",\n",
    "    folder_path + \"client_1.parquet\",\n",
    "    folder_path + \"client_2.parquet\",\n",
    "    folder_path + \"client_3.parquet\",\n",
    "    folder_path + \"client_4.parquet\",\n",
    "    folder_path + \"client_5.parquet\",\n",
    "    folder_path + \"client_6.parquet\",\n",
    "    folder_path + \"client_7.parquet\",\n",
    "    folder_path + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    client_features[client_path] = features\n",
    "\n",
    "common_features = set.intersection(*client_features.values())\n",
    "print(f\"Common Features Across All Clients: {common_features}\\n\")\n",
    "\n",
    "unique_features = {client_path: features - common_features for client_path, features in client_features.items()}\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "\n",
    "for client_path, features in unique_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Feature Set Group {i}:\")\n",
    "    print(f\"Unique Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "\n",
    "output_folder = 'datasets_pca_federated_new'\n",
    "pca_results = process_clients_with_grouped_pca(feature_groups, output_folder, n_components=2)\n",
    "shutil.copy(os.path.join(folder_copy_file, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "clients_paths = [\n",
    "    output_folder + \"/client_0_global_pca.parquet\",\n",
    "    output_folder + \"/client_1_global_pca.parquet\",\n",
    "    output_folder + \"/client_2_global_pca.parquet\",\n",
    "    output_folder + \"/client_3_global_pca.parquet\",\n",
    "    output_folder + \"/client_4_global_pca.parquet\",\n",
    "    output_folder + \"/client_5_global_pca.parquet\",\n",
    "    output_folder + \"/client_6_global_pca.parquet\",\n",
    "    output_folder + \"/client_7_global_pca.parquet\",\n",
    "    output_folder + \"/test_global_pca.parquet\"\n",
    "\n",
    "]\n",
    "\n",
    "#evaluate_pca_results(clients_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95db9cf5-9ce2-4687-b430-b39d07f6a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'dst_global_betweenness' dropped from fl_from_2_datasets/preprocessed/client_0.parquet.\n",
      "File fl_from_2_datasets/preprocessed/client_0.parquet has been updated and rewritten without 'dst_global_betweenness'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'fl_from_2_datasets/preprocessed/client_0.parquet'\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# Drop the 'dst_global_betweenness' column\n",
    "if 'dst_global_betweenness' in df.columns:\n",
    "    df = df.drop(columns=['dst_global_betweenness'])\n",
    "    print(f\"Column 'dst_global_betweenness' dropped from {file_path}.\")\n",
    "else:\n",
    "    print(f\"Column 'dst_global_betweenness' not found in {file_path}.\")\n",
    "\n",
    "# Rewrite the DataFrame back to the Parquet file (overwrite the original file)\n",
    "df.to_parquet(file_path)\n",
    "\n",
    "print(f\"File {file_path} has been updated and rewritten without 'dst_global_betweenness'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38efd7-02d0-41df-a50c-3da1ffc33dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
