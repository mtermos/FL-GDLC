{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "from src.network.network_features import cal_betweenness_centrality\n",
    "from sklearn.model_selection import train_test_split\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets\n",
    "\n",
    "\n",
    "\n",
    "with_sort_timestamp = False\n",
    "undersample_classes = True\n",
    "folder_path = \"fl_from_2_datasets/\"\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/cic_ton_iot.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset1 \u001b[39m=\u001b[39m datasets[\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m==>> dataset1.name: \u001b[39m\u001b[39m{\u001b[39;00mdataset1\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39m\"\u001b[39;49m\u001b[39m./dataset/cic_ton_iot.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# converting all infinity values into nan then dropping all records containing nan values\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df1\u001b[39m.\u001b[39mreplace([np\u001b[39m.\u001b[39minf, \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf], np\u001b[39m.\u001b[39mnan, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[39m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/cic_ton_iot.parquet'"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"./dataset/cic_ton_iot.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.low_classes:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.low_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'mitm' 'scanning' 'dos' 'ddos' 'injection' 'password' 'backdoor'\n",
      " 'ransomware' 'xss']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset2.name: cic_ids_2017\n"
     ]
    }
   ],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"./dataset/cic_ids_2017.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.low_classes:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.low_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Web Attack � Brute Force'\n",
      " 'Web Attack � XSS' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'ddos' 'PortScan' 'Bot' 'bruteforce' 'xss' 'FTP-Patator'\n",
      " 'SSH-Patator' 'DoS slowloris' 'DoS Slowhttptest' 'DoS Hulk'\n",
      " 'DoS GoldenEye']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'ransomware', 'DoS slowloris', 'backdoor', 'FTP-Patator', 'DoS Hulk', 'SSH-Patator', 'dos', 'injection', 'scanning', 'DoS GoldenEye', 'ddos', 'mitm', 'PortScan', 'bruteforce', 'DoS Slowhttptest', 'Bot', 'Benign', 'password', 'xss'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'PortScan', 8: 'SSH-Patator', 9: 'backdoor', 10: 'bruteforce', 11: 'ddos', 12: 'dos', 13: 'injection', 14: 'mitm', 15: 'password', 16: 'ransomware', 17: 'scanning', 18: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        2514059\n",
      "xss           2149308\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: ddos\n",
      "==>> class_label: dos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        1257030\n",
      "xss           1074654\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign              2265910\n",
      "DoS Hulk             222563\n",
      "PortScan             158804\n",
      "ddos                 128025\n",
      "DoS GoldenEye         10293\n",
      "FTP-Patator            7935\n",
      "SSH-Patator            5897\n",
      "DoS slowloris          5769\n",
      "DoS Slowhttptest       5499\n",
      "Bot                    1956\n",
      "bruteforce             1507\n",
      "xss                     652\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign              1132955\n",
      "DoS Hulk             222563\n",
      "PortScan             158804\n",
      "ddos                 128025\n",
      "DoS GoldenEye         10293\n",
      "FTP-Patator            7935\n",
      "SSH-Patator            5897\n",
      "DoS slowloris          5769\n",
      "DoS Slowhttptest       5499\n",
      "Bot                    1956\n",
      "bruteforce             1507\n",
      "xss                     652\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> number_of_nodes: 109592\n",
      "==>> number_of_edges: 210210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 3018900,\n",
       " 'num_benign': 1257030,\n",
       " 'percentage_of_benign_records': 41.6386763390639,\n",
       " 'num_attack': 1761870,\n",
       " 'percentage_of_attack_records': 58.3613236609361,\n",
       " 'attacks': ['Benign',\n",
       "  'xss',\n",
       "  'injection',\n",
       "  'password',\n",
       "  'backdoor',\n",
       "  'scanning',\n",
       "  'ransomware',\n",
       "  'ddos',\n",
       "  'mitm',\n",
       "  'dos']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> number_of_nodes: 18345\n",
      "==>> number_of_edges: 93155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 1681855,\n",
       " 'num_benign': 1132955,\n",
       " 'percentage_of_benign_records': 67.36341717924553,\n",
       " 'num_attack': 548900,\n",
       " 'percentage_of_attack_records': 32.63658282075446,\n",
       " 'attacks': ['Benign',\n",
       "  'ddos',\n",
       "  'DoS Hulk',\n",
       "  'FTP-Patator',\n",
       "  'PortScan',\n",
       "  'DoS slowloris',\n",
       "  'DoS GoldenEye',\n",
       "  'SSH-Patator',\n",
       "  'Bot',\n",
       "  'DoS Slowhttptest',\n",
       "  'xss',\n",
       "  'bruteforce']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centralities(df):\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=dataset1.src_ip_col,\n",
    "        target=dataset1.dst_ip_col,\n",
    "        create_using=nx.MultiDiGraph()\n",
    "    )\n",
    "\n",
    "    print(f\"===============\")\n",
    "    print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "    \n",
    "    print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "    print(f\"===============\")\n",
    "\n",
    "    degrees = nx.degree_centrality(G)\n",
    "    betwe = cal_betweenness_centrality(G)\n",
    "    pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "    df[\"src_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_centralities_multidigraph(df):\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(\n",
    "        df,\n",
    "        source=dataset1.src_ip_col,\n",
    "        target=dataset1.dst_ip_col,\n",
    "        create_using=nx.MultiDiGraph()\n",
    "    )\n",
    "\n",
    "    print(f\"===============\")\n",
    "    print(f\"==>> number_of_nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"==>> number_of_edges: {G.number_of_edges()}\")\n",
    "    print(f\"===============\")\n",
    "\n",
    "    degrees = nx.degree_centrality(G)\n",
    "    betwe = cal_betweenness_centrality(G)\n",
    "    pagerank = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "    df[\"src_multidigraph_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_degree\"] = df.apply(\n",
    "                lambda row: degrees.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_multidigraph_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_betweenness\"] = df.apply(\n",
    "                lambda row: betwe.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    df[\"src_multidigraph_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.src_ip_col], -1), axis=1)\n",
    "    df[\"dst_multidigraph_pagerank\"] = df.apply(\n",
    "                lambda row: pagerank.get(row[dataset1.dst_ip_col], -1), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "==>> number_of_nodes: 33432\n",
      "==>> number_of_edges: 470076\n",
      "===============\n",
      "===============\n",
      "==>> number_of_nodes: 33432\n",
      "==>> number_of_edges: 470076\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "test = pd.concat([test1, test2])\n",
    "test = add_centralities(test)\n",
    "test = add_centralities_multidigraph(test)\n",
    "test.to_parquet(folder_path + \"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    \n",
    "    #data_partition = add_centralities(data_partition)\n",
    "    #data_partition = add_centralities_multidigraph(data_partition)\n",
    "\n",
    "    data_partition.to_parquet(folder_path + \"client_{}.parquet\".format(cid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path + \"client_0.parquet\",\n",
    "    folder_path + \"client_1.parquet\",\n",
    "    folder_path + \"client_2.parquet\",\n",
    "    folder_path + \"client_3.parquet\",\n",
    "    folder_path + \"client_4.parquet\",\n",
    "    folder_path + \"client_5.parquet\",\n",
    "    folder_path + \"client_6.parquet\",\n",
    "    folder_path + \"client_7.parquet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_properties(G, name):\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['label'] = node\n",
    "    G1 = ig.Graph.from_networkx(G)\n",
    "    \n",
    "    labels = [G.nodes[node]['label'] for node in G.nodes()]\n",
    "    G1.vs['label'] = labels\n",
    "    \n",
    "    part = G1.community_infomap()\n",
    "    \n",
    "    communities = []\n",
    "    for com in part:\n",
    "        communities.append([G1.vs[node_index]['label'] for node_index in com])\n",
    "    \n",
    "    properties = {}\n",
    "    #properties[\"number_of_com\"] = len(communities)\n",
    "    properties[\"number_of_nodes\"] = G.number_of_nodes()\n",
    "    properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "    \n",
    "    degrees = [degree for _, degree in G.degree()]\n",
    "    properties[\"max_degree\"] = max(degrees)\n",
    "    properties[\"avg_degree\"] = sum(degrees) / len(degrees)\n",
    "    properties[\"transitivity\"] = nx.transitivity(G)\n",
    "    properties[\"density\"] = nx.density(G)\n",
    "    \n",
    "    node_to_community = {}\n",
    "    for community_index, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            node_to_community[node] = community_index\n",
    "    \n",
    "    inter_cluster_edges = 0\n",
    "    for u, v in G.edges():\n",
    "        if node_to_community[u] != node_to_community[v]:\n",
    "            inter_cluster_edges += 1\n",
    "    \n",
    "    properties[\"mixing_parameter\"] = inter_cluster_edges / G.number_of_edges()\n",
    "    \n",
    "    filename = os.path.join('fl_from_2_datasets', 'datasets_properties', f\"graph_{name}.json\")\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(properties, outfile)\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties for dataset_0: {'number_of_nodes': 37781, 'number_of_edges': 44590, 'max_degree': 9876, 'avg_degree': 2.3604457266880177, 'transitivity': 0.008497341929645968, 'density': 3.1239355832292454e-05, 'mixing_parameter': 0.1317335725498991}\n",
      "Graph for client 0 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_0.gexf\n",
      "Properties for dataset_1: {'number_of_nodes': 38177, 'number_of_edges': 45024, 'max_degree': 10147, 'avg_degree': 2.3586976451790345, 'transitivity': 0.004805040211363703, 'density': 3.089241467386623e-05, 'mixing_parameter': 0.10963041933191187}\n",
      "Graph for client 1 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_1.gexf\n",
      "Properties for dataset_2: {'number_of_nodes': 37646, 'number_of_edges': 44370, 'max_degree': 9748, 'avg_degree': 2.357222546884131, 'transitivity': 0.005274043433298862, 'density': 3.130857413845306e-05, 'mixing_parameter': 0.13838178949740815}\n",
      "Graph for client 2 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_2.gexf\n",
      "Properties for dataset_3: {'number_of_nodes': 37854, 'number_of_edges': 44796, 'max_degree': 9940, 'avg_degree': 2.366777619274053, 'transitivity': 0.0028469750889679717, 'density': 3.1262748253428435e-05, 'mixing_parameter': 0.1465755871059916}\n",
      "Graph for client 3 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_3.gexf\n",
      "Properties for dataset_4: {'number_of_nodes': 37770, 'number_of_edges': 44619, 'max_degree': 9928, 'avg_degree': 2.3626687847498014, 'transitivity': 0.006137121656301964, 'density': 3.127788377703674e-05, 'mixing_parameter': 0.1308635334722876}\n",
      "Graph for client 4 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_4.gexf\n",
      "Properties for dataset_5: {'number_of_nodes': 14870, 'number_of_edges': 54097, 'max_degree': 6011, 'avg_degree': 7.275991930060525, 'transitivity': 0.000742864884576571, 'density': 0.00024466984767168354, 'mixing_parameter': 0.5990535519529734}\n",
      "Graph for client 5 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_5.gexf\n",
      "Properties for dataset_6: {'number_of_nodes': 14855, 'number_of_edges': 54099, 'max_degree': 6065, 'avg_degree': 7.283608212722989, 'transitivity': 0.0007964008838849573, 'density': 0.0002451732938172542, 'mixing_parameter': 0.5984953511155474}\n",
      "Graph for client 6 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_6.gexf\n",
      "Properties for dataset_7: {'number_of_nodes': 14850, 'number_of_edges': 53891, 'max_degree': 5988, 'avg_degree': 7.258047138047138, 'transitivity': 0.0007258287531577934, 'density': 0.0002443951491025368, 'mixing_parameter': 0.5984301645914902}\n",
      "Graph for client 7 saved to ./fl_from_2_datasets/fl_client_graphs/graph_client_7.gexf\n",
      "All graphs have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for idx, client_path in enumerate(clients_paths):\n",
    "    df_client = pd.read_parquet(client_path)\n",
    "    src_ip_col = 'Src IP'\n",
    "    dst_ip_col = 'Dst IP'\n",
    "    G_client = nx.from_pandas_edgelist(\n",
    "        df_client,\n",
    "        source=src_ip_col,\n",
    "        target=dst_ip_col,\n",
    "        create_using=nx.DiGraph()\n",
    "    )\n",
    "    dataset_name = f\"dataset_{idx}\"\n",
    "    properties = compute_graph_properties(G_client, dataset_name)\n",
    "    print(f\"Properties for {dataset_name}: {properties}\")\n",
    "    graph_path = \"./fl_from_2_datasets/fl_client_graphs/graph_client_{}.gexf\".format(idx)\n",
    "    \n",
    "    nx.write_gexf(G_client, graph_path)\n",
    "    \n",
    "    print(f\"Graph for client {idx} saved to {graph_path}\")\n",
    "\n",
    "print(\"All graphs have been created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.44.20.15\n",
    "#198.55.220.44\n",
    "last_client_path = clients_paths[-1]\n",
    "#8.0.6.4\n",
    "#8.6.0.1\n",
    "\n",
    "df_client_last = pd.read_parquet(last_client_path)\n",
    "ip_address = '8.6.0.1'\n",
    "filtered_df = df_client[(df_client['Src IP'] == ip_address) | (df_client['Dst IP'] == ip_address)]\n",
    "filtered_df[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n",
       "       'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
       "       'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
       "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
       "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
       "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
       "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
       "       'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',\n",
       "       'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
       "       'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
       "       'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n",
       "       'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',\n",
       "       'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt',\n",
       "       'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count',\n",
       "       'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',\n",
       "       'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg',\n",
       "       'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n",
       "       'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n",
       "       'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',\n",
       "       'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',\n",
       "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
       "       'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Attack', 'Class',\n",
       "       'src_degree', 'dst_degree', 'src_betweenness', 'dst_betweenness',\n",
       "       'src_pagerank', 'dst_pagerank', 'src_multidigraph_degree',\n",
       "       'dst_multidigraph_degree', 'src_multidigraph_betweenness',\n",
       "       'dst_multidigraph_betweenness', 'src_multidigraph_pagerank',\n",
       "       'dst_multidigraph_pagerank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_client_last.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with PCA Component:\n",
      "                                          Flow ID          Src IP  Src Port  \\\n",
      "990242   172.217.10.225-192.168.10.12-443-51948-6  172.217.10.225     443.0   \n",
      "1526483       172.16.0.1-192.168.10.50-45442-80-6      172.16.0.1   45442.0   \n",
      "775084      192.168.10.3-192.168.10.16-53-9253-17   192.168.10.16    9253.0   \n",
      "83479     192.168.10.9-54.210.193.125-10222-443-6    192.168.10.9   10222.0   \n",
      "844875     192.168.10.15-217.196.33.13-52607-80-6   192.168.10.15   52607.0   \n",
      "\n",
      "                 Dst IP  Dst Port  Protocol            Timestamp  \\\n",
      "990242    192.168.10.12   51948.0       6.0  03/07/2017 04:10:14   \n",
      "1526483   192.168.10.50      80.0       6.0       5/7/2017 10:49   \n",
      "775084     192.168.10.3      53.0      17.0        7/7/2017 9:55   \n",
      "83479    54.210.193.125     443.0       6.0        4/7/2017 4:30   \n",
      "844875    217.196.33.13      80.0       6.0  03/07/2017 01:58:38   \n",
      "\n",
      "         Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...  dst_betweenness  \\\n",
      "990242             3.0           2.0           0.0  ...     9.895686e-02   \n",
      "1526483     84539305.0           4.0           7.0  ...     3.664551e-02   \n",
      "775084         61045.0           2.0           2.0  ...     1.091051e-01   \n",
      "83479       10526395.0           3.0           3.0  ...     1.365920e-07   \n",
      "844875       5368959.0           5.0           7.0  ...     8.304202e-08   \n",
      "\n",
      "         src_pagerank  dst_pagerank  src_multidigraph_degree  \\\n",
      "990242       0.000092      0.027914                 0.006964   \n",
      "1526483      0.005116      0.008887                11.626572   \n",
      "775084       0.026439      0.109230                 1.565923   \n",
      "83479        0.028355      0.000031                 1.933874   \n",
      "844875       0.027932      0.000044                 1.889588   \n",
      "\n",
      "         dst_multidigraph_degree  src_multidigraph_betweenness  \\\n",
      "990242                  1.868357                      0.000013   \n",
      "1526483                12.207167                      0.000078   \n",
      "775084                 10.016092                      0.078625   \n",
      "83479                   0.000811                      0.086542   \n",
      "844875                  0.001352                      0.081103   \n",
      "\n",
      "         dst_multidigraph_betweenness  src_multidigraph_pagerank  \\\n",
      "990242                   9.895686e-02                   0.000092   \n",
      "1526483                  3.664551e-02                   0.005116   \n",
      "775084                   1.091051e-01                   0.026439   \n",
      "83479                    1.365920e-07                   0.028355   \n",
      "844875                   8.304202e-08                   0.027932   \n",
      "\n",
      "         dst_multidigraph_pagerank       PC1  \n",
      "990242                    0.027914       NaN  \n",
      "1526483                   0.008887       NaN  \n",
      "775084                    0.109230       NaN  \n",
      "83479                     0.000031 -0.264189  \n",
      "844875                    0.000044       NaN  \n",
      "\n",
      "[5 rows x 98 columns]\n",
      "990242          NaN\n",
      "1526483         NaN\n",
      "775084          NaN\n",
      "83479     -0.264189\n",
      "844875          NaN\n",
      "             ...   \n",
      "150326     1.023150\n",
      "390864    -2.877478\n",
      "533755          NaN\n",
      "394422    -2.877478\n",
      "1214685         NaN\n",
      "Name: PC1, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "centrality_columns = [\n",
    "    'src_degree', 'dst_degree', 'src_betweenness', 'dst_betweenness',\n",
    "    'src_pagerank', 'dst_pagerank', 'src_multidigraph_degree',\n",
    "    'dst_multidigraph_degree', 'src_multidigraph_betweenness',\n",
    "    'dst_multidigraph_betweenness', 'src_multidigraph_pagerank',\n",
    "    'dst_multidigraph_pagerank'\n",
    "]\n",
    "\n",
    "centrality_data = df_client_last[centrality_columns]\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "centrality_data_imputed = imputer.fit_transform(centrality_data)\n",
    "\n",
    "if pd.isnull(centrality_data_imputed).any():\n",
    "    raise ValueError(\"There are still NaN values after imputation.\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_centrality_data = scaler.fit_transform(centrality_data_imputed)\n",
    "\n",
    "if pd.isnull(scaled_centrality_data).any():\n",
    "    raise ValueError(\"There are NaN values after standardization.\")\n",
    "\n",
    "pca = PCA(n_components=1)  \n",
    "principal_components = pca.fit_transform(scaled_centrality_data)\n",
    "\n",
    "if pd.isnull(principal_components).any():\n",
    "    raise ValueError(\"PCA output contains NaN values. Check the input data or PCA parameters.\")\n",
    "\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1'])\n",
    "\n",
    "df_new = pd.concat([df_client_last, df_pca], axis=1)\n",
    "\n",
    "\n",
    "print(\"Updated DataFrame with PCA Component:\")\n",
    "print(df_new.head())\n",
    "\n",
    "print(df_new['PC1'].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990242          NaN\n",
       "1526483         NaN\n",
       "775084          NaN\n",
       "83479     -0.264189\n",
       "844875          NaN\n",
       "             ...   \n",
       "150326     1.023150\n",
       "390864    -2.877478\n",
       "533755          NaN\n",
       "394422    -2.877478\n",
       "1214685         NaN\n",
       "Name: PC1, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[\"PC1\"].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
