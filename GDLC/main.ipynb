{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Network Traffic for Intrusion Detection\n",
    "\n",
    "This notebook is dedicated to analyzing network traffic data to detect potential intrusions. The analysis includes data preprocessing, feature exploration, and the application of machine learning models to classify network behavior as normal or suspicious. We used CNN, LSTM and GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "In this section, we load the network traffic dataset from its source. The dataset includes various features related to network activity, such as source IP, destination IP, packet sizes, and timestamps. Understanding the structure of this data is crucial for our analysis and subsequent feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from src.data.dataset_info import datasets\n",
    "from src.models import MyCNN, MyLSTM, MyGRU\n",
    "# from src.models.dense_nn import  MyDenseNN\n",
    "\n",
    "#specifying main configuration of the experiment\n",
    "multi_class = True\n",
    "with_network_features = True\n",
    "\n",
    "with_sort_timestamp = True\n",
    "sequence_length = 3\n",
    "with_cross_validation = True\n",
    "cross_validation_splits_num = 5\n",
    "\n",
    "# choosing the dataset\n",
    "dataset = datasets[0]\n",
    "name = dataset.name\n",
    "print(\"dataset: {}\".format(name))\n",
    "#path = \"./datasets/preprocessed/{}.pkl\".format(name)\n",
    "####path \n",
    "path = \"datasets\\\\preprocessed\\\\{}.pkl\".format(name)\n",
    "# graph_path = \"./datasets/preprocessed/graph_{}.gexf\".format(name)\n",
    "\n",
    "# loading the dataframe\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'mitm' 'scanning' 'dos' 'ddos' 'injection' 'password' 'backdoor'\n",
      " 'ransomware' 'xss']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Attack\n",
       "Benign        2514059\n",
       "xss           2149308\n",
       "password       340208\n",
       "injection      277696\n",
       "scanning        36205\n",
       "backdoor        27145\n",
       "ransomware       5098\n",
       "mitm              517\n",
       "ddos              202\n",
       "dos               145\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(df[dataset.class_col].unique())\n",
    "df[dataset.class_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "59\n",
      "10\n",
      "7\n",
      "39\n",
      "106\n",
      "cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "# the input dimension of the training set\n",
    "input_dim = df.shape[1] - len(dataset.drop_columns) - len(dataset.weak_columns) - 1  # for the label_column\n",
    "print(input_dim)\n",
    "if not with_network_features:\n",
    "    input_dim = input_dim - len(dataset.network_features)\n",
    "    \n",
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "num_classes = 2\n",
    "if multi_class:\n",
    "    num_classes = len(df[\"Attack\"].unique())\n",
    "\n",
    "num_epochs = 30\n",
    "    \n",
    "dropped_columns = dataset.drop_columns\n",
    "dataset_name = dataset.name\n",
    "print(input_dim)\n",
    "print(num_classes)\n",
    "print(len(dropped_columns))\n",
    "print(len(dataset.weak_columns))\n",
    "print(df.shape[1])\n",
    "print(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "cic_ton_iot\n",
      "10\n",
      "True\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(input_dim)\n",
    "print(dataset_name)\n",
    "print(num_classes)\n",
    "print(multi_class)\n",
    "print(len(dataset.network_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from importlib import reload\n",
    "#import src.models\n",
    "#reload(src.models)\n",
    "#from src.models import MyCNN\n",
    "nf = []\n",
    "if with_network_features:\n",
    "    nf = dataset.network_features\n",
    "\n",
    "models = [\n",
    "    MyCNN(\n",
    "        input_dim=input_dim,\n",
    "        dataset_name=dataset_name,\n",
    "        num_classes=num_classes,\n",
    "        multi_class=multi_class,\n",
    "        network_features=nf,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=256,\n",
    "        early_stop_patience=10,\n",
    "    ),\n",
    "    # MyLSTM(\n",
    "    #     sequence_length=sequence_length,\n",
    "    #     input_dim=input_dim,\n",
    "    #     dataset_name=dataset_name,\n",
    "    #     num_classes=num_classes,\n",
    "    #     multi_class=multi_class,\n",
    "    #     network_features=nf,\n",
    "    #     use_generator=True,\n",
    "    #     epochs=num_epochs,\n",
    "    #     batch_size=256,,\n",
    "        # early_stop_patience=10,\n",
    "    # ),\n",
    "    # MyGRU(\n",
    "    #     sequence_length=sequence_length,\n",
    "    #     input_dim=input_dim,\n",
    "    #     dataset_name=dataset_name,\n",
    "    #     num_classes=num_classes,\n",
    "    #     multi_class=multi_class,\n",
    "    #     network_features=nf,\n",
    "    #     use_generator=True,\n",
    "    #     epochs=num_epochs,\n",
    "    #     batch_size=256,,\n",
    "        # early_stop_patience=10,\n",
    "    # )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"stratified k-fold cross validation - manual sequences\"\n",
    "results[\"multi_class\"] = multi_class\n",
    "results[\"with_sort_timestamp\"] = with_sort_timestamp\n",
    "results[\"sequence_length\"] = sequence_length\n",
    "results[\"with_cross_validation\"] = with_cross_validation\n",
    "results[\"cross_validation_splits_num\"] = cross_validation_splits_num\n",
    "results[\"with_network_features\"] = with_network_features\n",
    "results[\"network_features\"] = dataset.cn_measures\n",
    "\n",
    "results[\"dataset_name\"] = dataset_name\n",
    "results[\"input_dim\"] = input_dim\n",
    "results[\"dropped_columns\"] = dropped_columns\n",
    "results[\"num_dropped_columns\"] = len(dropped_columns)\n",
    "\n",
    "results[\"models\"] = {}\n",
    "results[\"average_acc\"] = {}\n",
    "results[\"average\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          25/04/2019 05:18:52 pm\n",
       "1          25/04/2019 05:18:49 pm\n",
       "2          25/04/2019 05:18:37 pm\n",
       "3          25/04/2019 05:18:42 pm\n",
       "4          25/04/2019 05:18:42 pm\n",
       "                    ...          \n",
       "5351755    25/04/2019 04:34:34 pm\n",
       "5351756    25/04/2019 04:30:56 pm\n",
       "5351757    25/04/2019 04:48:39 pm\n",
       "5351758    25/04/2019 05:01:42 pm\n",
       "5351759    25/04/2019 04:09:55 pm\n",
       "Name: Timestamp, Length: 5350583, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[dataset.timestamp_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'benign', 1: 'attack'}\n",
      "==>> labels_names: {0: 'Benign', 1: 'xss', 2: 'password', 3: 'scanning', 4: 'injection', 5: 'ransomware', 6: 'backdoor', 7: 'mitm', 8: 'ddos', 9: 'dos'}\n",
      "\n",
      "==>> labels_names: {0: 'Benign', 1: 'xss', 2: 'password', 3: 'scanning', 4: 'injection', 5: 'ransomware', 6: 'backdoor', 7: 'mitm', 8: 'ddos', 9: 'dos'}\n"
     ]
    }
   ],
   "source": [
    "if with_sort_timestamp:\n",
    "    df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n",
    "    df.sort_values(dataset.timestamp_col, inplace= True)\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "print(f\"==>> labels_names: {labels_names}\")\n",
    "if multi_class:\n",
    "    fac = pd.factorize(df[dataset.class_col])\n",
    "    labels_names = {index: value for index, value in enumerate(fac[1])}\n",
    "    print(f\"==>> labels_names: {labels_names}\")\n",
    "    df[dataset.label_col] = fac[0]  # type: ignore\n",
    "\n",
    "print()\n",
    "df.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "df.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "\n",
    "#if not with_network_features:\n",
    " #   df = df.drop(dataset.network_features, axis=1)\n",
    "if not with_network_features:\n",
    "    df = df.drop([col for col in dataset.network_features if col in df.columns], axis=1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Src Port', 'Dst Port', 'Attack']\n",
      "Weak columns to be dropped: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Cnt', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Subflow Bwd Pkts', 'Flow IAT Mean', 'Fwd Pkt Len Max', 'Flow IAT Max', 'Active Std', 'Bwd Header Len', 'Tot Bwd Pkts', 'Bwd Pkt Len Mean', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'CWE Flag Count', 'Bwd IAT Tot', 'Fwd IAT Mean', 'Fwd Pkt Len Std', 'Pkt Len Mean', 'Flow IAT Min', 'TotLen Bwd Pkts', 'Bwd Pkt Len Max', 'Pkt Len Var', 'FIN Flag Cnt', 'Bwd IAT Mean', 'Idle Mean', 'Pkt Len Max', 'Flow Pkts/s', 'Flow Duration', 'Pkt Len Std', 'Fwd IAT Tot', 'PSH Flag Cnt', 'Active Mean', 'Bwd Pkt Len Std', 'Fwd Pkt Len Mean']\n",
      "Remaining columns after dropping: Index(['Protocol', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Min',\n",
      "       'Bwd Pkt Len Min', 'Flow Byts/s', 'Flow IAT Std', 'Fwd IAT Max',\n",
      "       'Fwd IAT Min', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
      "       'Fwd PSH Flags', 'Fwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n",
      "       'Pkt Len Min', 'SYN Flag Cnt', 'RST Flag Cnt', 'ACK Flag Cnt',\n",
      "       'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n",
      "       'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n",
      "       'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n",
      "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Max', 'Active Min',\n",
      "       'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'src_betweenness',\n",
      "       'dst_betweenness', 'src_local_betweenness', 'dst_local_betweenness',\n",
      "       'src_degree', 'dst_degree', 'src_local_degree', 'dst_local_degree',\n",
      "       'src_eigenvector', 'dst_eigenvector', 'src_closeness', 'dst_closeness',\n",
      "       'src_pagerank', 'dst_pagerank', 'src_local_pagerank',\n",
      "       'dst_local_pagerank', 'src_k_core', 'dst_k_core', 'src_k_truss',\n",
      "       'dst_k_truss', 'src_Comm', 'dst_Comm'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns to be dropped:\", dataset.drop_columns)\n",
    "print(\"Weak columns to be dropped:\", dataset.weak_columns)\n",
    "print(\"Remaining columns after dropping:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "Columns to be dropped: ['Flow ID', 'Src IP', 'Dst IP', 'Timestamp', 'Src Port', 'Dst Port', 'Attack']\n",
      "Weak columns to be dropped: ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'URG Flag Cnt', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Subflow Bwd Pkts', 'Flow IAT Mean', 'Fwd Pkt Len Max', 'Flow IAT Max', 'Active Std', 'Bwd Header Len', 'Tot Bwd Pkts', 'Bwd Pkt Len Mean', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'CWE Flag Count', 'Bwd IAT Tot', 'Fwd IAT Mean', 'Fwd Pkt Len Std', 'Pkt Len Mean', 'Flow IAT Min', 'TotLen Bwd Pkts', 'Bwd Pkt Len Max', 'Pkt Len Var', 'FIN Flag Cnt', 'Bwd IAT Mean', 'Idle Mean', 'Pkt Len Max', 'Flow Pkts/s', 'Flow Duration', 'Pkt Len Std', 'Fwd IAT Tot', 'PSH Flag Cnt', 'Active Mean', 'Bwd Pkt Len Std', 'Fwd Pkt Len Mean']\n",
      "Existing columns in the DataFrame: Index(['Protocol', 'Tot Fwd Pkts', 'TotLen Fwd Pkts', 'Fwd Pkt Len Min',\n",
      "       'Bwd Pkt Len Min', 'Flow Byts/s', 'Flow IAT Std', 'Fwd IAT Max',\n",
      "       'Fwd IAT Min', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
      "       'Fwd PSH Flags', 'Fwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n",
      "       'Pkt Len Min', 'SYN Flag Cnt', 'RST Flag Cnt', 'ACK Flag Cnt',\n",
      "       'ECE Flag Cnt', 'Down/Up Ratio', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n",
      "       'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n",
      "       'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n",
      "       'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Max', 'Active Min',\n",
      "       'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'src_betweenness',\n",
      "       'dst_betweenness', 'src_local_betweenness', 'dst_local_betweenness',\n",
      "       'src_degree', 'dst_degree', 'src_local_degree', 'dst_local_degree',\n",
      "       'src_eigenvector', 'dst_eigenvector', 'src_closeness', 'dst_closeness',\n",
      "       'src_pagerank', 'dst_pagerank', 'src_local_pagerank',\n",
      "       'dst_local_pagerank', 'src_k_core', 'dst_k_core', 'src_k_truss',\n",
      "       'dst_k_truss', 'src_Comm', 'dst_Comm'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(len(df.columns))\n",
    "print(\"Columns to be dropped:\", dataset.drop_columns)\n",
    "print(\"Weak columns to be dropped:\", dataset.weak_columns)\n",
    "print(\"Existing columns in the DataFrame:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['Label'].to_numpy()\n",
    "df = df.drop([dataset.label_col], axis=1).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "(5350583, 59)\n",
      "5350583\n",
      "==>> train_index: [     0      1      2 ... 891765 891766 891767]\n",
      "==>> training_labels: (891768,)\n",
      "==>> test_index: [ 891768  891769  891770 ... 1783528 1783529 1783530]\n",
      "==>> testing_labels: (891763,)\n",
      "fold: 1\n",
      "=====================================\n",
      "=====================================\n",
      "fold: 1/5\n",
      "training: cnn bc nf cnn-64-64\n",
      "sequential: False\n",
      "WARNING:tensorflow:From C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "hereeeeeeeeee\n",
      "59\n",
      "not hereeeeeeeeeeeee\n",
      "Shape of input data: (891768, 59)\n",
      "Shape of input data: (891768,)\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9957\n",
      "Epoch 1: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-01-0.0210.hdf5\n",
      "3484/3484 [==============================] - 49s 13ms/step - loss: 0.0210 - accuracy: 0.9957\n",
      "Epoch 2/30\n",
      "   1/3484 [..............................] - ETA: 1:13 - loss: 0.0018 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9971\n",
      "Epoch 2: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-02-0.0160.hdf5\n",
      "3484/3484 [==============================] - 45s 13ms/step - loss: 0.0160 - accuracy: 0.9971\n",
      "Epoch 3/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9971\n",
      "Epoch 3: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-03-0.0157.hdf5\n",
      "3484/3484 [==============================] - 47s 13ms/step - loss: 0.0157 - accuracy: 0.9971\n",
      "Epoch 4/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 4: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-04-0.0156.hdf5\n",
      "3484/3484 [==============================] - 46s 13ms/step - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 5/30\n",
      "3480/3484 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9971\n",
      "Epoch 5: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-05-0.0155.hdf5\n",
      "3484/3484 [==============================] - 45s 13ms/step - loss: 0.0155 - accuracy: 0.9971\n",
      "Epoch 6/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9971\n",
      "Epoch 6: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-06-0.0154.hdf5\n",
      "3484/3484 [==============================] - 48s 14ms/step - loss: 0.0154 - accuracy: 0.9971\n",
      "Epoch 7/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 7: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-07-0.0153.hdf5\n",
      "3484/3484 [==============================] - 47s 13ms/step - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 8/30\n",
      "3480/3484 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 8: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-08-0.0153.hdf5\n",
      "3484/3484 [==============================] - 47s 14ms/step - loss: 0.0153 - accuracy: 0.9971\n",
      "Epoch 9/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9971\n",
      "Epoch 9: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-09-0.0152.hdf5\n",
      "3484/3484 [==============================] - 44s 13ms/step - loss: 0.0152 - accuracy: 0.9971\n",
      "Epoch 10/30\n",
      "3482/3484 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9971\n",
      "Epoch 10: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-10-0.0151.hdf5\n",
      "3484/3484 [==============================] - 44s 13ms/step - loss: 0.0151 - accuracy: 0.9971\n",
      "Epoch 11/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9971\n",
      "Epoch 11: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-11-0.0151.hdf5\n",
      "3484/3484 [==============================] - 54s 16ms/step - loss: 0.0151 - accuracy: 0.9971\n",
      "Epoch 12/30\n",
      "3482/3484 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 12: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-12-0.0150.hdf5\n",
      "3484/3484 [==============================] - 59s 17ms/step - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 13/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 13: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-13-0.0150.hdf5\n",
      "3484/3484 [==============================] - 47s 14ms/step - loss: 0.0150 - accuracy: 0.9971\n",
      "Epoch 14/30\n",
      "3480/3484 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9971\n",
      "Epoch 14: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-14-0.0149.hdf5\n",
      "3484/3484 [==============================] - 42s 12ms/step - loss: 0.0149 - accuracy: 0.9971\n",
      "Epoch 15/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9971\n",
      "Epoch 15: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-15-0.0149.hdf5\n",
      "3484/3484 [==============================] - 49s 14ms/step - loss: 0.0149 - accuracy: 0.9971\n",
      "Epoch 16/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 16: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-16-0.0148.hdf5\n",
      "3484/3484 [==============================] - 51s 15ms/step - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 17/30\n",
      "3482/3484 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 17: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-17-0.0148.hdf5\n",
      "3484/3484 [==============================] - 48s 14ms/step - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 18/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 18: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-18-0.0148.hdf5\n",
      "3484/3484 [==============================] - 47s 14ms/step - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 19/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 19: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-19-0.0148.hdf5\n",
      "3484/3484 [==============================] - 63s 18ms/step - loss: 0.0148 - accuracy: 0.9971\n",
      "Epoch 20/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 20: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-20-0.0147.hdf5\n",
      "3484/3484 [==============================] - 71s 20ms/step - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 21/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 21: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-21-0.0147.hdf5\n",
      "3484/3484 [==============================] - 69s 20ms/step - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 22/30\n",
      "3480/3484 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 22: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-22-0.0147.hdf5\n",
      "3484/3484 [==============================] - 62s 18ms/step - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 23/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 23: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-23-0.0147.hdf5\n",
      "3484/3484 [==============================] - 50s 14ms/step - loss: 0.0147 - accuracy: 0.9971\n",
      "Epoch 24/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 24: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-24-0.0146.hdf5\n",
      "3484/3484 [==============================] - 68s 19ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 25/30\n",
      "3482/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 25: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-25-0.0146.hdf5\n",
      "3484/3484 [==============================] - 64s 18ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 26/30\n",
      "3481/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 26: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-26-0.0146.hdf5\n",
      "3484/3484 [==============================] - 50s 14ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 27/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 27: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-27-0.0146.hdf5\n",
      "3484/3484 [==============================] - 54s 16ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 28/30\n",
      "3483/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 28: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-28-0.0146.hdf5\n",
      "3484/3484 [==============================] - 53s 15ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 29/30\n",
      "3482/3484 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 29: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-29-0.0146.hdf5\n",
      "3484/3484 [==============================] - 57s 16ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 30/30\n",
      "3484/3484 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9971\n",
      "Epoch 30: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-30-0.0146.hdf5\n",
      "3484/3484 [==============================] - 54s 16ms/step - loss: 0.0146 - accuracy: 0.9971\n",
      "3484/3484 [==============================] - 24s 7ms/step\n",
      "confusion_matrix:\n",
      "[[358136  33387]\n",
      " [     4 500236]]\n",
      "End of confusion_matrix:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.96    391523\n",
      "           1       0.94      1.00      0.97    500240\n",
      "\n",
      "    accuracy                           0.96    891763\n",
      "   macro avg       0.97      0.96      0.96    891763\n",
      "weighted avg       0.96      0.96      0.96    891763\n",
      "\n",
      "End of Classification Report:\n",
      "[[358136  33387]\n",
      " [     4 500236]]\n",
      "cnn bc nf cnn-64-64: {'accuracy': 0.9625561948634335, 'recall': 0.9625561948634335, 'precision': 0.9648979486826417, 'f1s': 0.9623270182237648, 'FPR': 0.08527468373505516, 'FNR': 7.996161842315688e-06, 'time': 27.33600425720215, 'fold': 1}\n",
      "cnn bc nf cnn-64-64 average accuracy: 0.9625561948634335\n",
      "==>> train_index: [      0       1       2 ... 1783528 1783529 1783530]\n",
      "==>> training_labels: (1783531,)\n",
      "==>> test_index: [1783531 1783532 1783533 ... 2675291 2675292 2675293]\n",
      "==>> testing_labels: (891763,)\n",
      "fold: 2\n",
      "=====================================\n",
      "=====================================\n",
      "fold: 2/5\n",
      "training: cnn bc nf cnn-64-64\n",
      "sequential: False\n",
      "hereeeeeeeeee\n",
      "59\n",
      "not hereeeeeeeeeeeee\n",
      "Shape of input data: (1783531, 59)\n",
      "Shape of input data: (1783531,)\n",
      "Epoch 1/30\n",
      "6964/6967 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9786\n",
      "Epoch 1: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-01-0.0688.hdf5\n",
      "6967/6967 [==============================] - 89s 13ms/step - loss: 0.0688 - accuracy: 0.9786\n",
      "Epoch 2/30\n",
      "   9/6967 [..............................] - ETA: 1:28 - loss: 0.0681 - accuracy: 0.9761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6967/6967 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9798\n",
      "Epoch 2: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-02-0.0630.hdf5\n",
      "6967/6967 [==============================] - 83s 12ms/step - loss: 0.0630 - accuracy: 0.9798\n",
      "Epoch 3/30\n",
      "6964/6967 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9798\n",
      "Epoch 3: saving model to ./models/weights/cic_ton_iot/cnn bc nf cnn-64-64\\weights-improvement-03-0.0615.hdf5\n",
      "6967/6967 [==============================] - 85s 12ms/step - loss: 0.0615 - accuracy: 0.9798\n",
      "Epoch 4/30\n",
      "1991/6967 [=======>......................] - ETA: 1:07 - loss: 0.0610 - accuracy: 0.9799"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, training\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of input data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, training_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(training,\n\u001b[0;32m     37\u001b[0m             training_labels)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     38\u001b[0m predictions, prediction_time \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m     39\u001b[0m     testing)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     40\u001b[0m model_name, scores, class_report \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     predictions,\n\u001b[0;32m     42\u001b[0m     testing_labels,\n\u001b[0;32m     43\u001b[0m     prediction_time\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32m~\\Documents\\Thesis\\GDLC-main\\GDLC-main\\src\\models\\cnn.py:119\u001b[0m, in \u001b[0;36mMyCNN.train\u001b[1;34m(self, training_data, training_labels)\u001b[0m\n\u001b[0;32m    115\u001b[0m callbacks_list \u001b[38;5;241m=\u001b[39m [checkpoint, earlyStopping]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# X_train = np.reshape(training, (training.shape[0],training.shape[1],1))\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(training_data, training_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs,\n\u001b[0;32m    120\u001b[0m                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, callbacks\u001b[38;5;241m=\u001b[39mcallbacks_list)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "tscv = TimeSeriesSplit(n_splits=cross_validation_splits_num)\n",
    "print(tscv)\n",
    "print(df.shape)\n",
    "print(df.shape[0])\n",
    "i = 0\n",
    "for train_index, test_index in tscv.split(df):\n",
    "    training_labels = labels[train_index]\n",
    "    print(f\"==>> train_index: {train_index}\")\n",
    "    print(f\"==>> training_labels: {training_labels.shape}\")\n",
    "    testing_labels = labels[test_index]\n",
    "    print(f\"==>> test_index: {test_index}\")\n",
    "    print(f\"==>> testing_labels: {testing_labels.shape}\")\n",
    "\n",
    "    i += 1\n",
    "    print(\"fold: {}\".format(i))\n",
    "    # print(\"train_index: {}\".format(train_index))\n",
    "    print(\"=====================================\")\n",
    "    print(\"=====================================\")\n",
    "    # print(\"fold: {}/{}\".format(i, len(list_of_dfs)))\n",
    "    print(\"fold: {}/{}\".format(i, cross_validation_splits_num))\n",
    "\n",
    "    for model in models:\n",
    "        print(\"training: {}\".format(model.model_name()))\n",
    "        print(\"sequential: {}\".format(model.sequential))\n",
    "\n",
    "        training = df[train_index]\n",
    "        testing = df[test_index]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        training = scaler.fit_transform(training)\n",
    "        testing = scaler.transform(testing)\n",
    "\n",
    "        model.build()\n",
    "        print(\"Shape of input data:\", training.shape)\n",
    "        print(\"Shape of input data:\", training_labels.shape)\n",
    "        model.train(training,\n",
    "                    training_labels)  # type: ignore\n",
    "        predictions, prediction_time = model.predict(\n",
    "            testing)  # type: ignore\n",
    "        model_name, scores, class_report = model.evaluate(  # type: ignore\n",
    "            predictions,\n",
    "            testing_labels,\n",
    "            prediction_time\n",
    "        )\n",
    "        scores[\"fold\"] = i\n",
    "        if i == 1:\n",
    "            results[\"models\"][model_name] = {}\n",
    "            results[\"models\"][model_name][\"scores\"] = [scores]\n",
    "            results[\"models\"][model_name][\"class_report\"] = [class_report]\n",
    "        else:\n",
    "            results[\"models\"][model_name][\"scores\"].append(scores)\n",
    "            results[\"models\"][model_name][\"class_report\"].append(\n",
    "                class_report)\n",
    "        # results[str(i) + model_name] = scores\n",
    "        print(\"{}: {}\".format(model_name, scores))\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model.model_name()\n",
    "        average_acc = 0\n",
    "        average_recall = 0\n",
    "        average_precision = 0\n",
    "        average_f1s = 0\n",
    "        average_FPR = 0\n",
    "        average_FNR = 0\n",
    "        for result in results[\"models\"][model_name][\"scores\"]:  # type: ignore\n",
    "            average_acc += result[\"accuracy\"]\n",
    "            average_recall += result[\"recall\"]\n",
    "            average_precision += result[\"precision\"]\n",
    "            average_f1s += result[\"f1s\"]\n",
    "            average_FPR += result[\"FPR\"]\n",
    "            average_FNR += result[\"FNR\"]\n",
    "        average_acc = average_acc / i\n",
    "        average_recall = average_recall / i\n",
    "        average_precision = average_precision / i\n",
    "        average_f1s = average_f1s / i\n",
    "        average_FPR = average_FPR / i\n",
    "        average_FNR = average_FNR / i\n",
    "        if i == 1:\n",
    "            results[\"models\"][model_name][\"average\"] = [\n",
    "                {\n",
    "                    \"average_acc\": average_acc,\n",
    "                    \"average_recall\": average_recall,\n",
    "                    \"average_precision\": average_precision,\n",
    "                    \"average_f1s\": average_f1s,\n",
    "                    \"average_FPR\": average_FPR,\n",
    "                    \"average_FNR\": average_FNR,\n",
    "                    \"fold\": i\n",
    "                }\n",
    "            ]\n",
    "            results[\"average_acc\"][model_name] = average_acc\n",
    "            results[\"average\"][model_name] = {\n",
    "                \"average_acc\": average_acc,\n",
    "                \"average_recall\": average_recall,\n",
    "                \"average_precision\": average_precision,\n",
    "                \"average_f1s\": average_f1s,\n",
    "                \"average_FPR\": average_FPR,\n",
    "                \"average_FNR\": average_FNR\n",
    "            }\n",
    "        else:\n",
    "            results[\"models\"][model_name][\"average\"].append(\n",
    "                {\n",
    "                    \"average_acc\": average_acc,\n",
    "                    \"average_recall\": average_recall,\n",
    "                    \"average_precision\": average_precision,\n",
    "                    \"average_f1s\": average_f1s,\n",
    "                    \"average_FPR\": average_FPR,\n",
    "                    \"average_FNR\": average_FNR,\n",
    "                    \"fold\": i\n",
    "                })\n",
    "            results[\"average_acc\"][model_name] = average_acc\n",
    "            results[\"average\"][model_name] = {\n",
    "                \"average_acc\": average_acc,\n",
    "                \"average_recall\": average_recall,\n",
    "                \"average_precision\": average_precision,\n",
    "                \"average_f1s\": average_f1s,\n",
    "                \"average_FPR\": average_FPR,\n",
    "                \"average_FNR\": average_FNR\n",
    "            }\n",
    "        print(\"{} average accuracy: {}\".format(model_name, average_acc))\n",
    "\n",
    "results[\"endtime\"] = time.strftime(\"%Y:%m:%d-%H:%M:%S\")\n",
    "\n",
    "print(f\"==>> results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "if not os.path.isdir('./results/{}'.format(dataset_name)):\n",
    "    os.mkdir('./results/{}'.format(dataset_name))\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# saving the results to a file for future refernece\n",
    "filename = ('./results/{}/{}.json'.format(dataset_name,\n",
    "            time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
