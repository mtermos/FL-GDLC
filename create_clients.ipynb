{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets, cn_measures_type_1, cn_measures_type_2, cn_measures_type_3, cn_measures_type_4, network_features_type_1,network_features_type_2,network_features_type_3,network_features_type_4\n",
    "from src.graph_level_measures import compute_graph_properties\n",
    "from src.add_centralities import add_centralities\n",
    "from src.add_pca_columns import process_clients_with_grouped_pca, evaluate_pca_results, process_clients_with_pca\n",
    "\n",
    "\n",
    "with_sort_timestamp = True\n",
    "undersample_classes = True\n",
    "folder_path = \"temp/\"\n",
    "folder_path_prep = \"temp/preprocessed/\"\n",
    "output_folder = 'datasets/gdlc'\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca3130",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecea5",
   "metadata": {},
   "source": [
    "### Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"./datasets/original/cic_ton_iot.parquet\")\n",
    "# df1 = pd.read_parquet(\"./testing_dfs/cic_ton_iot_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.weak_columns:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854b4573-67e2-4691-be7a-3926173792c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset2.name: cic_ids_2017\n"
     ]
    }
   ],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"./datasets/original/cic_ids_2017.parquet\")\n",
    "# df2 = pd.read_parquet(\"./testing_dfs/cic_ids_2017_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.weak_columns:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.weak_columns)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "### Attacks Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'mitm' 'scanning' 'dos' 'ddos' 'injection' 'password' 'backdoor'\n",
      " 'ransomware' 'xss']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc262cb",
   "metadata": {},
   "source": [
    "renaming some attacks to fit the naming in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'ddos' 'PortScan' 'Bot' 'Infiltration' 'bruteforce' 'xss'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'Bot', 'backdoor', 'PortScan', 'Infiltration', 'dos', 'Heartbleed', 'DoS GoldenEye', 'Benign', 'DoS slowloris', 'ddos', 'Web Attack � Sql Injection', 'bruteforce', 'injection', 'scanning', 'xss', 'SSH-Patator', 'ransomware', 'DoS Hulk', 'DoS Slowhttptest', 'FTP-Patator', 'password', 'mitm'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfea377",
   "metadata": {},
   "source": [
    "### Sorting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1215ca63",
   "metadata": {},
   "source": [
    "### Encoding Attacks into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'Heartbleed', 8: 'Infiltration', 9: 'PortScan', 10: 'SSH-Patator', 11: 'Web Attack � Sql Injection', 12: 'backdoor', 13: 'bruteforce', 14: 'ddos', 15: 'dos', 16: 'injection', 17: 'mitm', 18: 'password', 19: 'ransomware', 20: 'scanning', 21: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1c995b",
   "metadata": {},
   "source": [
    "### Undersampling classes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        2514059\n",
      "xss           2149308\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: ddos\n",
      "==>> class_label: dos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        1257030\n",
      "xss           1074654\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        2265910\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        1132955\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90723f84",
   "metadata": {},
   "source": [
    "### saving labels encodings and datasets properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 3018900,\n",
       " 'num_benign': 1257030,\n",
       " 'percentage_of_benign_records': 41.6386763390639,\n",
       " 'num_attack': 1761870,\n",
       " 'percentage_of_attack_records': 58.3613236609361,\n",
       " 'attacks': ['xss',\n",
       "  'Benign',\n",
       "  'password',\n",
       "  'injection',\n",
       "  'scanning',\n",
       "  'ransomware',\n",
       "  'backdoor',\n",
       "  'mitm',\n",
       "  'dos',\n",
       "  'ddos'],\n",
       " 'number_of_nodes': 109340,\n",
       " 'number_of_edges': 209275}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df1_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "    \n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 1681923,\n",
       " 'num_benign': 1132955,\n",
       " 'percentage_of_benign_records': 67.36069368217213,\n",
       " 'num_attack': 548968,\n",
       " 'percentage_of_attack_records': 32.63930631782787,\n",
       " 'attacks': ['Benign',\n",
       "  'DoS Hulk',\n",
       "  'PortScan',\n",
       "  'SSH-Patator',\n",
       "  'ddos',\n",
       "  'DoS Slowhttptest',\n",
       "  'bruteforce',\n",
       "  'FTP-Patator',\n",
       "  'DoS GoldenEye',\n",
       "  'DoS slowloris',\n",
       "  'Bot',\n",
       "  'xss',\n",
       "  'Infiltration',\n",
       "  'Web Attack � Sql Injection',\n",
       "  'Heartbleed'],\n",
       " 'number_of_nodes': 18364,\n",
       " 'number_of_edges': 93050}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df2_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8ce8b0",
   "metadata": {},
   "source": [
    "# Splitting into Clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cf8bc",
   "metadata": {},
   "source": [
    "### creating main training and testing splits\n",
    "\n",
    "test parts will be concatenated to create the main testing df\n",
    "\n",
    "train parts will be further splitted into clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])\n",
    "\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    # train1[dataset1.timestamp_col] = pd.to_datetime(train1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    train1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "    train2.sort_values(dataset2.timestamp_col, inplace= True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab56873",
   "metadata": {},
   "source": [
    "### Computing graph-level measures (to apply GDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c21e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrateur\\Desktop\\FL-GDLC\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed properties for client_0: {'name': 'client_0', 'number_of_nodes': 51328, 'number_of_edges': 85513, 'max_degree': 25835, 'avg_degree': 3.3320215087281797, 'transitivity': 9.279595167204398e-05, 'density': 6.49175192146079e-05, 'mixing_parameter': 0.40161145089050787}\n",
      "Computed properties for client_1: {'name': 'client_1', 'number_of_nodes': 200, 'number_of_edges': 256, 'max_degree': 112, 'avg_degree': 2.56, 'transitivity': 0.016811003565970453, 'density': 0.012864321608040201, 'mixing_parameter': 0.1796875}\n",
      "Computed properties for client_2: {'name': 'client_2', 'number_of_nodes': 118, 'number_of_edges': 142, 'max_degree': 59, 'avg_degree': 2.406779661016949, 'transitivity': 0.025817555938037865, 'density': 0.020570766333478197, 'mixing_parameter': 0.19014084507042253}\n",
      "Computed properties for client_3: {'name': 'client_3', 'number_of_nodes': 143, 'number_of_edges': 170, 'max_degree': 71, 'avg_degree': 2.3776223776223775, 'transitivity': 0.01805157593123209, 'density': 0.01674381956072097, 'mixing_parameter': 0.15294117647058825}\n",
      "Computed properties for client_4: {'name': 'client_4', 'number_of_nodes': 90525, 'number_of_edges': 106332, 'max_degree': 38474, 'avg_degree': 2.349229494614747, 'transitivity': 6.4607431606213975e-06, 'density': 2.5951454803309037e-05, 'mixing_parameter': 0.14855358687883233}\n",
      "Computed properties for client_5: {'name': 'client_5', 'number_of_nodes': 13149, 'number_of_edges': 34200, 'max_degree': 4173, 'avg_degree': 5.201916495550993, 'transitivity': 0.0007550054338359526, 'density': 0.0003956431773312285, 'mixing_parameter': 0.0016666666666666668}\n",
      "Computed properties for client_6: {'name': 'client_6', 'number_of_nodes': 9814, 'number_of_edges': 21755, 'max_degree': 2441, 'avg_degree': 4.43346240065213, 'transitivity': 0.0012677679704819502, 'density': 0.0004517948028790512, 'mixing_parameter': 0.549620776832912}\n",
      "Computed properties for client_7: {'name': 'client_7', 'number_of_nodes': 9235, 'number_of_edges': 20253, 'max_degree': 2533, 'avg_degree': 4.386139685977261, 'transitivity': 0.00017048222588768213, 'density': 0.0004749988830384731, 'mixing_parameter': 0.5447094257640843}\n",
      "Computed properties for test dataset: {'name': 'test', 'number_of_nodes': 33418, 'number_of_edges': 47631, 'max_degree': 5605, 'avg_degree': 2.850619426656293, 'transitivity': 0.0004773451100764247, 'density': 8.53044685835441e-05, 'mixing_parameter': 0.07591694484684344}\n"
     ]
    }
   ],
   "source": [
    "# split dfs into clients\n",
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "graphs_properties_path = os.path.join(output_folder, 'graphs_properties')\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    data_partition.to_parquet(\n",
    "        folder_path + \"client_{}.parquet\".format(cid))\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        data_partition, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "    properties = compute_graph_properties(G, f\"client_{cid}\", graphs_properties_path)\n",
    "    print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "\n",
    "\n",
    "    \n",
    "test = pd.concat([test1, test2])\n",
    "test.to_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "G_test = nx.from_pandas_edgelist(\n",
    "    test, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "test_properties = compute_graph_properties(G_test, \"test\", graphs_properties_path)\n",
    "\n",
    "print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64944281",
   "metadata": {},
   "source": [
    "### Adding Centralities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de87e0cc",
   "metadata": {},
   "source": [
    "Specifying the centralities to add, according to which branch (type) from the four branches of GDLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2105bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_measures_types = [\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "]\n",
    "\n",
    "network_features_types = [\n",
    "    network_features_type_1,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "    network_features_type_1,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "]\n",
    "\n",
    "# homogeneous clients, using same centralities \n",
    "# cn_measures_type_0 = [\"betweenness\", \"degree\", \"pagerank\"]\n",
    "# network_features_type_0 = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\n",
    "\n",
    "\n",
    "# cn_measures_types = [\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "# ]\n",
    "\n",
    "# network_features_types = [\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a455ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_filenames = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_columns = []\n",
    "def process_dataset(name, path, dataset, cn_measures, network_features):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(folder_path_prep, \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(folder_path_prep,\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # df.dropna(axis=0, how='any', inplace=True)\n",
    "    # df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    columns = add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features)\n",
    "    centralities_columns.append(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be17222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: client_0\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 51328)\n",
      "==>> features_dicts: ('local_betweenness', 51328)\n",
      "==>> features_dicts: ('degree', 51328)\n",
      "==>> features_dicts: ('local_degree', 51328)\n",
      "==>> features_dicts: ('eigenvector', 51328)\n",
      "==>> features_dicts: ('closeness', 51328)\n",
      "==>> features_dicts: ('pagerank', 51328)\n",
      "==>> features_dicts: ('local_pagerank', 51328)\n",
      "==>> features_dicts: ('k_core', 51328)\n",
      "==>> features_dicts: ('k_truss', 51328)\n",
      "==>> features_dicts: ('Comm', 51328)\n",
      "DataFrame written to temp/preprocessed/client_0.parquet\n",
      "Processing dataset: client_1\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 200)\n",
      "==>> features_dicts: ('local_betweenness', 200)\n",
      "==>> features_dicts: ('pagerank', 200)\n",
      "==>> features_dicts: ('local_pagerank', 200)\n",
      "==>> features_dicts: ('k_core', 200)\n",
      "==>> features_dicts: ('k_truss', 200)\n",
      "==>> features_dicts: ('Comm', 194)\n",
      "DataFrame written to temp/preprocessed/client_1.parquet\n",
      "Processing dataset: client_2\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 118)\n",
      "==>> features_dicts: ('local_betweenness', 118)\n",
      "==>> features_dicts: ('pagerank', 118)\n",
      "==>> features_dicts: ('local_pagerank', 118)\n",
      "==>> features_dicts: ('k_core', 118)\n",
      "==>> features_dicts: ('k_truss', 118)\n",
      "==>> features_dicts: ('Comm', 118)\n",
      "DataFrame written to temp/preprocessed/client_2.parquet\n",
      "Processing dataset: client_3\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 143)\n",
      "==>> features_dicts: ('local_betweenness', 143)\n",
      "==>> features_dicts: ('pagerank', 143)\n",
      "==>> features_dicts: ('local_pagerank', 143)\n",
      "==>> features_dicts: ('k_core', 143)\n",
      "==>> features_dicts: ('k_truss', 143)\n",
      "==>> features_dicts: ('Comm', 143)\n",
      "DataFrame written to temp/preprocessed/client_3.parquet\n",
      "Processing dataset: client_4\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 90525)\n",
      "==>> features_dicts: ('local_betweenness', 90525)\n",
      "==>> features_dicts: ('degree', 90525)\n",
      "==>> features_dicts: ('local_degree', 90525)\n",
      "==>> features_dicts: ('eigenvector', 90525)\n",
      "==>> features_dicts: ('closeness', 90525)\n",
      "==>> features_dicts: ('pagerank', 90525)\n",
      "==>> features_dicts: ('local_pagerank', 90525)\n",
      "==>> features_dicts: ('k_core', 90525)\n",
      "==>> features_dicts: ('k_truss', 90525)\n",
      "==>> features_dicts: ('Comm', 90525)\n",
      "DataFrame written to temp/preprocessed/client_4.parquet\n",
      "Processing dataset: client_5\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 13149)\n",
      "==>> features_dicts: ('global_betweenness', 13149)\n",
      "==>> features_dicts: ('degree', 13149)\n",
      "==>> features_dicts: ('global_degree', 13149)\n",
      "==>> features_dicts: ('eigenvector', 13149)\n",
      "==>> features_dicts: ('closeness', 13149)\n",
      "==>> features_dicts: ('pagerank', 13149)\n",
      "==>> features_dicts: ('global_pagerank', 13149)\n",
      "==>> features_dicts: ('k_core', 13149)\n",
      "==>> features_dicts: ('k_truss', 13149)\n",
      "==>> features_dicts: ('mv', 13149)\n",
      "DataFrame written to temp/preprocessed/client_5.parquet\n",
      "Processing dataset: client_6\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 9814)\n",
      "==>> features_dicts: ('local_betweenness', 9814)\n",
      "==>> features_dicts: ('degree', 9814)\n",
      "==>> features_dicts: ('local_degree', 9814)\n",
      "==>> features_dicts: ('eigenvector', 9814)\n",
      "==>> features_dicts: ('closeness', 9814)\n",
      "==>> features_dicts: ('pagerank', 9814)\n",
      "==>> features_dicts: ('local_pagerank', 9814)\n",
      "==>> features_dicts: ('k_core', 9814)\n",
      "==>> features_dicts: ('k_truss', 9814)\n",
      "==>> features_dicts: ('Comm', 9814)\n",
      "DataFrame written to temp/preprocessed/client_6.parquet\n",
      "Processing dataset: client_7\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 9235)\n",
      "==>> features_dicts: ('local_betweenness', 9235)\n",
      "==>> features_dicts: ('degree', 9235)\n",
      "==>> features_dicts: ('local_degree', 9235)\n",
      "==>> features_dicts: ('eigenvector', 9235)\n",
      "==>> features_dicts: ('closeness', 9235)\n",
      "==>> features_dicts: ('pagerank', 9235)\n",
      "==>> features_dicts: ('local_pagerank', 9235)\n",
      "==>> features_dicts: ('k_core', 9235)\n",
      "==>> features_dicts: ('k_truss', 9235)\n",
      "==>> features_dicts: ('Comm', 9235)\n",
      "DataFrame written to temp/preprocessed/client_7.parquet\n",
      "Processing dataset: test\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 33418)\n",
      "==>> features_dicts: ('global_betweenness', 33418)\n",
      "==>> features_dicts: ('degree', 33418)\n",
      "==>> features_dicts: ('global_degree', 33418)\n",
      "==>> features_dicts: ('eigenvector', 33418)\n",
      "==>> features_dicts: ('closeness', 33418)\n",
      "==>> features_dicts: ('pagerank', 33418)\n",
      "==>> features_dicts: ('global_pagerank', 33418)\n",
      "==>> features_dicts: ('k_core', 33418)\n",
      "==>> features_dicts: ('k_truss', 33418)\n",
      "==>> features_dicts: ('mv', 33418)\n",
      "DataFrame written to temp/preprocessed/test.parquet\n"
     ]
    }
   ],
   "source": [
    "process_dataset(\"client_0\", clients_paths[0], datasets[0], cn_measures_types[0], network_features_types[0])\n",
    "process_dataset(\"client_1\", clients_paths[1], datasets[0], cn_measures_types[1], network_features_types[1])\n",
    "process_dataset(\"client_2\", clients_paths[2], datasets[0], cn_measures_types[2], network_features_types[2])\n",
    "process_dataset(\"client_3\", clients_paths[3], datasets[0], cn_measures_types[3], network_features_types[3])\n",
    "process_dataset(\"client_4\", clients_paths[4], datasets[0], cn_measures_types[4], network_features_types[4])\n",
    "process_dataset(\"client_5\", clients_paths[5], datasets[1], cn_measures_types[5], network_features_types[5])\n",
    "process_dataset(\"client_6\", clients_paths[6], datasets[1], cn_measures_types[6], network_features_types[6])\n",
    "process_dataset(\"client_7\", clients_paths[7], datasets[1], cn_measures_types[7], network_features_types[7])\n",
    "process_dataset(\"test\", clients_paths[8], datasets[1], cn_measures_types[8], network_features_types[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a238e436",
   "metadata": {},
   "source": [
    "### Adding PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae855b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features Across All Clients: {'Pkt Len Std', 'Fwd IAT Mean', 'Fwd IAT Min', 'Bwd Pkt Len Mean', 'Pkt Len Mean', 'src_betweenness', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Flow Byts/s', 'Label', 'Bwd Seg Size Avg', 'Down/Up Ratio', 'SYN Flag Cnt', 'Fwd URG Flags', 'CWE Flag Count', 'Tot Bwd Pkts', 'Fwd Byts/b Avg', 'Flow ID', 'Active Std', 'Fwd Seg Size Min', 'URG Flag Cnt', 'Bwd Pkt Len Std', 'Bwd IAT Std', 'Dst IP', 'Flow Pkts/s', 'Src IP', 'Flow IAT Max', 'Idle Mean', 'Bwd Blk Rate Avg', 'RST Flag Cnt', 'FIN Flag Cnt', 'Active Max', 'Fwd Seg Size Avg', 'Bwd IAT Max', 'Idle Max', 'Bwd IAT Tot', 'src_k_core', 'Flow IAT Min', 'Init Bwd Win Byts', 'Bwd IAT Mean', 'Bwd Header Len', 'dst_k_core', 'Pkt Len Max', 'Pkt Len Min', 'Class', 'Fwd Pkt Len Std', 'Bwd URG Flags', 'Flow IAT Mean', 'src_pagerank', 'TotLen Bwd Pkts', 'Fwd Pkt Len Min', 'Dst Port', 'dst_pagerank', 'Fwd Pkts/s', 'src_k_truss', 'Subflow Bwd Byts', 'Fwd Pkt Len Mean', 'Attack', 'Fwd IAT Tot', 'Fwd Pkts/b Avg', 'Idle Min', 'Protocol', 'Bwd Byts/b Avg', 'Tot Fwd Pkts', 'Bwd Pkt Len Min', 'Active Mean', 'ECE Flag Cnt', 'ACK Flag Cnt', 'Bwd IAT Min', 'Active Min', 'Fwd PSH Flags', 'Bwd Pkts/s', 'Flow IAT Std', 'Src Port', 'Fwd IAT Max', 'TotLen Fwd Pkts', 'Fwd Pkt Len Max', 'Fwd Header Len', 'Bwd Pkt Len Max', 'dst_k_truss', 'Pkt Size Avg', 'Pkt Len Var', 'Idle Std', 'Subflow Fwd Pkts', 'Fwd Blk Rate Avg', 'Fwd Act Data Pkts', 'Timestamp', 'Flow Duration', 'Init Fwd Win Byts', 'PSH Flag Cnt', 'Bwd PSH Flags', 'dst_betweenness', 'Bwd Pkts/b Avg'}\n",
      "\n",
      "Unique Feature Set Group 1:\n",
      "Unique Features: {'src_local_degree', 'dst_local_betweenness', 'dst_degree', 'dst_local_pagerank', 'src_closeness', 'dst_eigenvector', 'dst_Comm', 'src_eigenvector', 'src_degree', 'src_local_pagerank', 'src_Comm', 'dst_closeness', 'src_local_betweenness', 'dst_local_degree'}\n",
      "Clients: ['temp/preprocessed/client_0.parquet', 'temp/preprocessed/client_4.parquet', 'temp/preprocessed/client_6.parquet', 'temp/preprocessed/client_7.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 2:\n",
      "Unique Features: {'src_local_pagerank', 'src_Comm', 'dst_local_betweenness', 'dst_local_pagerank', 'src_local_betweenness', 'dst_Comm'}\n",
      "Clients: ['temp/preprocessed/client_1.parquet', 'temp/preprocessed/client_2.parquet', 'temp/preprocessed/client_3.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 3:\n",
      "Unique Features: {'dst_mv', 'dst_degree', 'dst_global_betweenness', 'src_closeness', 'dst_eigenvector', 'dst_global_pagerank', 'src_eigenvector', 'src_degree', 'dst_global_degree', 'src_mv', 'src_global_degree', 'dst_closeness', 'src_global_pagerank', 'src_global_betweenness'}\n",
      "Clients: ['temp/preprocessed/client_5.parquet', 'temp/preprocessed/test.parquet']\n",
      "----------\n",
      "Processed federated PCA for client temp/preprocessed/client_0.parquet, saved to datasets/gdlc\\client_0.parquet\n",
      "Client temp/preprocessed/client_0.parquet Local PCA Reconstruction Error: 21.915587695306442\n",
      "Client temp/preprocessed/client_0.parquet Federated PCA Reconstruction Error: 21.915587695306208\n",
      "Processed federated PCA for client temp/preprocessed/client_4.parquet, saved to datasets/gdlc\\client_4.parquet\n",
      "Client temp/preprocessed/client_4.parquet Local PCA Reconstruction Error: 20.827010867435416\n",
      "Client temp/preprocessed/client_4.parquet Federated PCA Reconstruction Error: 20.827010867435412\n",
      "Processed federated PCA for client temp/preprocessed/client_6.parquet, saved to datasets/gdlc\\client_6.parquet\n",
      "Client temp/preprocessed/client_6.parquet Local PCA Reconstruction Error: 23.85603635054457\n",
      "Client temp/preprocessed/client_6.parquet Federated PCA Reconstruction Error: 23.8560363505446\n",
      "Processed federated PCA for client temp/preprocessed/client_7.parquet, saved to datasets/gdlc\\client_7.parquet\n",
      "Client temp/preprocessed/client_7.parquet Local PCA Reconstruction Error: 22.92608321652365\n",
      "Client temp/preprocessed/client_7.parquet Federated PCA Reconstruction Error: 22.926083216523487\n",
      "Processed federated PCA for client temp/preprocessed/client_1.parquet, saved to datasets/gdlc\\client_1.parquet\n",
      "Client temp/preprocessed/client_1.parquet Local PCA Reconstruction Error: 9.360580457725463\n",
      "Client temp/preprocessed/client_1.parquet Federated PCA Reconstruction Error: 9.360580457725458\n",
      "Processed federated PCA for client temp/preprocessed/client_2.parquet, saved to datasets/gdlc\\client_2.parquet\n",
      "Client temp/preprocessed/client_2.parquet Local PCA Reconstruction Error: 8.375106662870824\n",
      "Client temp/preprocessed/client_2.parquet Federated PCA Reconstruction Error: 8.375106662870824\n",
      "Processed federated PCA for client temp/preprocessed/client_3.parquet, saved to datasets/gdlc\\client_3.parquet\n",
      "Client temp/preprocessed/client_3.parquet Local PCA Reconstruction Error: 9.423302650549402\n",
      "Client temp/preprocessed/client_3.parquet Federated PCA Reconstruction Error: 9.423302650549402\n",
      "Processed federated PCA for client temp/preprocessed/client_5.parquet, saved to datasets/gdlc\\client_5.parquet\n",
      "Client temp/preprocessed/client_5.parquet Local PCA Reconstruction Error: 12.631121412207417\n",
      "Client temp/preprocessed/client_5.parquet Federated PCA Reconstruction Error: 12.631121412207444\n",
      "Processed federated PCA for client temp/preprocessed/test.parquet, saved to datasets/gdlc\\test.parquet\n",
      "Client temp/preprocessed/test.parquet Local PCA Reconstruction Error: 21.875208285518816\n",
      "Client temp/preprocessed/test.parquet Federated PCA Reconstruction Error: 21.875208285518863\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path_prep + \"client_0.parquet\",\n",
    "    folder_path_prep + \"client_1.parquet\",\n",
    "    folder_path_prep + \"client_2.parquet\",\n",
    "    folder_path_prep + \"client_3.parquet\",\n",
    "    folder_path_prep + \"client_4.parquet\",\n",
    "    folder_path_prep + \"client_5.parquet\",\n",
    "    folder_path_prep + \"client_6.parquet\",\n",
    "    folder_path_prep + \"client_7.parquet\",\n",
    "    folder_path_prep + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    client_features[client_path] = features\n",
    "\n",
    "common_features = set.intersection(*client_features.values())\n",
    "print(f\"Common Features Across All Clients: {common_features}\\n\")\n",
    "\n",
    "unique_features = {client_path: features - common_features for client_path, features in client_features.items()}\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "\n",
    "for client_path, features in unique_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Feature Set Group {i}:\")\n",
    "    print(f\"Unique Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "\n",
    "pca_results, pca_columns = process_clients_with_grouped_pca(feature_groups, output_folder, n_components=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fc1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(os.path.join(folder_path, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "with open(output_folder + '/added_columns.pkl', 'wb') as f:\n",
    "    pickle.dump([centralities_columns, pca_columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f3df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients_paths = [\n",
    "#     output_folder + \"/client_0.parquet\",\n",
    "#     output_folder + \"/client_1.parquet\",\n",
    "#     output_folder + \"/client_2.parquet\",\n",
    "#     output_folder + \"/client_3.parquet\",\n",
    "#     output_folder + \"/client_4.parquet\",\n",
    "#     output_folder + \"/client_5.parquet\",\n",
    "#     output_folder + \"/client_6.parquet\",\n",
    "#     output_folder + \"/client_7.parquet\",\n",
    "#     output_folder + \"/test.parquet\"\n",
    "\n",
    "# ]\n",
    "\n",
    "# evaluate_pca_results(clients_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
