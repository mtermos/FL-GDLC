{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets, cn_measures_type_1, cn_measures_type_2, cn_measures_type_3, cn_measures_type_4, network_features_type_1,network_features_type_2,network_features_type_3,network_features_type_4\n",
    "from src.graph_level_measures import compute_graph_properties\n",
    "from src.add_centralities import add_centralities\n",
    "from src.add_pca_columns import process_clients_with_grouped_pca, evaluate_pca_results, process_clients_with_pca\n",
    "\n",
    "\n",
    "with_sort_timestamp = True\n",
    "undersample_classes = True\n",
    "folder_path = \"temp/\"\n",
    "folder_path_prep = \"temp/preprocessed/\"\n",
    "output_folder = 'datasets/gdlc'\n",
    "# output_folder = 'datasets/dbp'\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca3130",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecea5",
   "metadata": {},
   "source": [
    "### Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"./datasets/original/cic_ton_iot.parquet\")\n",
    "# df1 = pd.read_parquet(\"./testing_dfs/cic_ton_iot_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.weak_columns:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854b4573-67e2-4691-be7a-3926173792c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset2.name: cic_ids_2017\n"
     ]
    }
   ],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"./datasets/original/cic_ids_2017.parquet\")\n",
    "# df2 = pd.read_parquet(\"./testing_dfs/cic_ids_2017_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.weak_columns:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.weak_columns)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "### Attacks Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benign' 'mitm' 'scanning' 'dos' 'ddos' 'injection' 'password' 'backdoor'\n",
      " 'ransomware' 'xss']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc262cb",
   "metadata": {},
   "source": [
    "renaming some attacks to fit the naming in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'ddos' 'PortScan' 'Bot' 'Infiltration' 'bruteforce' 'xss'\n",
      " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'SSH-Patator', 'xss', 'Benign', 'Infiltration', 'password', 'Web Attack � Sql Injection', 'PortScan', 'injection', 'DoS GoldenEye', 'Heartbleed', 'ddos', 'ransomware', 'DoS slowloris', 'Bot', 'mitm', 'DoS Slowhttptest', 'backdoor', 'scanning', 'bruteforce', 'dos', 'FTP-Patator', 'DoS Hulk'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfea377",
   "metadata": {},
   "source": [
    "### Sorting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3f48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1215ca63",
   "metadata": {},
   "source": [
    "### Encoding Attacks into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'Heartbleed', 8: 'Infiltration', 9: 'PortScan', 10: 'SSH-Patator', 11: 'Web Attack � Sql Injection', 12: 'backdoor', 13: 'bruteforce', 14: 'ddos', 15: 'dos', 16: 'injection', 17: 'mitm', 18: 'password', 19: 'ransomware', 20: 'scanning', 21: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1c995b",
   "metadata": {},
   "source": [
    "### Undersampling classes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        2514059\n",
      "xss           2149308\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: ddos\n",
      "==>> class_label: dos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        1257030\n",
      "xss           1074654\n",
      "password       340208\n",
      "injection      277696\n",
      "scanning        36205\n",
      "backdoor        27145\n",
      "ransomware       5098\n",
      "mitm              517\n",
      "ddos              202\n",
      "dos               145\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        2265910\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        1132955\n",
      "DoS Hulk                       222563\n",
      "PortScan                       158804\n",
      "ddos                           128025\n",
      "DoS GoldenEye                   10293\n",
      "FTP-Patator                      7935\n",
      "SSH-Patator                      5897\n",
      "DoS slowloris                    5769\n",
      "DoS Slowhttptest                 5499\n",
      "Bot                              1956\n",
      "bruteforce                       1507\n",
      "xss                               652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90723f84",
   "metadata": {},
   "source": [
    "### saving labels encodings and datasets properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 3018900,\n",
       " 'num_benign': 1257030,\n",
       " 'percentage_of_benign_records': 41.6386763390639,\n",
       " 'num_attack': 1761870,\n",
       " 'percentage_of_attack_records': 58.3613236609361,\n",
       " 'attacks': ['password',\n",
       "  'Benign',\n",
       "  'xss',\n",
       "  'scanning',\n",
       "  'injection',\n",
       "  'ransomware',\n",
       "  'backdoor',\n",
       "  'ddos',\n",
       "  'mitm',\n",
       "  'dos'],\n",
       " 'number_of_nodes': 109559,\n",
       " 'number_of_edges': 210367}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df1_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "    \n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 1681923,\n",
       " 'num_benign': 1132955,\n",
       " 'percentage_of_benign_records': 67.36069368217213,\n",
       " 'num_attack': 548968,\n",
       " 'percentage_of_attack_records': 32.63930631782787,\n",
       " 'attacks': ['Benign',\n",
       "  'ddos',\n",
       "  'PortScan',\n",
       "  'DoS Hulk',\n",
       "  'FTP-Patator',\n",
       "  'DoS GoldenEye',\n",
       "  'DoS Slowhttptest',\n",
       "  'DoS slowloris',\n",
       "  'SSH-Patator',\n",
       "  'xss',\n",
       "  'Bot',\n",
       "  'bruteforce',\n",
       "  'Web Attack � Sql Injection',\n",
       "  'Heartbleed',\n",
       "  'Infiltration'],\n",
       " 'number_of_nodes': 18338,\n",
       " 'number_of_edges': 92936}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df2_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8ce8b0",
   "metadata": {},
   "source": [
    "# Splitting into Clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cf8bc",
   "metadata": {},
   "source": [
    "### creating main training and testing splits\n",
    "\n",
    "test parts will be concatenated to create the main testing df\n",
    "\n",
    "train parts will be further splitted into clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])\n",
    "\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    # train1[dataset1.timestamp_col] = pd.to_datetime(train1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    train1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "    train2.sort_values(dataset2.timestamp_col, inplace= True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab56873",
   "metadata": {},
   "source": [
    "### Computing graph-level measures (to apply GDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c21e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrateur\\Desktop\\FL-GDLC\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# split dfs into clients\n",
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "graphs_properties_path = os.path.join(output_folder, 'graphs_properties')\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    data_partition.to_parquet(\n",
    "        folder_path + \"client_{}.parquet\".format(cid))\n",
    "\n",
    "    # G = nx.from_pandas_edgelist(\n",
    "    #     data_partition, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "    # properties = compute_graph_properties(G, f\"client_{cid}\", graphs_properties_path)\n",
    "    # print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "\n",
    "\n",
    "    \n",
    "test = pd.concat([test1, test2])\n",
    "test.to_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "# G_test = nx.from_pandas_edgelist(\n",
    "#     test, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "# test_properties = compute_graph_properties(G_test, \"test\", graphs_properties_path)\n",
    "\n",
    "# print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64944281",
   "metadata": {},
   "source": [
    "### Adding Centralities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de87e0cc",
   "metadata": {},
   "source": [
    "Specifying the centralities to add, according to which branch (type) from the four branches of GDLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2105bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_measures_types = [\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "]\n",
    "\n",
    "network_features_types = [\n",
    "    network_features_type_1,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "    network_features_type_1,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "]\n",
    "\n",
    "# homogeneous clients, using same centralities \n",
    "\n",
    "# cn_measures_type_0 = [\"betweenness\", \"degree\", \"pagerank\"]\n",
    "# network_features_type_0 = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\n",
    "\n",
    "\n",
    "# cn_measures_types = [\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "# ]\n",
    "\n",
    "# network_features_types = [\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a455ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_filenames = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_columns = []\n",
    "def process_dataset(name, path, dataset, cn_measures, network_features):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(folder_path_prep, \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(folder_path_prep,\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # df.dropna(axis=0, how='any', inplace=True)\n",
    "    # df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    columns = add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features)\n",
    "    centralities_columns.append(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be17222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: client_0\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 51484)\n",
      "==>> features_dicts: ('local_betweenness', 51484)\n",
      "==>> features_dicts: ('degree', 51484)\n",
      "==>> features_dicts: ('local_degree', 51484)\n",
      "==>> features_dicts: ('eigenvector', 51484)\n",
      "==>> features_dicts: ('closeness', 51484)\n",
      "==>> features_dicts: ('pagerank', 51484)\n",
      "==>> features_dicts: ('local_pagerank', 51484)\n",
      "==>> features_dicts: ('k_core', 51484)\n",
      "==>> features_dicts: ('k_truss', 51484)\n",
      "==>> features_dicts: ('Comm', 51484)\n",
      "DataFrame written to temp/preprocessed/client_0.parquet\n",
      "Processing dataset: client_1\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 185)\n",
      "==>> features_dicts: ('local_betweenness', 185)\n",
      "==>> features_dicts: ('pagerank', 185)\n",
      "==>> features_dicts: ('local_pagerank', 185)\n",
      "==>> features_dicts: ('k_core', 185)\n",
      "==>> features_dicts: ('k_truss', 185)\n",
      "==>> features_dicts: ('Comm', 183)\n",
      "DataFrame written to temp/preprocessed/client_1.parquet\n",
      "Processing dataset: client_2\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 114)\n",
      "==>> features_dicts: ('local_betweenness', 114)\n",
      "==>> features_dicts: ('pagerank', 114)\n",
      "==>> features_dicts: ('local_pagerank', 114)\n",
      "==>> features_dicts: ('k_core', 114)\n",
      "==>> features_dicts: ('k_truss', 114)\n",
      "==>> features_dicts: ('Comm', 114)\n",
      "DataFrame written to temp/preprocessed/client_2.parquet\n",
      "Processing dataset: client_3\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 152)\n",
      "==>> features_dicts: ('local_betweenness', 152)\n",
      "==>> features_dicts: ('pagerank', 152)\n",
      "==>> features_dicts: ('local_pagerank', 152)\n",
      "==>> features_dicts: ('k_core', 152)\n",
      "==>> features_dicts: ('k_truss', 152)\n",
      "==>> features_dicts: ('Comm', 152)\n",
      "DataFrame written to temp/preprocessed/client_3.parquet\n",
      "Processing dataset: client_4\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 90978)\n",
      "==>> features_dicts: ('local_betweenness', 90978)\n",
      "==>> features_dicts: ('degree', 90978)\n",
      "==>> features_dicts: ('local_degree', 90978)\n",
      "==>> features_dicts: ('eigenvector', 90978)\n",
      "==>> features_dicts: ('closeness', 90978)\n",
      "==>> features_dicts: ('pagerank', 90978)\n",
      "==>> features_dicts: ('local_pagerank', 90978)\n",
      "==>> features_dicts: ('k_core', 90978)\n",
      "==>> features_dicts: ('k_truss', 90978)\n",
      "==>> features_dicts: ('Comm', 90978)\n",
      "DataFrame written to temp/preprocessed/client_4.parquet\n",
      "Processing dataset: client_5\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 13071)\n",
      "==>> features_dicts: ('global_betweenness', 13071)\n",
      "==>> features_dicts: ('degree', 13071)\n",
      "==>> features_dicts: ('global_degree', 13071)\n",
      "==>> features_dicts: ('eigenvector', 13071)\n",
      "==>> features_dicts: ('closeness', 13071)\n",
      "==>> features_dicts: ('pagerank', 13071)\n",
      "==>> features_dicts: ('global_pagerank', 13071)\n",
      "==>> features_dicts: ('k_core', 13071)\n",
      "==>> features_dicts: ('k_truss', 13071)\n",
      "==>> features_dicts: ('mv', 13071)\n",
      "DataFrame written to temp/preprocessed/client_5.parquet\n",
      "Processing dataset: client_6\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 9831)\n",
      "==>> features_dicts: ('local_betweenness', 9831)\n",
      "==>> features_dicts: ('degree', 9831)\n",
      "==>> features_dicts: ('local_degree', 9831)\n",
      "==>> features_dicts: ('eigenvector', 9831)\n",
      "==>> features_dicts: ('closeness', 9831)\n",
      "==>> features_dicts: ('pagerank', 9831)\n",
      "==>> features_dicts: ('local_pagerank', 9831)\n",
      "==>> features_dicts: ('k_core', 9831)\n",
      "==>> features_dicts: ('k_truss', 9831)\n",
      "==>> features_dicts: ('Comm', 9831)\n",
      "DataFrame written to temp/preprocessed/client_6.parquet\n",
      "Processing dataset: client_7\n",
      "calculated betweenness\n",
      "calculated local_betweenness\n",
      "calculated degree\n",
      "calculated local_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated local_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated Comm\n",
      "==>> features_dicts: ('betweenness', 9254)\n",
      "==>> features_dicts: ('local_betweenness', 9254)\n",
      "==>> features_dicts: ('degree', 9254)\n",
      "==>> features_dicts: ('local_degree', 9254)\n",
      "==>> features_dicts: ('eigenvector', 9254)\n",
      "==>> features_dicts: ('closeness', 9254)\n",
      "==>> features_dicts: ('pagerank', 9254)\n",
      "==>> features_dicts: ('local_pagerank', 9254)\n",
      "==>> features_dicts: ('k_core', 9254)\n",
      "==>> features_dicts: ('k_truss', 9254)\n",
      "==>> features_dicts: ('Comm', 9254)\n",
      "DataFrame written to temp/preprocessed/client_7.parquet\n",
      "Processing dataset: test\n",
      "calculated betweenness\n",
      "calculated global_betweenness\n",
      "calculated degree\n",
      "calculated global_degree\n",
      "calculated eigenvector\n",
      "calculated closeness\n",
      "calculated pagerank\n",
      "calculated global_pagerank\n",
      "calculated k_core\n",
      "calculated k_truss\n",
      "calculated mv\n",
      "==>> features_dicts: ('betweenness', 33467)\n",
      "==>> features_dicts: ('global_betweenness', 33467)\n",
      "==>> features_dicts: ('degree', 33467)\n",
      "==>> features_dicts: ('global_degree', 33467)\n",
      "==>> features_dicts: ('eigenvector', 33467)\n",
      "==>> features_dicts: ('closeness', 33467)\n",
      "==>> features_dicts: ('pagerank', 33467)\n",
      "==>> features_dicts: ('global_pagerank', 33467)\n",
      "==>> features_dicts: ('k_core', 33467)\n",
      "==>> features_dicts: ('k_truss', 33467)\n",
      "==>> features_dicts: ('mv', 33467)\n",
      "DataFrame written to temp/preprocessed/test.parquet\n"
     ]
    }
   ],
   "source": [
    "process_dataset(\"client_0\", clients_paths[0], datasets[0], cn_measures_types[0], network_features_types[0])\n",
    "process_dataset(\"client_1\", clients_paths[1], datasets[0], cn_measures_types[1], network_features_types[1])\n",
    "process_dataset(\"client_2\", clients_paths[2], datasets[0], cn_measures_types[2], network_features_types[2])\n",
    "process_dataset(\"client_3\", clients_paths[3], datasets[0], cn_measures_types[3], network_features_types[3])\n",
    "process_dataset(\"client_4\", clients_paths[4], datasets[0], cn_measures_types[4], network_features_types[4])\n",
    "process_dataset(\"client_5\", clients_paths[5], datasets[1], cn_measures_types[5], network_features_types[5])\n",
    "process_dataset(\"client_6\", clients_paths[6], datasets[1], cn_measures_types[6], network_features_types[6])\n",
    "process_dataset(\"client_7\", clients_paths[7], datasets[1], cn_measures_types[7], network_features_types[7])\n",
    "process_dataset(\"test\", clients_paths[8], datasets[1], cn_measures_types[8], network_features_types[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a238e436",
   "metadata": {},
   "source": [
    "### Adding PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae855b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features Across All Clients: {'Bwd Header Len', 'Pkt Len Max', 'Flow IAT Mean', 'Fwd Byts/b Avg', 'FIN Flag Cnt', 'RST Flag Cnt', 'Subflow Bwd Pkts', 'Bwd Seg Size Avg', 'Flow IAT Std', 'Bwd URG Flags', 'Pkt Len Min', 'Active Std', 'Bwd IAT Tot', 'Bwd Pkt Len Mean', 'Fwd IAT Mean', 'Timestamp', 'Idle Min', 'Flow IAT Max', 'dst_k_core', 'dst_pagerank', 'Tot Bwd Pkts', 'Fwd Blk Rate Avg', 'Bwd Pkt Len Std', 'Idle Std', 'ACK Flag Cnt', 'Bwd PSH Flags', 'Flow Byts/s', 'Fwd Seg Size Min', 'Tot Fwd Pkts', 'dst_k_truss', 'Protocol', 'src_k_core', 'Fwd Act Data Pkts', 'Active Mean', 'Src IP', 'Down/Up Ratio', 'Pkt Len Var', 'Idle Max', 'dst_betweenness', 'Fwd IAT Max', 'Attack', 'Dst Port', 'Fwd IAT Tot', 'Flow ID', 'src_k_truss', 'Flow Pkts/s', 'Bwd Pkts/b Avg', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Fwd Pkt Len Mean', 'Bwd Pkt Len Max', 'Dst IP', 'Active Max', 'Fwd Pkt Len Min', 'Flow IAT Min', 'Fwd Pkt Len Max', 'Pkt Len Std', 'PSH Flag Cnt', 'Bwd Byts/b Avg', 'Fwd Seg Size Avg', 'Flow Duration', 'Bwd IAT Std', 'Class', 'Bwd IAT Min', 'Fwd Pkt Len Std', 'src_betweenness', 'Fwd PSH Flags', 'Fwd Pkts/b Avg', 'Fwd Pkts/s', 'Bwd Pkt Len Min', 'ECE Flag Cnt', 'Fwd Header Len', 'src_pagerank', 'Src Port', 'Bwd Blk Rate Avg', 'Idle Mean', 'Init Bwd Win Byts', 'Bwd IAT Mean', 'Bwd Pkts/s', 'TotLen Fwd Pkts', 'URG Flag Cnt', 'Bwd IAT Max', 'Fwd URG Flags', 'Init Fwd Win Byts', 'Active Min', 'Subflow Fwd Pkts', 'Pkt Size Avg', 'CWE Flag Count', 'Label', 'TotLen Bwd Pkts', 'Pkt Len Mean', 'Fwd IAT Min', 'SYN Flag Cnt'}\n",
      "\n",
      "Unique Feature Set Group 1:\n",
      "Unique Features: {'dst_local_degree', 'src_local_degree', 'src_closeness', 'src_eigenvector', 'dst_local_betweenness', 'dst_local_pagerank', 'dst_degree', 'src_local_pagerank', 'src_Comm', 'dst_Comm', 'src_local_betweenness', 'dst_eigenvector', 'src_degree', 'dst_closeness'}\n",
      "Clients: ['temp/preprocessed/client_0.parquet', 'temp/preprocessed/client_4.parquet', 'temp/preprocessed/client_6.parquet', 'temp/preprocessed/client_7.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 2:\n",
      "Unique Features: {'src_local_pagerank', 'dst_Comm', 'src_local_betweenness', 'dst_local_betweenness', 'dst_local_pagerank', 'src_Comm'}\n",
      "Clients: ['temp/preprocessed/client_1.parquet', 'temp/preprocessed/client_2.parquet', 'temp/preprocessed/client_3.parquet']\n",
      "----------\n",
      "Unique Feature Set Group 3:\n",
      "Unique Features: {'dst_mv', 'dst_global_betweenness', 'src_global_betweenness', 'src_global_degree', 'src_closeness', 'src_mv', 'src_eigenvector', 'dst_global_degree', 'dst_degree', 'dst_global_pagerank', 'src_global_pagerank', 'dst_eigenvector', 'src_degree', 'dst_closeness'}\n",
      "Clients: ['temp/preprocessed/client_5.parquet', 'temp/preprocessed/test.parquet']\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['dst_local_degree', 'src_local_degree', 'src_closeness', 'src_eigenvector', 'dst_degree', 'dst_eigenvector', 'src_degree', 'dst_closeness'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClients: \u001b[39m\u001b[39m{\u001b[39;00mclients\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m pca_results, pca_columns \u001b[39m=\u001b[39m process_clients_with_grouped_pca(feature_groups, output_folder, n_components\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\FL-GDLC\\src\\add_pca_columns.py:163\u001b[0m, in \u001b[0;36mprocess_clients_with_grouped_pca\u001b[1;34m(feature_groups, output_folder, n_components, client_cn_measures)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo centrality measures found in client \u001b[39m\u001b[39m{\u001b[39;00mclient_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m centrality_data_pca, explained_variance, mean, scale, pca_components \u001b[39m=\u001b[39m calculate_local_pca(\n\u001b[0;32m    164\u001b[0m     df, client_cn_measures, n_components)\n\u001b[0;32m    165\u001b[0m all_local_pca_results\u001b[39m.\u001b[39mappend(centrality_data_pca)\n\u001b[0;32m    167\u001b[0m local_explained_variances[client_path] \u001b[39m=\u001b[39m explained_variance\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\FL-GDLC\\src\\add_pca_columns.py:124\u001b[0m, in \u001b[0;36mprocess_clients_with_grouped_pca.<locals>.calculate_local_pca\u001b[1;34m(df, cn_measures, n_components)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_local_pca\u001b[39m(df, cn_measures, n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m     centrality_data \u001b[39m=\u001b[39m df[cn_measures]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[0;32m    125\u001b[0m     scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m    126\u001b[0m     centrality_data_std \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(centrality_data)\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\FL-GDLC\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\FL-GDLC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\FL-GDLC\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['dst_local_degree', 'src_local_degree', 'src_closeness', 'src_eigenvector', 'dst_degree', 'dst_eigenvector', 'src_degree', 'dst_closeness'] not in index\""
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path_prep + \"client_0.parquet\",\n",
    "    folder_path_prep + \"client_1.parquet\",\n",
    "    folder_path_prep + \"client_2.parquet\",\n",
    "    folder_path_prep + \"client_3.parquet\",\n",
    "    folder_path_prep + \"client_4.parquet\",\n",
    "    folder_path_prep + \"client_5.parquet\",\n",
    "    folder_path_prep + \"client_6.parquet\",\n",
    "    folder_path_prep + \"client_7.parquet\",\n",
    "    folder_path_prep + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    client_features[client_path] = features\n",
    "\n",
    "common_features = set.intersection(*client_features.values())\n",
    "print(f\"Common Features Across All Clients: {common_features}\\n\")\n",
    "\n",
    "unique_features = {client_path: features - common_features for client_path, features in client_features.items()}\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "\n",
    "for client_path, features in unique_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Feature Set Group {i}:\")\n",
    "    print(f\"Unique Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "\n",
    "pca_results, pca_columns = process_clients_with_grouped_pca(feature_groups, output_folder, n_components=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(os.path.join(folder_path, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "with open(output_folder + '/added_columns.pkl', 'wb') as f:\n",
    "    pickle.dump([centralities_columns, pca_columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients_paths = [\n",
    "#     output_folder + \"/client_0.parquet\",\n",
    "#     output_folder + \"/client_1.parquet\",\n",
    "#     output_folder + \"/client_2.parquet\",\n",
    "#     output_folder + \"/client_3.parquet\",\n",
    "#     output_folder + \"/client_4.parquet\",\n",
    "#     output_folder + \"/client_5.parquet\",\n",
    "#     output_folder + \"/client_6.parquet\",\n",
    "#     output_folder + \"/client_7.parquet\",\n",
    "#     output_folder + \"/test.parquet\"\n",
    "\n",
    "# ]\n",
    "\n",
    "# evaluate_pca_results(clients_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
