{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets, cn_measures_type_1, cn_measures_type_2, cn_measures_type_3, cn_measures_type_4, network_features_type_1,network_features_type_2,network_features_type_3,network_features_type_4\n",
    "from src.graph_level_measures import compute_graph_properties\n",
    "from src.add_centralities import add_centralities\n",
    "from src.add_pca_columns import process_clients_with_grouped_pca, evaluate_pca_results, process_clients_with_pca, process_clients_with_grouped_pca_rmse\n",
    "\n",
    "\n",
    "with_sort_timestamp = True\n",
    "undersample_classes = True\n",
    "folder_path = \"temp/\"\n",
    "folder_path_prep = \"temp/preprocessed/\"\n",
    "output_folder = 'datasets/gdlc'\n",
    "# output_folder = 'datasets/dbp'\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca3130",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecea5",
   "metadata": {},
   "source": [
    "### Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "df1 = pd.read_parquet(\"./datasets/original/cic_ton_iot.parquet\")\n",
    "# df1 = pd.read_parquet(\"./testing_dfs/cic_ton_iot_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.weak_columns:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b4573-67e2-4691-be7a-3926173792c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "df2 = pd.read_parquet(\"./datasets/original/cic_ids_2017.parquet\")\n",
    "# df2 = pd.read_parquet(\"./testing_dfs/cic_ids_2017_5_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.weak_columns:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.weak_columns)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "### Attacks Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc262cb",
   "metadata": {},
   "source": [
    "renaming some attacks to fit the naming in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfea377",
   "metadata": {},
   "source": [
    "### Sorting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1215ca63",
   "metadata": {},
   "source": [
    "### Encoding Attacks into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1c995b",
   "metadata": {},
   "source": [
    "### Undersampling classes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90723f84",
   "metadata": {},
   "source": [
    "### saving labels encodings and datasets properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df1_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "    \n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df2_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8ce8b0",
   "metadata": {},
   "source": [
    "# Splitting into Clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cf8bc",
   "metadata": {},
   "source": [
    "### creating main training and testing splits\n",
    "\n",
    "test parts will be concatenated to create the main testing df\n",
    "\n",
    "train parts will be further splitted into clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])\n",
    "\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    # train1[dataset1.timestamp_col] = pd.to_datetime(train1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    train1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "    train2.sort_values(dataset2.timestamp_col, inplace= True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab56873",
   "metadata": {},
   "source": [
    "### Computing graph-level measures (to apply GDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c21e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dfs into clients\n",
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "graphs_properties_path = os.path.join(output_folder, 'graphs_properties')\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    data_partition.to_parquet(\n",
    "        folder_path + \"client_{}.parquet\".format(cid))\n",
    "\n",
    "    # G = nx.from_pandas_edgelist(\n",
    "    #     data_partition, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "    # properties = compute_graph_properties(G, f\"client_{cid}\", graphs_properties_path)\n",
    "    # print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "\n",
    "\n",
    "    \n",
    "test = pd.concat([test1, test2])\n",
    "test.to_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "# G_test = nx.from_pandas_edgelist(\n",
    "#     test, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "# test_properties = compute_graph_properties(G_test, \"test\", graphs_properties_path)\n",
    "\n",
    "# print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64944281",
   "metadata": {},
   "source": [
    "### Adding Centralities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de87e0cc",
   "metadata": {},
   "source": [
    "Specifying the centralities to add, according to which branch (type) from the four branches of GDLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_measures_types = [\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "]\n",
    "\n",
    "network_features_types = [\n",
    "    network_features_type_1,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "    network_features_type_1,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "]\n",
    "\n",
    "# homogeneous clients, using same centralities \n",
    "\n",
    "# cn_measures_type_0 = [\"betweenness\", \"degree\", \"pagerank\"]\n",
    "# network_features_type_0 = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\n",
    "\n",
    "\n",
    "# cn_measures_types = [\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "# ]\n",
    "\n",
    "# network_features_types = [\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a455ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_filenames = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralities_columns = []\n",
    "def process_dataset(name, path, dataset, cn_measures, network_features):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(folder_path_prep, \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(folder_path_prep,\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # df.dropna(axis=0, how='any', inplace=True)\n",
    "    # df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    columns = add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features)\n",
    "    centralities_columns.append(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\"client_0\", clients_paths[0], datasets[0], cn_measures_types[0], network_features_types[0])\n",
    "process_dataset(\"client_1\", clients_paths[1], datasets[0], cn_measures_types[1], network_features_types[1])\n",
    "process_dataset(\"client_2\", clients_paths[2], datasets[0], cn_measures_types[2], network_features_types[2])\n",
    "process_dataset(\"client_3\", clients_paths[3], datasets[0], cn_measures_types[3], network_features_types[3])\n",
    "process_dataset(\"client_4\", clients_paths[4], datasets[0], cn_measures_types[4], network_features_types[4])\n",
    "process_dataset(\"client_5\", clients_paths[5], datasets[1], cn_measures_types[5], network_features_types[5])\n",
    "process_dataset(\"client_6\", clients_paths[6], datasets[1], cn_measures_types[6], network_features_types[6])\n",
    "process_dataset(\"client_7\", clients_paths[7], datasets[1], cn_measures_types[7], network_features_types[7])\n",
    "process_dataset(\"test\", clients_paths[8], datasets[1], cn_measures_types[8], network_features_types[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a238e436",
   "metadata": {},
   "source": [
    "### Adding PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae855b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_centrality_measures = set([\n",
    "    'src_degree', 'dst_degree', 'dst_betweenness', 'pagerank', 'dst_local_pagerank',\n",
    "    'src_k_core', 'dst_mv', 'global_betweenness', 'src_local_pagerank', 'dst_local_degree',\n",
    "    'dst_global_pagerank', 'betweenness', 'local_pagerank', 'eigenvector', 'src_global_degree',\n",
    "    'src_global_betweenness', 'src_closeness', 'global_pagerank', 'local_betweenness',\n",
    "    'dst_eigenvector', 'dst_global_betweenness', 'src_global_pagerank', 'k_truss', 'global_degree',\n",
    "    'src_local_degree', 'degree', 'dst_closeness', 'dst_k_truss', 'dst_global_degree', 'dst_pagerank',\n",
    "    'local_degree', 'src_pagerank', 'src_Comm', 'dst_local_betweenness', 'src_k_truss', 'dst_k_core',\n",
    "    'closeness', 'src_mv', 'Comm', 'src_eigenvector', 'dst_Comm', 'src_betweenness', 'k_core',\n",
    "    'src_local_betweenness', 'mv'\n",
    "])\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path_prep + \"client_0.parquet\",\n",
    "    folder_path_prep + \"client_1.parquet\",\n",
    "    folder_path_prep + \"client_2.parquet\",\n",
    "    folder_path_prep + \"client_3.parquet\",\n",
    "    folder_path_prep + \"client_4.parquet\",\n",
    "    folder_path_prep + \"client_5.parquet\",\n",
    "    folder_path_prep + \"client_6.parquet\",\n",
    "    folder_path_prep + \"client_7.parquet\",\n",
    "    folder_path_prep + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    centrality_features = features.intersection(all_centrality_measures)\n",
    "    client_features[client_path] = centrality_features\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "for client_path, features in client_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Centrality Feature Set Group {i}:\")\n",
    "    print(f\"Centrality Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "pca_results, pca_columns = process_clients_with_grouped_pca_rmse(\n",
    "    feature_groups,\n",
    "    output_folder,\n",
    "    n_components=7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(os.path.join(folder_path, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "with open(output_folder + '/added_columns.pkl', 'wb') as f:\n",
    "    pickle.dump([centralities_columns, pca_columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients_paths = [\n",
    "#     output_folder + \"/client_0.parquet\",\n",
    "#     output_folder + \"/client_1.parquet\",\n",
    "#     output_folder + \"/client_2.parquet\",\n",
    "#     output_folder + \"/client_3.parquet\",\n",
    "#     output_folder + \"/client_4.parquet\",\n",
    "#     output_folder + \"/client_5.parquet\",\n",
    "#     output_folder + \"/client_6.parquet\",\n",
    "#     output_folder + \"/client_7.parquet\",\n",
    "#     output_folder + \"/test.parquet\"\n",
    "\n",
    "# ]\n",
    "\n",
    "# evaluate_pca_results(clients_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
