{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cb9753-9962-4cfa-b78f-ed13216e3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# datasets is a list of available datasets descriptions containing: path, key columns names, and suitable complex network features\n",
    "from src.data.dataset_info import datasets, cn_measures_type_1, cn_measures_type_2, cn_measures_type_3, cn_measures_type_4, network_features_type_1,network_features_type_2,network_features_type_3,network_features_type_4\n",
    "from src.graph_level_measures import compute_graph_properties\n",
    "from src.add_centralities import add_centralities\n",
    "from src.add_pca_columns import process_clients_with_grouped_pca, evaluate_pca_results, process_clients_with_pca\n",
    "\n",
    "\n",
    "with_sort_timestamp = True\n",
    "undersample_classes = True\n",
    "folder_path = \"temp/\"\n",
    "folder_path_prep = \"temp/preprocessed/\"\n",
    "output_folder = 'datasets/bdp'\n",
    "\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.mkdir(folder_path)\n",
    "    os.mkdir(folder_path_prep)\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ca3130",
   "metadata": {},
   "source": [
    "# Preparing Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43cecea5",
   "metadata": {},
   "source": [
    "### Reading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8e652c-7d74-47a3-9faa-7da0f290d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset1.name: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "dataset1 = datasets[0]\n",
    "print(f\"==>> dataset1.name: {dataset1.name}\")\n",
    "# df1 = pd.read_parquet(\"./datasets/cic_ton_iot.parquet\")\n",
    "df1 = pd.read_parquet(\"./datasets/original/cic_ton_iot_1_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df1.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df1.drop_duplicates(subset=list(set(df1.columns) - set([dataset1.timestamp_col, dataset1.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset1.weak_columns:\n",
    "    df1 = df1[~df1[dataset1.class_col].isin(dataset1.weak_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854b4573-67e2-4691-be7a-3926173792c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> dataset2.name: cic_ids_2017\n"
     ]
    }
   ],
   "source": [
    "dataset2 = datasets[1]\n",
    "print(f\"==>> dataset2.name: {dataset2.name}\")\n",
    "# df2 = pd.read_parquet(\"./datasets/cic_ids_2017.parquet\")\n",
    "df2 = pd.read_parquet(\"./datasets/original/cic_ids_2017_1_percent.parquet\")\n",
    "# converting all infinity values into nan then dropping all records containing nan values\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "df2.drop_duplicates(subset=list(set(df2.columns) - set([dataset2.timestamp_col, dataset2.flow_id_col])), keep=\"first\", inplace=True)\n",
    "\n",
    "if dataset2.weak_columns:\n",
    "    df2 = df2[~df2[dataset2.class_col].isin(dataset2.weak_columns)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b147094b",
   "metadata": {},
   "source": [
    "### Attacks Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98616b2-7166-4702-a1ae-c00ab83f4f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xss' 'Benign' 'password' 'injection' 'backdoor' 'scanning' 'ransomware'\n",
      " 'ddos' 'mitm']\n"
     ]
    }
   ],
   "source": [
    "classes1 = df1[dataset1.class_col].unique()\n",
    "print(classes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd28659f-d720-4b8b-ac2f-aa4eb5dd279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['BENIGN' 'PortScan' 'DoS Hulk' 'FTP-Patator' 'DDoS' 'Bot' 'DoS GoldenEye'\n",
      " 'DoS slowloris' 'SSH-Patator' 'DoS Slowhttptest'\n",
      " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
      " 'Web Attack � Sql Injection']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adc262cb",
   "metadata": {},
   "source": [
    "renaming some attacks to fit the naming in df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c52211-e107-49d3-a5a3-a2d942abad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[dataset2.class_col] = df2[dataset2.class_col].replace({\"BENIGN\": \"Benign\",\n",
    "                                                            \"DDoS\": \"ddos\",\n",
    "                                                            \"Web Attack � Brute Force\": \"bruteforce\",\n",
    "                                                            \"Web Attack � XSS\": \"xss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93132189-7c75-48e0-a13a-d2a75f463ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes2: ['Benign' 'PortScan' 'DoS Hulk' 'FTP-Patator' 'ddos' 'Bot' 'DoS GoldenEye'\n",
      " 'DoS slowloris' 'SSH-Patator' 'DoS Slowhttptest' 'bruteforce' 'xss'\n",
      " 'Web Attack � Sql Injection']\n"
     ]
    }
   ],
   "source": [
    "classes2 = df2[dataset2.class_col].unique()\n",
    "print(f\"==>> classes2: {classes2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bd293c-061e-4e4d-968f-28d459bea765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes: {'scanning', 'ransomware', 'DoS Slowhttptest', 'xss', 'bruteforce', 'injection', 'ddos', 'mitm', 'SSH-Patator', 'DoS GoldenEye', 'FTP-Patator', 'password', 'Web Attack � Sql Injection', 'backdoor', 'Bot', 'PortScan', 'DoS slowloris', 'DoS Hulk', 'Benign'}\n"
     ]
    }
   ],
   "source": [
    "classes = set(np.concatenate([classes2,classes1]))\n",
    "print(f\"==>> classes: {classes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfea377",
   "metadata": {},
   "source": [
    "### Sorting (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f48d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_sort_timestamp:\n",
    "    df1[dataset1.timestamp_col] = pd.to_datetime(df1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    df1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    df2[dataset2.timestamp_col] = pd.to_datetime(df2[dataset2.timestamp_col].str.strip(), format=dataset2.timestamp_format)\n",
    "    df2.sort_values(dataset2.timestamp_col, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1215ca63",
   "metadata": {},
   "source": [
    "### Encoding Attacks into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b820edb5-7f77-4a02-8305-9491e3c8a980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> labels_names: {0: 'Benign', 1: 'Bot', 2: 'DoS GoldenEye', 3: 'DoS Hulk', 4: 'DoS Slowhttptest', 5: 'DoS slowloris', 6: 'FTP-Patator', 7: 'PortScan', 8: 'SSH-Patator', 9: 'Web Attack � Sql Injection', 10: 'backdoor', 11: 'bruteforce', 12: 'ddos', 13: 'injection', 14: 'mitm', 15: 'password', 16: 'ransomware', 17: 'scanning', 18: 'xss'}\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(list(classes))\n",
    "\n",
    "df1[dataset1.class_num_col] = label_encoder.transform(df1[dataset1.class_col])\n",
    "df2[dataset2.class_num_col] = label_encoder.transform(df2[dataset2.class_col])\n",
    "labels_names = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))\n",
    "\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1c995b",
   "metadata": {},
   "source": [
    "### Undersampling classes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cbab61-3bd1-43c1-892c-4db0c150f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        25154\n",
      "xss           21487\n",
      "password       3476\n",
      "injection      2717\n",
      "scanning        347\n",
      "backdoor        268\n",
      "ransomware       48\n",
      "mitm              4\n",
      "ddos              3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fd9f7c-aa56-477a-9f24-4b9c4bf63826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_label: Benign\n",
      "==>> class_label: xss\n",
      "==>> class_label: password\n",
      "==>> class_label: injection\n",
      "==>> class_label: scanning\n",
      "==>> class_label: backdoor\n",
      "==>> class_label: ransomware\n",
      "==>> class_label: mitm\n",
      "==>> class_label: ddos\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:2]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        print(f\"==>> class_label: {class_label}\")\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df1[df1[dataset1.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df1[df1[dataset1.class_col] == class_label])\n",
    "\n",
    "    df1 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df1 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eceb2754-3b56-467b-8470-9ab1c832f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign        12577\n",
      "xss           10744\n",
      "password       3476\n",
      "injection      2717\n",
      "scanning        347\n",
      "backdoor        268\n",
      "ransomware       48\n",
      "mitm              4\n",
      "ddos              3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df1.groupby(dataset1.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322651d3-96ef-4ba0-aa6a-dfadd19030d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        22664\n",
      "DoS Hulk                       2301\n",
      "PortScan                       1535\n",
      "ddos                           1326\n",
      "DoS GoldenEye                    92\n",
      "FTP-Patator                      86\n",
      "DoS Slowhttptest                 61\n",
      "SSH-Patator                      61\n",
      "DoS slowloris                    52\n",
      "Bot                              28\n",
      "bruteforce                       13\n",
      "xss                               7\n",
      "Web Attack � Sql Injection        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0459c6e2-6a02-40f9-a26c-273311f10e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if undersample_classes:\n",
    "    # Get the classes with the highest number of records (you can choose how many to undersample)\n",
    "    classes_to_undersample = class_counts_sorted.index[:1]\n",
    "\n",
    "    # Undersample the classes with the highest number of records\n",
    "    dfs = []\n",
    "    for class_label in class_counts_sorted.index:\n",
    "        if class_label in classes_to_undersample:\n",
    "            class_df = df2[df2[dataset2.class_col] == class_label]\n",
    "            undersampled_df = class_df.sample(frac=0.5)  # Specify the fraction of samples to keep\n",
    "            dfs.append(undersampled_df)\n",
    "        else:\n",
    "            dfs.append(df2[df2[dataset2.class_col] == class_label])\n",
    "\n",
    "    df2 = []\n",
    "    # Optional: shuffle the undersampled DataFrame\n",
    "    df2 = pd.concat(dfs).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210b63b7-cc3d-4b7b-921e-62ce34426204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> class_counts_sorted: Attack\n",
      "Benign                        11332\n",
      "DoS Hulk                       2301\n",
      "PortScan                       1535\n",
      "ddos                           1326\n",
      "DoS GoldenEye                    92\n",
      "FTP-Patator                      86\n",
      "DoS Slowhttptest                 61\n",
      "SSH-Patator                      61\n",
      "DoS slowloris                    52\n",
      "Bot                              28\n",
      "bruteforce                       13\n",
      "xss                               7\n",
      "Web Attack � Sql Injection        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if undersample_classes:\n",
    "    # Group by the class column and get the count of records in each class\n",
    "    class_counts = df2.groupby(dataset2.class_col).size()\n",
    "\n",
    "    # Sort the counts in descending order\n",
    "    class_counts_sorted = class_counts.sort_values(ascending=False)\n",
    "    print(f\"==>> class_counts_sorted: {class_counts_sorted}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90723f84",
   "metadata": {},
   "source": [
    "### saving labels encodings and datasets properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e2c1f3-facf-492a-a6b4-6ef70cb008eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + '/labels_names.pkl', 'wb') as f:\n",
    "    pickle.dump([labels_names, classes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b4c519-7e2a-4441-8101-73615e352968",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m properties[\u001b[39m\"\u001b[39m\u001b[39mnumber_of_edges\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m G\u001b[39m.\u001b[39mnumber_of_edges()\n\u001b[0;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(folder_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/df1_properties.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 31\u001b[0m     json\u001b[39m.\u001b[39;49mdump(properties, f)\n\u001b[0;32m     33\u001b[0m properties\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:180\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m--> 180\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(chunk)\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "total_count = len(df1)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df1[df1['Label'] == 0])\n",
    "num_attack = len(df1[df1['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df1[\"Attack\"].unique()) \n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df1,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df1_properties.txt', 'w') as f:\n",
    "    json.dump(properties, f)\n",
    "    \n",
    "properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18d60a-c1bf-47af-8b06-72c19a2d63d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> number_of_nodes: 18360\n",
      "==>> number_of_edges: 93248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'cic_ton_iot',\n",
       " 'length': 1681923,\n",
       " 'num_benign': 1132955,\n",
       " 'percentage_of_benign_records': 67.36069368217213,\n",
       " 'num_attack': 548968,\n",
       " 'percentage_of_attack_records': 32.63930631782787,\n",
       " 'attacks': ['Benign',\n",
       "  'ddos',\n",
       "  'PortScan',\n",
       "  'DoS Hulk',\n",
       "  'FTP-Patator',\n",
       "  'Bot',\n",
       "  'DoS GoldenEye',\n",
       "  'SSH-Patator',\n",
       "  'DoS Slowhttptest',\n",
       "  'DoS slowloris',\n",
       "  'bruteforce',\n",
       "  'xss',\n",
       "  'Heartbleed',\n",
       "  'Infiltration',\n",
       "  'Web Attack � Sql Injection']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = len(df2)\n",
    "\n",
    "properties = {\n",
    "    \"name\": dataset1.name,\n",
    "    \"length\": total_count,\n",
    "}\n",
    "\n",
    "num_benign = len(df2[df2['Label'] == 0])\n",
    "num_attack = len(df2[df2['Label'] == 1])\n",
    "\n",
    "properties[\"num_benign\"] = num_benign\n",
    "\n",
    "properties[\"percentage_of_benign_records\"] = ((num_benign * 100)/total_count)\n",
    "\n",
    "properties[\"num_attack\"] = num_attack\n",
    "properties[\"percentage_of_attack_records\"] = ((num_attack * 100)/total_count)\n",
    "\n",
    "properties[\"attacks\"] = list(df2[\"Attack\"].unique())  # .to_list()\n",
    "\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    df2,\n",
    "    source=dataset1.src_ip_col,\n",
    "    target=dataset1.dst_ip_col,\n",
    "    create_using=nx.DiGraph()\n",
    ")\n",
    "\n",
    "properties[\"number_of_nodes\"] = G.number_of_nodes() \n",
    "properties[\"number_of_edges\"] = G.number_of_edges()\n",
    "\n",
    "with open(folder_path + '/df2_properties.pkl', 'wb') as f:\n",
    "    json.dump(properties, f)\n",
    "\n",
    "properties\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8ce8b0",
   "metadata": {},
   "source": [
    "# Splitting into Clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e10cf8bc",
   "metadata": {},
   "source": [
    "### creating main training and testing splits\n",
    "\n",
    "test parts will be concatenated to create the main testing df\n",
    "\n",
    "train parts will be further splitted into clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff965d8-b49c-4bee-80a4-fc522d9d7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(df1, test_size=0.1, shuffle= True, random_state=1, stratify=df1[dataset1.class_col])\n",
    "train2, test2 = train_test_split(df2, test_size=0.1, shuffle= True, random_state=1, stratify=df2[dataset2.class_col])\n",
    "\n",
    "\n",
    "if with_sort_timestamp:\n",
    "    # train1[dataset1.timestamp_col] = pd.to_datetime(train1[dataset1.timestamp_col].str.strip(), format=dataset1.timestamp_format)\n",
    "    train1.sort_values(dataset1.timestamp_col, inplace= True)\n",
    "    train2.sort_values(dataset2.timestamp_col, inplace= True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ab56873",
   "metadata": {},
   "source": [
    "### Computing graph-level measures (to apply GDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c21e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dfs into clients\n",
    "client_data = np.array_split(train1, 5) + np.array_split(train2, 3)\n",
    "\n",
    "graphs_properties_path = os.path.join(output_folder, 'graphs_properties')\n",
    "\n",
    "for cid, data_partition in enumerate(client_data):\n",
    "    data_partition.to_parquet(\n",
    "        folder_path + \"client_{}.parquet\".format(cid))\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        data_partition, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "    properties = compute_graph_properties(G, f\"client_{cid}\", graphs_properties_path)\n",
    "    print(f\"Computed properties for client_{cid}: {properties}\")\n",
    "\n",
    "\n",
    "    \n",
    "test = pd.concat([test1, test2])\n",
    "test.to_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "G_test = nx.from_pandas_edgelist(\n",
    "    test, source=dataset1.src_ip_col, target=dataset1.dst_ip_col, create_using=nx.Graph())\n",
    "test_properties = compute_graph_properties(G_test, \"test\", graphs_properties_path)\n",
    "\n",
    "print(f\"Computed properties for test dataset: {test_properties}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64944281",
   "metadata": {},
   "source": [
    "### Adding Centralities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de87e0cc",
   "metadata": {},
   "source": [
    "Specifying the centralities to add, according to which branch (type) from the four branches of GDLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_measures_types = [\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_3,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_1,\n",
    "    cn_measures_type_2,\n",
    "]\n",
    "\n",
    "network_features_types = [\n",
    "    network_features_type_1,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_3,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "    network_features_type_1,\n",
    "    network_features_type_1,\n",
    "    network_features_type_2,\n",
    "]\n",
    "\n",
    "# homogeneous clients, using same centralities \n",
    "# cn_measures_type_0 = [\"betweenness\", \"degree\", \"pagerank\"]\n",
    "# network_features_type_0 = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']\n",
    "\n",
    "\n",
    "# cn_measures_types = [\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "#     cn_measures_type_0,\n",
    "# ]\n",
    "\n",
    "# network_features_types = [\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "#     network_features_type_0,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a455ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_filenames = [\n",
    "    \"client_0.parquet\",\n",
    "    \"client_1.parquet\",\n",
    "    \"client_2.parquet\",\n",
    "    \"client_3.parquet\",\n",
    "    \"client_4.parquet\",\n",
    "    \"client_5.parquet\",\n",
    "    \"client_6.parquet\",\n",
    "    \"client_7.parquet\",\n",
    "    \"test.parquet\"\n",
    "]\n",
    "clients_paths = [os.path.join(folder_path, name) for name in client_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(name, path, dataset, cn_measures, network_features):\n",
    "    print(\"Processing dataset: {}\".format(name))\n",
    "    new_path = os.path.join(folder_path_prep, \"{}.parquet\".format(name))\n",
    "    graph_path = os.path.join(folder_path_prep,\"graphs\",\"graph_{}.gexf\".format(name))\n",
    "    os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(graph_path), exist_ok=True)\n",
    "    \n",
    "    df = pd.read_parquet(path)\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # df.dropna(axis=0, how='any', inplace=True)\n",
    "    # df.drop_duplicates(subset=list(set(df.columns) - set([dataset.timestamp_col, dataset.flow_id_col])), keep=\"first\", inplace=True)\n",
    "    \n",
    "    add_centralities(df, new_path, graph_path, dataset, cn_measures, network_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\"client_0\", clients_paths[0], datasets[0], cn_measures_types[0], network_features_types[0])\n",
    "process_dataset(\"client_1\", clients_paths[1], datasets[0], cn_measures_types[1], network_features_types[1])\n",
    "process_dataset(\"client_2\", clients_paths[2], datasets[0], cn_measures_types[2], network_features_types[2])\n",
    "process_dataset(\"client_3\", clients_paths[3], datasets[0], cn_measures_types[3], network_features_types[3])\n",
    "process_dataset(\"client_4\", clients_paths[4], datasets[0], cn_measures_types[4], network_features_types[4])\n",
    "process_dataset(\"client_5\", clients_paths[5], datasets[1], cn_measures_types[5], network_features_types[5])\n",
    "process_dataset(\"client_6\", clients_paths[6], datasets[1], cn_measures_types[6], network_features_types[6])\n",
    "process_dataset(\"client_7\", clients_paths[7], datasets[1], cn_measures_types[7], network_features_types[7])\n",
    "process_dataset(\"test\", clients_paths[8], datasets[1], cn_measures_types[8], network_features_types[8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a238e436",
   "metadata": {},
   "source": [
    "### Adding PCA columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae855b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "clients_paths = [\n",
    "    folder_path + \"client_0.parquet\",\n",
    "    folder_path + \"client_1.parquet\",\n",
    "    folder_path + \"client_2.parquet\",\n",
    "    folder_path + \"client_3.parquet\",\n",
    "    folder_path + \"client_4.parquet\",\n",
    "    folder_path + \"client_5.parquet\",\n",
    "    folder_path + \"client_6.parquet\",\n",
    "    folder_path + \"client_7.parquet\",\n",
    "    folder_path + \"test.parquet\",\n",
    "]\n",
    "\n",
    "client_features = {}\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    features = set(df.columns)\n",
    "    client_features[client_path] = features\n",
    "\n",
    "common_features = set.intersection(*client_features.values())\n",
    "print(f\"Common Features Across All Clients: {common_features}\\n\")\n",
    "\n",
    "unique_features = {client_path: features - common_features for client_path, features in client_features.items()}\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "\n",
    "for client_path, features in unique_features.items():\n",
    "    feature_groups[frozenset(features)].append(client_path)\n",
    "\n",
    "for i, (unique_feature_set, clients) in enumerate(feature_groups.items(), 1):\n",
    "    print(f\"Unique Feature Set Group {i}:\")\n",
    "    print(f\"Unique Features: {set(unique_feature_set)}\")\n",
    "    print(f\"Clients: {clients}\")\n",
    "    print(\"----------\")\n",
    "\n",
    "\n",
    "pca_results = process_clients_with_grouped_pca(feature_groups, output_folder, n_components=2)\n",
    "shutil.copy(os.path.join(folder_path, 'labels_names.pkl'), os.path.join(output_folder, '/labels_names.pkl'))\n",
    "clients_paths = [\n",
    "    output_folder + \"/client_0_global_pca.parquet\",\n",
    "    output_folder + \"/client_1_global_pca.parquet\",\n",
    "    output_folder + \"/client_2_global_pca.parquet\",\n",
    "    output_folder + \"/client_3_global_pca.parquet\",\n",
    "    output_folder + \"/client_4_global_pca.parquet\",\n",
    "    output_folder + \"/client_5_global_pca.parquet\",\n",
    "    output_folder + \"/client_6_global_pca.parquet\",\n",
    "    output_folder + \"/client_7_global_pca.parquet\",\n",
    "    output_folder + \"/test_global_pca.parquet\"\n",
    "\n",
    "]\n",
    "\n",
    "#evaluate_pca_results(clients_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
