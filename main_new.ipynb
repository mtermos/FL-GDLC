{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_class: false\n",
      "with_network_features: false\n",
      "n_clients: 5\n",
      "n_rounds: 20\n",
      "config_fit:\n",
      "  lr: 0.01\n",
      "  momentum: 0.9\n",
      "  local_epochs: 1\n",
      "  batch_size: 256\n",
      "\n",
      "dataset: cic_ton_iot\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from logging import INFO, DEBUG\n",
    "from flwr.common.logger import log\n",
    "\n",
    "\n",
    "from src.models.evaluation_metrics import custom_acc_mc, custom_acc_binary\n",
    "\n",
    "from src.data.dataset_info import datasets\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# choosing the dataset\n",
    "dataset = datasets[0]\n",
    "print(\"dataset: {}\".format(dataset.name))\n",
    "# folder_path = \"./fl_from_2_datasets_pca/\"\n",
    "# folder_path = \"./datasets_pca_federated_new/\"\n",
    "folder_path = \"./datasets_bdp/\"\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBD_1 = 0.0001\n",
    "LAMBD_2 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240815-211133'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_paths = [\n",
    "    folder_path + \"test.parquet\",\n",
    "    folder_path + \"client_0_pca.parquet\",\n",
    "    folder_path + \"client_1_pca.parquet\",\n",
    "    folder_path + \"client_2_pca.parquet\",\n",
    "    folder_path + \"client_3_pca.parquet\",\n",
    "    folder_path + \"client_4_pca.parquet\",\n",
    "    folder_path + \"client_5_pca.parquet\",\n",
    "    folder_path + \"client_6_pca.parquet\",\n",
    "    folder_path + \"client_7_pca.parquet\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intersection of columns across all clients: {'Flow Byts/s', 'Pkt Len Min', 'TotLen Bwd Pkts', 'Tot Bwd Pkts', 'ACK Flag Cnt', 'pca_1', 'Fwd Pkts/b Avg', 'Active Mean', 'Timestamp', 'Bwd Pkts/b Avg', 'dst_betweenness', 'Bwd Pkt Len Mean', 'src_betweenness', 'src_pagerank', 'Tot Fwd Pkts', 'Bwd Blk Rate Avg', 'Fwd Blk Rate Avg', 'Idle Mean', 'Flow ID', 'Attack', 'Pkt Len Std', 'Fwd Seg Size Min', 'Fwd PSH Flags', 'Subflow Fwd Byts', 'Bwd Header Len', 'Idle Min', 'Bwd Pkt Len Min', 'src_degree', 'Flow IAT Max', 'Label', 'Pkt Len Var', 'Init Fwd Win Byts', 'RST Flag Cnt', 'Bwd Pkt Len Max', 'Fwd Header Len', 'FIN Flag Cnt', 'Src IP', 'Class', 'Active Std', 'PSH Flag Cnt', 'Bwd Byts/b Avg', 'Dst Port', 'Bwd Pkt Len Std', 'Fwd Pkts/s', 'Fwd URG Flags', 'Fwd Pkt Len Min', 'dst_degree', 'Bwd URG Flags', 'Bwd IAT Tot', 'Fwd Act Data Pkts', 'Fwd IAT Mean', 'Active Min', 'Protocol', 'Down/Up Ratio', 'Idle Max', 'Pkt Size Avg', 'Bwd IAT Max', 'Flow IAT Min', 'CWE Flag Count', 'Fwd Seg Size Avg', 'Dst IP', 'Flow IAT Mean', 'Fwd Byts/b Avg', 'Init Bwd Win Byts', 'Subflow Fwd Pkts', 'SYN Flag Cnt', 'Bwd PSH Flags', 'Idle Std', 'Fwd IAT Max', 'Fwd Pkt Len Max', 'Fwd IAT Min', 'Fwd IAT Tot', 'Flow IAT Std', 'ECE Flag Cnt', 'Pkt Len Max', 'TotLen Fwd Pkts', 'Flow Pkts/s', 'Bwd IAT Min', 'Fwd Pkt Len Std', 'URG Flag Cnt', 'Src Port', 'Bwd IAT Mean', 'Fwd Pkt Len Mean', 'Active Max', 'dst_pagerank', 'Bwd Pkts/s', 'Bwd Seg Size Avg', 'Subflow Bwd Byts', 'Bwd IAT Std', 'Subflow Bwd Pkts', 'pca_2', 'Flow Duration', 'Pkt Len Mean'}\n",
      "Difference in columns for client 0: set()\n",
      "Difference in columns for client 1: set()\n",
      "Difference in columns for client 2: set()\n",
      "Difference in columns for client 3: set()\n",
      "Difference in columns for client 4: set()\n",
      "Difference in columns for client 5: set()\n",
      "Difference in columns for client 6: set()\n",
      "Difference in columns for client 7: set()\n",
      "Difference in columns for client 8: set()\n"
     ]
    }
   ],
   "source": [
    "clients_paths = [\n",
    "    folder_path + \"test.parquet\",\n",
    "    folder_path + \"client_0_pca.parquet\",\n",
    "    folder_path + \"client_1_pca.parquet\",\n",
    "    folder_path + \"client_2_pca.parquet\",\n",
    "    folder_path + \"client_3_pca.parquet\",\n",
    "    folder_path + \"client_4_pca.parquet\",\n",
    "    folder_path + \"client_5_pca.parquet\",\n",
    "    folder_path + \"client_6_pca.parquet\",\n",
    "    folder_path + \"client_7_pca.parquet\"\n",
    "]\n",
    "client_columns = []\n",
    "\n",
    "for client_path in clients_paths:\n",
    "    df = pd.read_parquet(client_path)\n",
    "    client_columns.append(set(df.columns))\n",
    "\n",
    "common_columns = set.intersection(*client_columns)\n",
    "\n",
    "differences = [columns - common_columns for columns in client_columns]\n",
    "\n",
    "#for idx, columns in enumerate(client_columns):\n",
    " #   print(f\"Client {idx} columns: {columns}\")\n",
    "\n",
    "print(f\"\\nIntersection of columns across all clients: {common_columns}\")\n",
    "\n",
    "for idx, diff in enumerate(differences):\n",
    "    print(f\"Difference in columns for client {idx}: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_2 = [\n",
    "    \"dst_global_betweenness\",\n",
    "    \"src_global_degree\",\n",
    "    \"dst_global_degree\",\n",
    "    \"src_mv\",\n",
    "    \"src_global_pagerank\",\n",
    "    \"dst_global_pagerank\",\n",
    "    \"src_global_betweenness\",\n",
    "    \"dst_mv\"\n",
    "]\n",
    "\n",
    "cn_1 = [\n",
    "    \"dst_local_pagerank\",\n",
    "    \"src_local_betweenness\",\n",
    "    \"src_Comm\",\n",
    "    \"src_local_degree\",\n",
    "    \"dst_local_betweenness\",\n",
    "    \"dst_Comm\",\n",
    "    \"dst_local_degree\",\n",
    "    \"src_local_pagerank\"\n",
    "]\n",
    "\n",
    "for i, client_path in enumerate(clients_paths):\n",
    "    # Determine which set of columns to drop based on the client index\n",
    "    if i < 5:\n",
    "        drop_columns = cn_2\n",
    "    else:\n",
    "        drop_columns = cn_1\n",
    "    df = pd.read_parquet(client_path)\n",
    "    df.drop(columns=drop_columns, errors='ignore', inplace=True)\n",
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "df.drop(columns=cn_2, errors='ignore', inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> classes_set: {'benign', 'attack'}\n",
      "==>> num_classes: 2\n",
      "==>> labels_names: {0: 'benign', 1: 'attack'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# the input dimension of the training set\n",
    "# input_dim = df.shape[1] - len(drop_columns) - len(weak_columns) - 1  # for the label_column\n",
    "  \n",
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "classes_set = {\"benign\", \"attack\"}\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "num_classes = 2\n",
    "if cfg.multi_class:\n",
    "    with open(folder_path + \"labels_names.pkl\", 'rb') as f:\n",
    "        labels_names, classes_set = pickle.load(f)\n",
    "    num_classes = len(classes_set)\n",
    "    \n",
    "labels_names = {int(k): v for k, v in labels_names.items()}\n",
    "\n",
    "print(f\"==>> classes_set: {classes_set}\")\n",
    "print(f\"==>> num_classes: {num_classes}\")\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets_bdp/test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(folder_path + \"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flow ID</th>\n",
       "      <th>Src IP</th>\n",
       "      <th>Src Port</th>\n",
       "      <th>Dst IP</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Tot Bwd Pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Class</th>\n",
       "      <th>src_betweenness</th>\n",
       "      <th>dst_betweenness</th>\n",
       "      <th>src_degree</th>\n",
       "      <th>dst_degree</th>\n",
       "      <th>src_pagerank</th>\n",
       "      <th>dst_pagerank</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117477</th>\n",
       "      <td>192.168.1.190-192.168.1.35-80-34692-6</td>\n",
       "      <td>192.168.1.190</td>\n",
       "      <td>80.0</td>\n",
       "      <td>192.168.1.35</td>\n",
       "      <td>34692.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-04-27 21:12:03</td>\n",
       "      <td>166030.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "      <td>1.667824e-03</td>\n",
       "      <td>9.059850e-05</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>-0.220379</td>\n",
       "      <td>-0.561945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488690</th>\n",
       "      <td>192.168.1.195-192.168.1.79-58678-9197-6</td>\n",
       "      <td>192.168.1.195</td>\n",
       "      <td>58678.0</td>\n",
       "      <td>192.168.1.79</td>\n",
       "      <td>9197.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-04-27 17:08:13</td>\n",
       "      <td>363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "      <td>2.104518e-03</td>\n",
       "      <td>8.135874e-06</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.206620</td>\n",
       "      <td>-0.639181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495319</th>\n",
       "      <td>192.168.1.35-192.168.1.195-42260-80-6</td>\n",
       "      <td>192.168.1.35</td>\n",
       "      <td>42260.0</td>\n",
       "      <td>192.168.1.195</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-04-27 21:10:12</td>\n",
       "      <td>25084.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>injection</td>\n",
       "      <td>16</td>\n",
       "      <td>9.059850e-05</td>\n",
       "      <td>2.104518e-03</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.664609</td>\n",
       "      <td>-0.414151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69616</th>\n",
       "      <td>192.168.1.39-18.194.169.124-58026-80-6</td>\n",
       "      <td>192.168.1.39</td>\n",
       "      <td>58026.0</td>\n",
       "      <td>18.194.169.124</td>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-04-27 21:33:17</td>\n",
       "      <td>268.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xss</td>\n",
       "      <td>21</td>\n",
       "      <td>6.762077e-05</td>\n",
       "      <td>7.539369e-09</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.157520</td>\n",
       "      <td>-0.983877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499459</th>\n",
       "      <td>52.28.231.150-192.168.1.39-80-47584-6</td>\n",
       "      <td>52.28.231.150</td>\n",
       "      <td>80.0</td>\n",
       "      <td>192.168.1.39</td>\n",
       "      <td>47584.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-04-27 20:03:05</td>\n",
       "      <td>751556.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Benign</td>\n",
       "      <td>0</td>\n",
       "      <td>7.539369e-09</td>\n",
       "      <td>6.762077e-05</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.206437</td>\n",
       "      <td>-0.981731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Flow ID         Src IP  Src Port  \\\n",
       "117477     192.168.1.190-192.168.1.35-80-34692-6  192.168.1.190      80.0   \n",
       "488690   192.168.1.195-192.168.1.79-58678-9197-6  192.168.1.195   58678.0   \n",
       "1495319    192.168.1.35-192.168.1.195-42260-80-6   192.168.1.35   42260.0   \n",
       "69616     192.168.1.39-18.194.169.124-58026-80-6   192.168.1.39   58026.0   \n",
       "2499459    52.28.231.150-192.168.1.39-80-47584-6  52.28.231.150      80.0   \n",
       "\n",
       "                 Dst IP  Dst Port  Protocol           Timestamp  \\\n",
       "117477     192.168.1.35   34692.0       6.0 2019-04-27 21:12:03   \n",
       "488690     192.168.1.79    9197.0       6.0 2019-04-27 17:08:13   \n",
       "1495319   192.168.1.195      80.0       6.0 2019-04-27 21:10:12   \n",
       "69616    18.194.169.124      80.0       6.0 2019-04-27 21:33:17   \n",
       "2499459    192.168.1.39   47584.0       6.0 2019-04-27 20:03:05   \n",
       "\n",
       "         Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  ...     Attack  Class  \\\n",
       "117477        166030.0           4.0           4.0  ...     Benign      0   \n",
       "488690           363.0           2.0           0.0  ...     Benign      0   \n",
       "1495319        25084.0           5.0           4.0  ...  injection     16   \n",
       "69616            268.0           2.0           0.0  ...        xss     21   \n",
       "2499459       751556.0           7.0           7.0  ...     Benign      0   \n",
       "\n",
       "         src_betweenness  dst_betweenness  src_degree  dst_degree  \\\n",
       "117477      1.667824e-03     9.059850e-05    0.009442    0.000745   \n",
       "488690      2.104518e-03     8.135874e-06    0.002145    0.000238   \n",
       "1495319     9.059850e-05     2.104518e-03    0.000745    0.002145   \n",
       "69616       6.762077e-05     7.539369e-09    0.000655    0.000238   \n",
       "2499459     7.539369e-09     6.762077e-05    0.000238    0.000655   \n",
       "\n",
       "         src_pagerank  dst_pagerank     pca_1     pca_2  \n",
       "117477       0.000300      0.000133 -0.220379 -0.561945  \n",
       "488690       0.000316      0.000047 -0.206620 -0.639181  \n",
       "1495319      0.000133      0.000316  0.664609 -0.414151  \n",
       "69616        0.000128      0.000042  0.157520 -0.983877  \n",
       "2499459      0.000042      0.000128  0.206437 -0.981731  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "# test = pd.read_parquet(\"fl_from_2_datasets/preprocessed/test.parquet\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "# test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "test.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    # cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "    cdata.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, Input, regularizers, callbacks, metrics, optimizers, initializers\n",
    "# from src.models.evaluation_metrics import f1_m\n",
    "\n",
    "def create_keras_model(input_shape, alpha = learning_rate):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv1D(80, kernel_size=3,\n",
    "                activation=\"relu\", input_shape=(input_shape, 1), kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    # .L1L2(l1=LAMBD_1, l2=LAMBD_2)\n",
    "    model.add(layers.Conv1D(80, 3, activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    # model.add(layers.LSTM(units=80,\n",
    "    #                         activation='relu',\n",
    "    #                         kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         recurrent_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         bias_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         return_sequences=False,\n",
    "    #                         ))\n",
    "    # model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(200,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(200,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(80,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "\n",
    "    if cfg.multi_class:\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">128,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">81</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │            \u001b[38;5;34m36\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m80\u001b[0m)         │        \u001b[38;5;34m19,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m80\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m80\u001b[0m)          │            \u001b[38;5;34m16\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m640\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m128,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m40,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │           \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │        \u001b[38;5;34m16,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m81\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,173</span> (801.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m205,173\u001b[0m (801.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,173</span> (801.46 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,173\u001b[0m (801.46 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = {}\n",
    "\n",
    "results_final[\"model\"] = {}\n",
    "\n",
    "results_final[\"baseline\"] = {}\n",
    "results_final[\"baseline\"][\"accuracy\"] = {}\n",
    "results_final[\"baseline\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"centralities - PCA\"] = {}\n",
    "results_final[\"centralities - PCA\"][\"accuracy\"] = {}\n",
    "results_final[\"centralities - PCA\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"centralities - DiGraph\"] = {}\n",
    "results_final[\"centralities - DiGraph\"][\"accuracy\"] = {}\n",
    "results_final[\"centralities - DiGraph\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"centralities - MultiDiGraph\"] = {}\n",
    "results_final[\"centralities - MultiDiGraph\"][\"accuracy\"] = {}\n",
    "results_final[\"centralities - MultiDiGraph\"][\"f1s\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configuration': '2dt - baseline',\n",
       " 'dtime': '20240815-211133',\n",
       " 'multi_class': False,\n",
       " 'learning_rate': 0.0001,\n",
       " 'dataset_name': 'cic_ton_iot',\n",
       " 'num_classes': 2,\n",
       " 'labels_names': {0: 'benign', 1: 'attack'},\n",
       " 'input_dim': 38,\n",
       " 'scores': {'server': {},\n",
       "  'clients': {},\n",
       "  'accuracy': {},\n",
       "  'f1s': {},\n",
       "  'test_by_class': {'accuracy': {'Benign': {},\n",
       "    'injection': {},\n",
       "    'xss': {},\n",
       "    'password': {},\n",
       "    'backdoor': {},\n",
       "    'ransomware': {},\n",
       "    'scanning': {},\n",
       "    'ddos': {},\n",
       "    'mitm': {},\n",
       "    'dos': {},\n",
       "    'DoS Hulk': {},\n",
       "    'DoS GoldenEye': {},\n",
       "    'PortScan': {},\n",
       "    'DoS slowloris': {},\n",
       "    'FTP-Patator': {},\n",
       "    'SSH-Patator': {},\n",
       "    'Bot': {},\n",
       "    'DoS Slowhttptest': {},\n",
       "    'bruteforce': {},\n",
       "    'Infiltration': {},\n",
       "    'Web Attack � Sql Injection': {},\n",
       "    'Heartbleed': {}},\n",
       "   'f1s': {'Benign': {},\n",
       "    'injection': {},\n",
       "    'xss': {},\n",
       "    'password': {},\n",
       "    'backdoor': {},\n",
       "    'ransomware': {},\n",
       "    'scanning': {},\n",
       "    'ddos': {},\n",
       "    'mitm': {},\n",
       "    'dos': {},\n",
       "    'DoS Hulk': {},\n",
       "    'DoS GoldenEye': {},\n",
       "    'PortScan': {},\n",
       "    'DoS slowloris': {},\n",
       "    'FTP-Patator': {},\n",
       "    'SSH-Patator': {},\n",
       "    'Bot': {},\n",
       "    'DoS Slowhttptest': {},\n",
       "    'bruteforce': {},\n",
       "    'Infiltration': {},\n",
       "    'Web Attack � Sql Injection': {},\n",
       "    'Heartbleed': {}},\n",
       "   'length': 1}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - baseline\"\n",
    "results[\"dtime\"] = dtime\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/baseline/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/baseline/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"baseline\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"baseline\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "    results\n",
    "    return metrics\n",
    "    # total_examples = 0\n",
    "    # federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    # for num_examples, m in metrics:\n",
    "    #     for k, v in m.items():\n",
    "    #         federated_metrics[k] += num_examples * v\n",
    "    #     total_examples += num_examples\n",
    "    # return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    # fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n",
    "\n",
    "# strategy.aggregate_fit = \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-08-15 21:37:28,167 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n",
      "2024-08-15 21:37:33,476\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-08-15 21:37:36,070 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 32.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 12640101580.0, 'memory': 25280203163.0, 'node:__internal_head__': 1.0}\n",
      "INFO flwr 2024-08-15 21:37:36,072 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-08-15 21:37:36,074 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 3, 'num_gpus': 0.0}\n",
      "INFO flwr 2024-08-15 21:37:36,099 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 10 actors\n",
      "INFO flwr 2024-08-15 21:37:36,101 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-08-15 21:37:36,102 | server.py:276 | Requesting initial parameters from one random client\n",
      "\u001b[2m\u001b[36m(pid=18312)\u001b[0m 2024-08-15 21:37:38.001219: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=16304)\u001b[0m 2024-08-15 21:37:38.051789: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the e\n",
      "\u001b[2m\u001b[36m(pid=16304)\u001b[0m nvironment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(pid=19660)\u001b[0m 2024-08-15 21:37:39.257108: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off erro\n",
      "\u001b[2m\u001b[36m(pid=19660)\u001b[0m rs f\n",
      "\u001b[2m\u001b[36m(pid=19660)\u001b[0m ro\n",
      "\u001b[2m\u001b[36m(pid=19660)\u001b[0m m different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=19900)\u001b[0m c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=19900)\u001b[0m   super().__init__(\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=19900)\u001b[0m 2024-08-15 21:37:41.571783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=19900)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO flwr 2024-08-15 21:37:41,785 | server.py:280 | Received initial parameters from one random client\n",
      "INFO flwr 2024-08-15 21:37:41,786 | server.py:91 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1837/1837\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - accuracy: 0.3558 - loss: 1.7025\n",
      "\u001b[1m1837/1837\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:142: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TPR = TP/(TP+FN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:152: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FNR = FN/(TP+FN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  TNR = TN/(TN+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:146: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  PPV = TP/(TP+FP)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:150: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FPR = FP/(FP+TN)\n",
      "c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\src\\models\\evaluation_metrics.py:154: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  FDR = FP/(TP+FP)\n",
      "INFO flwr 2024-08-15 21:39:01,880 | 589012468.py:68 | ==>> scores: {'accuracy': 0.43806519274255823, 'recall': 0.43806519274255823, 'precision': 0.41787846497284115, 'f1s': 0.40735971600095916, 'FPR': 0.3373486918355307, 'FNR': 0.7942133596441121, 'class_report': '              precision    recall  f1-score   support\\n\\n           0       0.46      0.66      0.55    238999\\n           1       0.37      0.21      0.26    231084\\n\\n    accuracy                           0.44    470083\\n   macro avg       0.42      0.43      0.40    470083\\nweighted avg       0.42      0.44      0.41    470083\\n'}\n",
      "INFO flwr 2024-08-15 21:39:01,893 | server.py:94 | initial parameters (loss, other metrics): 1.6247950792312622, {'accuracy': 0.43806520104408264, 'f1s': 0.40735971600095916, 'FPR': 0.3373486918355307, 'FNR': 0.7942133596441121}\n",
      "INFO flwr 2024-08-15 21:39:01,895 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-08-15 21:39:01,897 | server.py:222 | fit_round 1: strategy sampled 9 clients (out of 9)\n",
      "\u001b[2m\u001b[36m(pid=8372)\u001b[0m 2024-08-15 21:37:39.302111: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\u001b[32m [repeated 17x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=17676)\u001b[0m c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=17676)\u001b[0m   super().__init__(\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=17676)\u001b[0m 2024-08-15 21:39:03.964966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=17676)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16304)\u001b[0m c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16304)\u001b[0m   super().__init__(\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16304)\u001b[0m 2024-08-15 21:39:04.954263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=16304)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=8372)\u001b[0m c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=8372)\u001b[0m   super().__init__(\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=8372)\u001b[0m 2024-08-15 21:39:34.293494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=8372)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=12516)\u001b[0m c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=12516)\u001b[0m   super().__init__(\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=12516)\u001b[0m 2024-08-15 21:40:04.681970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=12516)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "DEBUG flwr 2024-08-15 21:40:57,258 | server.py:236 | fit_round 1 received 9 results and 0 failures\n",
      "ERROR flwr 2024-08-15 21:40:57,282 | app.py:313 | unhashable type: 'dict'\n",
      "ERROR flwr 2024-08-15 21:40:57,284 | app.py:314 | Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\simulation\\app.py\", line 308, in start_simulation\n",
      "    hist = run_fl(\n",
      "           ^^^^^^^\n",
      "  File \"c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\app.py\", line 225, in run_fl\n",
      "    hist = server.fit(num_rounds=config.num_rounds, timeout=config.round_timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\server.py\", line 117, in fit\n",
      "    history.add_metrics_distributed_fit(\n",
      "  File \"c:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\history.py\", line 49, in add_metrics_distributed_fit\n",
      "    if key not in self.metrics_distributed_fit:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'dict'\n",
      "\n",
      "ERROR flwr 2024-08-15 21:40:57,285 | app.py:315 | Your simulation crashed :(. This could be because of several reasons. The most common are: \n",
      "\t > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: \n",
      "\t\t - You might be using a class attribute in your clients that hasn't been defined.\n",
      "\t\t - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).\n",
      "\t\t - The return types of methods in your clients/strategies might be incorrect.\n",
      "\t > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n",
      "\t > All the actors in your pool crashed. This could be because: \n",
      "\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 3, 'num_gpus': 0.0} is not enough for your run). Use fewer concurrent actors. \n",
      "\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 3, 'num_gpus': 0.0}.\n",
      "Take a look at the Flower simulation examples for guidance <https://flower.dev/docs/framework/how-to-run-simulations.html>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>> weighted_average: [(489061, {'accuracy': 0.941207766532898, 'loss': 0.7245796918869019, 'val_accuracy': 0.9493752121925354, 'val_loss': 0.6017386317253113}), (454119, {'accuracy': 0.9954681396484375, 'loss': 0.49481987953186035, 'val_accuracy': 0.9944706559181213, 'val_loss': 0.30321046710014343}), (489061, {'accuracy': 0.9294770956039429, 'loss': 0.7650564908981323, 'val_accuracy': 0.9363279938697815, 'val_loss': 0.6472267508506775}), (489061, {'accuracy': 0.950599193572998, 'loss': 0.7105528116226196, 'val_accuracy': 0.9613919258117676, 'val_loss': 0.5903477668762207}), (489061, {'accuracy': 0.9547234177589417, 'loss': 0.692179262638092, 'val_accuracy': 0.9645203351974487, 'val_loss': 0.5684477090835571}), (423074, {'accuracy': 0.9417926669120789, 'loss': 0.7606189846992493, 'val_accuracy': 0.9537535309791565, 'val_loss': 0.651157557964325}), (454119, {'accuracy': 0.9759380221366882, 'loss': 0.6470673680305481, 'val_accuracy': 0.9866621494293213, 'val_loss': 0.5170385837554932}), (489061, {'accuracy': 0.9477958679199219, 'loss': 0.707598865032196, 'val_accuracy': 0.961631178855896, 'val_loss': 0.5826747417449951}), (454118, {'accuracy': 0.9889456033706665, 'loss': 0.5890803337097168, 'val_accuracy': 0.9924690127372742, 'val_loss': 0.4351385235786438})]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Simulation crashed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\simulation\\app.py:308\u001b[0m, in \u001b[0;36mstart_simulation\u001b[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m     hist \u001b[39m=\u001b[39m run_fl(\n\u001b[0;32m    309\u001b[0m         server\u001b[39m=\u001b[39;49minitialized_server,\n\u001b[0;32m    310\u001b[0m         config\u001b[39m=\u001b[39;49minitialized_config,\n\u001b[0;32m    311\u001b[0m     )\n\u001b[0;32m    312\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\app.py:225\u001b[0m, in \u001b[0;36mrun_fl\u001b[1;34m(server, config)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m hist \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39;49mfit(num_rounds\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rounds, timeout\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mround_timeout)\n\u001b[0;32m    226\u001b[0m log(INFO, \u001b[39m\"\u001b[39m\u001b[39mapp_fit: losses_distributed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(hist\u001b[39m.\u001b[39mlosses_distributed))\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\server.py:117\u001b[0m, in \u001b[0;36mServer.fit\u001b[1;34m(self, num_rounds, timeout)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m parameters_prime\n\u001b[1;32m--> 117\u001b[0m     history\u001b[39m.\u001b[39;49madd_metrics_distributed_fit(\n\u001b[0;32m    118\u001b[0m         server_round\u001b[39m=\u001b[39;49mcurrent_round, metrics\u001b[39m=\u001b[39;49mfit_metrics\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m \u001b[39m# Evaluate model using strategy implementation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\server\\history.py:49\u001b[0m, in \u001b[0;36mHistory.add_metrics_distributed_fit\u001b[1;34m(self, server_round, metrics)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m metrics:\n\u001b[0;32m     47\u001b[0m     \u001b[39m# if not (isinstance(metrics[key], float) or isinstance(metrics[key], int)):\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39m#     continue  # ignore non-numeric key/value pairs\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetrics_distributed_fit:\n\u001b[0;32m     50\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics_distributed_fit[key] \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'dict'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmath\u001b[39;00m \u001b[39mimport\u001b[39;00m floor\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m fl\u001b[39m.\u001b[39;49msimulation\u001b[39m.\u001b[39;49mstart_simulation(\n\u001b[0;32m      4\u001b[0m     client_fn\u001b[39m=\u001b[39;49mgenerate_client_fn(),  \u001b[39m# a function that spawns a particular client\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39m# num_clients=cfg.n_clients,  # total number of clients\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m     num_clients\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(client_data),  \u001b[39m# total number of clients\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m     config\u001b[39m=\u001b[39;49mfl\u001b[39m.\u001b[39;49mserver\u001b[39m.\u001b[39;49mServerConfig(\n\u001b[0;32m      8\u001b[0m         num_rounds\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mn_rounds\n\u001b[0;32m      9\u001b[0m         \u001b[39m# num_rounds=5\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m     ),  \u001b[39m# minimal config for the server loop telling the number of rounds in FL\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,  \u001b[39m# our strategy of choice\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m     client_resources\u001b[39m=\u001b[39;49m{\n\u001b[0;32m     13\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_cpus\u001b[39;49m\u001b[39m\"\u001b[39;49m: floor(multiprocessing\u001b[39m.\u001b[39;49mcpu_count() \u001b[39m/\u001b[39;49m \u001b[39mlen\u001b[39;49m(client_data)),\n\u001b[0;32m     14\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_gpus\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m     },\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\centralities-with-fl-nids-main\\.venv\\Lib\\site-packages\\flwr\\simulation\\app.py:344\u001b[0m, in \u001b[0;36mstart_simulation\u001b[1;34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[0m\n\u001b[0;32m    314\u001b[0m     log(ERROR, traceback\u001b[39m.\u001b[39mformat_exc())\n\u001b[0;32m    315\u001b[0m     log(\n\u001b[0;32m    316\u001b[0m         ERROR,\n\u001b[0;32m    317\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYour simulation crashed :(. This could be because of several reasons. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m         client_resources,\n\u001b[0;32m    343\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSimulation crashed.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mex\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     \u001b[39m# Stop time monitoring resources in cluster\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     f_stop\u001b[39m.\u001b[39mset()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Simulation crashed."
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results/{}'.format(dtime)):\n",
    "    os.mkdir('./results/{}'.format(dtime))\n",
    "\n",
    "# if not os.path.isdir('./results/{}'.format(dataset_name)):\n",
    "#     os.mkdir('./results/{}'.format(dataset_name))\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "filename = ('./results/{}/baseline.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL__FL-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "# test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "#test.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    # cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "   # cdata.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - PCA\"\n",
    "results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        # self.model = create_keras_model(input_shape= self.x_train.shape[1], alpha=lr)\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        # log(INFO, f\"==>> config: {config}\")\n",
    "        # log(INFO, f\"==>> float(config[lr]): {lr}\")\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/PCA/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/PCA/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"centralities - PCA\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"centralities - PCA\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    total_examples = 0\n",
    "    federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    for num_examples, m in metrics:\n",
    "        for k, v in m.items():\n",
    "            federated_metrics[k] += num_examples * v\n",
    "        total_examples += num_examples\n",
    "    return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/pca.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - DiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "if cfg.multi_class:\n",
    "    test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "# test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "# test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "test.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    test_by_class = {}\n",
    "    classes = test[dataset.class_col].unique()\n",
    "    for class_value in classes:\n",
    "        test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "        test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "        test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "        test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "        test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "        test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "test_labels = test[dataset.label_col].to_numpy()\n",
    "test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "input_dim = test.shape[1]\n",
    "\n",
    "client_data = []\n",
    "for client_path in clients_paths:\n",
    "    client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "for i in range(len(client_data)):\n",
    "    \n",
    "    cdata = client_data[i]\n",
    "    \n",
    "    if cfg.multi_class:\n",
    "        cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "    # cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "    # cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "    cdata.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "    cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "    cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "    cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "    y_train = c_train[dataset.label_col].to_numpy()\n",
    "    x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    y_test = c_test[dataset.label_col].to_numpy()\n",
    "    x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "    client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - Centralities - DiGraph\"\n",
    "results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(input_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "        self.cid = cid\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        # self.model = create_keras_model(input_shape= self.x_train.shape[1], alpha=lr)\n",
    "        self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "        # log(INFO, f\"==>> config: {config}\")\n",
    "        # log(INFO, f\"==>> float(config[lr]): {lr}\")\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/digraph/client_{}\".format(dtime, self.cid)\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                 epochs=config[\"local_epochs\"],\n",
    "                                 batch_size=config[\"batch_size\"],\n",
    "                                 validation_data=(self.x_test, self.y_test),\n",
    "                                 verbose=0,\n",
    "                                 callbacks=[tensorboard_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn():\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/digraph/server\".format(dtime) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[\"centralities - DiGraph\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[\"centralities - DiGraph\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    total_examples = 0\n",
    "    federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    for num_examples, m in metrics:\n",
    "        for k, v in m.items():\n",
    "            federated_metrics[k] += num_examples * v\n",
    "        total_examples += num_examples\n",
    "    return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    ")  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/digraph.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - MultiDiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_parquet(folder_path + \"test.parquet\")\n",
    "\n",
    "# if cfg.multi_class:\n",
    "#     test[dataset.label_col] = test[dataset.class_num_col]\n",
    "    \n",
    "# test.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "# # test.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "# test.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "# if not cfg.multi_class:\n",
    "#     test_by_class = {}\n",
    "#     classes = test[dataset.class_col].unique()\n",
    "#     for class_value in classes:\n",
    "#         test_class = test[test[dataset.class_col] == class_value].copy()\n",
    "#         test_class.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "#         test_class.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "#         test_class.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#         test_class_labels = test_class[dataset.label_col].to_numpy()\n",
    "#         test_class = test_class.drop([dataset.label_col], axis=1).to_numpy()\n",
    "\n",
    "#         test_by_class[class_value] = (test_class, test_class_labels)\n",
    "    \n",
    "    \n",
    "# test.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "# test.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "# test.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "# test_labels = test[dataset.label_col].to_numpy()\n",
    "# test = test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "# input_dim = test.shape[1]\n",
    "\n",
    "# client_data = []\n",
    "# for client_path in clients_paths:\n",
    "#     client_data.append(pd.read_parquet(client_path))\n",
    "    \n",
    "# for i in range(len(client_data)):\n",
    "    \n",
    "#     cdata = client_data[i]\n",
    "    \n",
    "#     if cfg.multi_class:\n",
    "#         cdata[dataset.label_col] = cdata[dataset.class_num_col]\n",
    "        \n",
    "#     cdata.drop([\"src_degree\", \"dst_degree\", \"src_betweenness\", \"dst_betweenness\", \"src_pagerank\", \"dst_pagerank\"], axis=1, inplace=True)\n",
    "#     # cdata.drop([\"src_multidigraph_degree\", \"dst_multidigraph_degree\", \"src_multidigraph_betweenness\", \"dst_multidigraph_betweenness\", \"src_multidigraph_pagerank\", \"dst_multidigraph_pagerank\"], axis=1, inplace=True)\n",
    "#     cdata.drop([\"pca_1\", \"pca_2\"], axis=1, inplace=True)\n",
    "\n",
    "#     cdata.drop(dataset.drop_columns, axis=1, inplace=True)\n",
    "#     cdata.drop(dataset.weak_columns, axis=1, inplace=True)\n",
    "#     cdata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     c_train, c_test = train_test_split(cdata, test_size=0.1)\n",
    "\n",
    "#     y_train = c_train[dataset.label_col].to_numpy()\n",
    "#     x_train = c_train.drop([dataset.label_col], axis=1).to_numpy()\n",
    "#     y_test = c_test[dataset.label_col].to_numpy()\n",
    "#     x_test = c_test.drop([dataset.label_col], axis=1).to_numpy()\n",
    "    \n",
    "#     client_data[i] = (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}  # a dictionary that will contain all the options and results of models\n",
    "# # add all options to the results dictionary, to know what options selected for obtained results\n",
    "# results[\"configuration\"] = \"2dt - Centralities - MultiDiGraph\"\n",
    "# results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# results[\"multi_class\"] = cfg.multi_class\n",
    "# results[\"learning_rate\"] = learning_rate\n",
    "# results[\"dataset_name\"] = dataset.name\n",
    "# results[\"num_classes\"] = num_classes\n",
    "# results[\"labels_names\"] = labels_names\n",
    "# results[\"input_dim\"] = input_dim\n",
    "\n",
    "# results[\"scores\"] = {}\n",
    "# results[\"scores\"][\"server\"] = {}\n",
    "# results[\"scores\"][\"clients\"] = {}\n",
    "# results[\"scores\"][\"accuracy\"] = {}\n",
    "# results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "# if not cfg.multi_class:\n",
    "#     results[\"scores\"][\"test_by_class\"] = {}\n",
    "#     results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "#     results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "#     for k in test_by_class.keys():\n",
    "#         results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "#         results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "#         results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_keras_model(input_dim)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class FLClient(fl.client.NumPyClient):\n",
    "#     def __init__(self, cid, x_train, y_train, x_test, y_test):\n",
    "#         self.cid = cid\n",
    "#         self.x_train, self.y_train = x_train, y_train\n",
    "#         self.x_test, self.y_test = x_test, y_test\n",
    "#         self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "#     def get_parameters(self, config):\n",
    "#         return self.model.get_weights()\n",
    "\n",
    "#     def set_parameters(self, parameters, config):\n",
    "#         self.model.set_weights(parameters)\n",
    "\n",
    "#     def fit(self, parameters, config):\n",
    "        \n",
    "#         lr=float(config[\"lr\"])\n",
    "#         # self.model = create_keras_model(input_shape= self.x_train.shape[1], alpha=lr)\n",
    "#         self.model = create_keras_model(input_shape=input_dim, alpha=lr)\n",
    "#         # log(INFO, f\"==>> config: {config}\")\n",
    "#         # log(INFO, f\"==>> float(config[lr]): {lr}\")\n",
    "#         self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "#         logdir = \"logs/scalars/{}/multidigraph/client_{}\".format(dtime, self.cid)\n",
    "#         tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#         history = self.model.fit(self.x_train, self.y_train,\n",
    "#                                  epochs=config[\"local_epochs\"],\n",
    "#                                  batch_size=config[\"batch_size\"],\n",
    "#                                  validation_data=(self.x_test, self.y_test),\n",
    "#                                  verbose=0,\n",
    "#                                  callbacks=[tensorboard_callback])\n",
    "\n",
    "#         return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "#     def evaluate(self, parameters, config):\n",
    "#         self.set_parameters(parameters, config)\n",
    "#         loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "#         return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_client_fn():\n",
    "#     def client_fn(cid: str):\n",
    "#         i = int(cid)\n",
    "#         return FLClient(cid, client_data[i][0], client_data[i][1], client_data[i][2], client_data[i][3]).to_client()\n",
    "\n",
    "#     return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "#     def fit_config_fn(server_round: int):\n",
    "#         alpha = learning_rate\n",
    "#         if server_round > 5:\n",
    "#             alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "#         return {\n",
    "#             \"lr\": alpha,\n",
    "#             \"local_epochs\": config.local_epochs,\n",
    "#             \"batch_size\": config.batch_size,\n",
    "#         }\n",
    "\n",
    "#     return fit_config_fn\n",
    "\n",
    "\n",
    "# def get_evaluate_fn(x_test_sever, y_test_server):\n",
    "\n",
    "#     def evaluate_fn(server_round: int, parameters, config):\n",
    "#         # eval_model = model\n",
    "#         eval_model = create_keras_model(input_shape=input_dim)\n",
    "#         eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "#         logdir = \"logs/scalars/{}/multidigraph/server\".format(dtime) \n",
    "#         # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#         tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "#         test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "#                                                   batch_size = cfg.config_fit.batch_size,\n",
    "#                                                   callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "#         y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "#         if cfg.multi_class:\n",
    "#             y_pred = np.argmax(y_pred, axis=1)\n",
    "#             scores = custom_acc_mc(y_test_server, y_pred)\n",
    "#         else:\n",
    "#             y_pred = np.transpose(y_pred)[0]\n",
    "#             y_pred = list(\n",
    "#                 map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "#             scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "#         results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "#         results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "#         results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "#         results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "#         results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "#         results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "#         results_final[\"centralities - MultiDiGraph\"][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "#         results_final[\"centralities - MultiDiGraph\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "#         if not cfg.multi_class:\n",
    "#             for k in test_by_class.keys():\n",
    "#                 y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "#                 y_pred_class = np.transpose(y_pred_class)[0]\n",
    "#                 y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "#                 scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "#                 results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "#                 results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "#         log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "#         return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "#     return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_average(metrics):\n",
    "#     print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "#     total_examples = 0\n",
    "#     federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "#     for num_examples, m in metrics:\n",
    "#         for k, v in m.items():\n",
    "#             federated_metrics[k] += num_examples * v\n",
    "#         total_examples += num_examples\n",
    "#     return {k: v / total_examples for k, v in federated_metrics.items()}\n",
    "\n",
    "# strategy = fl.server.strategy.FedAvg(\n",
    "#     fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "#     min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "#     fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "#     min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "#     min_available_clients=len(client_data),  # total clients in the simulation\n",
    "#     fit_metrics_aggregation_fn = weighted_average,\n",
    "#     # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "#     on_fit_config_fn=get_on_fit_config(\n",
    "#         cfg.config_fit\n",
    "#     ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "#     evaluate_fn=get_evaluate_fn(test, test_labels),\n",
    "# )  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from math import floor\n",
    "# history = fl.simulation.start_simulation(\n",
    "#     client_fn=generate_client_fn(),  # a function that spawns a particular client\n",
    "#     # num_clients=cfg.n_clients,  # total number of clients\n",
    "#     num_clients=len(client_data),  # total number of clients\n",
    "#     config=fl.server.ServerConfig(\n",
    "#         num_rounds=cfg.n_rounds\n",
    "#         # num_rounds=5\n",
    "#     ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "#     strategy=strategy,  # our strategy of choice\n",
    "#     client_resources={\n",
    "#         \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "#         \"num_gpus\": 0.0,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"==>> history: {history}\")\n",
    "# print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = ('./results/{}/multidigraph.json'.format(dtime))\n",
    "# outfile = open(filename, 'w')\n",
    "# outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "# outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "# outfile = open(filename, 'w')\n",
    "# outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "# outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
