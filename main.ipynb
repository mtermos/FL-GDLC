{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from logging import INFO, DEBUG\n",
    "from flwr.common.logger import log\n",
    "\n",
    "from keras import layers, models, Input, regularizers, callbacks, optimizers\n",
    "\n",
    "from src.models.evaluation_metrics import custom_acc_mc, custom_acc_binary\n",
    "from src.data.dataset_info import datasets\n",
    "from src.read_clients import read_clients\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "\n",
    "dataset = datasets[0]\n",
    "\n",
    "folder_path = \"./datasets/gdlc/\"\n",
    "# folder_path = \"./datasets/dbp/\"\n",
    "\n",
    "lr_decay = True\n",
    "early_stopping = False\n",
    "\n",
    "pca = True\n",
    "digraph_centralities = False\n",
    "multi_graph_centralities = False\n",
    "\n",
    "learning_rate = 0.0001\n",
    "LAMBD_1 = 0.0001\n",
    "LAMBD_2 = 0.001\n",
    "\n",
    "dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_paths = [\n",
    "    folder_path + \"client_0.parquet\",\n",
    "    folder_path + \"client_1.parquet\",\n",
    "    folder_path + \"client_2.parquet\",\n",
    "    folder_path + \"client_3.parquet\",\n",
    "    folder_path + \"client_4.parquet\",\n",
    "    folder_path + \"client_5.parquet\",\n",
    "    folder_path + \"client_6.parquet\",\n",
    "    folder_path + \"client_7.parquet\"\n",
    "    # folder_path + \"test.parquet\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + \"added_columns.pkl\", 'rb') as f:\n",
    "    centralities_columns, pca_columns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input dimension of the training set\n",
    "# input_dim = df.shape[1] - len(drop_columns) - len(weak_columns) - 1  # for the label_column\n",
    "  \n",
    "# specifying the number of classes, since it is different from one dataset to another and also if binary or multi-class classification\n",
    "classes_set = {\"benign\", \"attack\"}\n",
    "labels_names = {0: \"benign\", 1: \"attack\"}\n",
    "num_classes = 2\n",
    "if cfg.multi_class:\n",
    "    with open(folder_path + \"labels_names.pkl\", 'rb') as f:\n",
    "        labels_names, classes_set = pickle.load(f)\n",
    "    num_classes = len(classes_set)\n",
    "    \n",
    "labels_names = {int(k): v for k, v in labels_names.items()}\n",
    "\n",
    "print(f\"==>> classes_set: {classes_set}\")\n",
    "print(f\"==>> num_classes: {num_classes}\")\n",
    "print(f\"==>> labels_names: {labels_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(input_shape, alpha = learning_rate):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv1D(80, kernel_size=3,\n",
    "                activation=\"relu\", input_shape=(input_shape, 1), kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    # .L1L2(l1=LAMBD_1, l2=LAMBD_2)\n",
    "    model.add(layers.Conv1D(80, 3, activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    # model.add(layers.LSTM(units=80,\n",
    "    #                         activation='relu',\n",
    "    #                         kernel_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         recurrent_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         bias_regularizer=regularizers.L1L2(l1=LAMBD_1, l2=LAMBD_2),\n",
    "    #                         return_sequences=False,\n",
    "    #                         ))\n",
    "    # model.add(layers.LayerNormalization(axis=1))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(200,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(200,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "    model.add(layers.Dense(80,activation='relu', kernel_regularizer=regularizers.L2(l2=LAMBD_2)))\n",
    "    model.add(layers.LayerNormalization(axis=1))\n",
    "\n",
    "    if cfg.multi_class:\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=alpha),\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model(80)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = {}\n",
    "\n",
    "# results_final[\"model\"] = model.to_json()\n",
    "results_final[\"model\"] = {}\n",
    "results_final[\"configuration\"] = {\n",
    "    \"folder_path\": folder_path,\n",
    "    \"lr_decay\": lr_decay,\n",
    "    \"early_stopping\": early_stopping,\n",
    "    \"pca\": pca,\n",
    "    \"digraph_centralities\": digraph_centralities,\n",
    "    \"multi_graph_centralities\": multi_graph_centralities,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"LAMBD_1\": LAMBD_1,\n",
    "    \"LAMBD_2\": LAMBD_2,\n",
    "    \"cfg\": OmegaConf.to_container(cfg)\n",
    "}\n",
    "\n",
    "results_final[\"baseline\"] = {}\n",
    "results_final[\"baseline\"][\"accuracy\"] = {}\n",
    "results_final[\"baseline\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"PCA\"] = {}\n",
    "results_final[\"PCA\"][\"accuracy\"] = {}\n",
    "results_final[\"PCA\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"digraph\"] = {}\n",
    "results_final[\"digraph\"][\"accuracy\"] = {}\n",
    "results_final[\"digraph\"][\"f1s\"] = {}\n",
    "\n",
    "results_final[\"multidigraph\"] = {}\n",
    "results_final[\"multidigraph\"][\"accuracy\"] = {}\n",
    "results_final[\"multidigraph\"][\"f1s\"] = {}\n",
    "\n",
    "results_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, logdir, x_train, y_train, x_val, y_val, x_test, y_test, input_dim):\n",
    "        self.logdir = logdir\n",
    "        self.x_train, self.y_train = x_train, y_train\n",
    "        self.x_val, self.y_val = x_val, y_val  \n",
    "        self.x_test, self.y_test = x_test, y_test\n",
    "        self.input_dim = input_dim\n",
    "        self.model = create_keras_model(input_shape=input_dim)\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def set_parameters(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \n",
    "        lr=float(config[\"lr\"])\n",
    "        self.model = create_keras_model(input_shape=self.input_dim, alpha=lr)\n",
    "        self.set_parameters(parameters, config)\n",
    "\n",
    "        \n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=self.logdir)\n",
    "        \n",
    "        early_stopping_callback = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        history = self.model.fit(self.x_train, self.y_train,\n",
    "                                epochs=config[\"local_epochs\"],\n",
    "                                batch_size=config[\"batch_size\"],\n",
    "                                validation_data=(self.x_val, self.y_val),  \n",
    "                                verbose=0,\n",
    "                                callbacks=[tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "        return self.get_parameters(config), len(self.x_train), {k: v[-1] for k, v in history.history.items()}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters, config)\n",
    "        loss, accuracy = self.model.evaluate(self.x_test, self.y_test, cfg.config_fit.batch_size, verbose=0)\n",
    "        return loss, len(self.x_test), {\"accuracy\": accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn(data, simulation_name, input_dim):\n",
    "    def client_fn(cid: str):\n",
    "        i = int(cid)\n",
    "        logdir = \"logs/scalars/{}/{}/client_{}\".format(dtime, simulation_name, cid)\n",
    "        return FLClient(\n",
    "            logdir,\n",
    "            data[i][0],  # x_train\n",
    "            data[i][1],  # y_train\n",
    "            data[i][2],  # x_val\n",
    "            data[i][3],  # y_val\n",
    "            data[i][4],  # x_test\n",
    "            data[i][5],   # y_test\n",
    "            input_dim\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_on_fit_config(config: DictConfig):\n",
    "\n",
    "    def fit_config_fn(server_round: int):\n",
    "        alpha = learning_rate\n",
    "        if lr_decay and server_round > 5:\n",
    "            alpha = alpha / (1 + 0.5 * server_round)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"lr\": alpha,\n",
    "            \"local_epochs\": config.local_epochs,\n",
    "            \"batch_size\": config.batch_size,\n",
    "        }\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "\n",
    "def get_evaluate_fn(x_test_sever, y_test_server, input_dim, simulation_name, results, test_by_class):\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        # eval_model = model\n",
    "        eval_model = create_keras_model(input_shape=input_dim)\n",
    "        eval_model.set_weights(parameters)\n",
    "\n",
    "        \n",
    "        logdir = \"logs/scalars/{}/{}/server\".format(dtime, simulation_name) \n",
    "        # logdir = \"logs/scalars/client{}_\".format(config[\"cid\"]) + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "        test_loss, test_acc = eval_model.evaluate(x_test_sever, y_test_server,\n",
    "                                                  batch_size = cfg.config_fit.batch_size,\n",
    "                                                  callbacks=[tensorboard_callback])\n",
    "        \n",
    "        \n",
    "        y_pred = eval_model.predict(x_test_sever, batch_size = cfg.config_fit.batch_size)\n",
    "        \n",
    "        if cfg.multi_class:\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            scores = custom_acc_mc(y_test_server, y_pred)\n",
    "        else:\n",
    "            y_pred = np.transpose(y_pred)[0]\n",
    "            y_pred = list(\n",
    "                map(lambda x: 0 if x < 0.5 else 1, y_pred))\n",
    "            scores = custom_acc_binary(y_test_server, y_pred)\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        \n",
    "        results[\"scores\"][\"accuracy\"][server_round] = test_acc\n",
    "        results[\"scores\"][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        results[\"scores\"][\"server\"][server_round] = scores\n",
    "        \n",
    "        results_final[simulation_name][\"accuracy\"][server_round] = scores[\"accuracy\"]\n",
    "        results_final[simulation_name][\"f1s\"][server_round] = scores[\"f1s\"]\n",
    "        \n",
    "        if not cfg.multi_class:\n",
    "            for k in test_by_class.keys():\n",
    "                y_pred_class = eval_model.predict(test_by_class[k][0], batch_size = cfg.config_fit.batch_size, verbose = 0)\n",
    "                y_pred_class = np.transpose(y_pred_class)[0]\n",
    "                y_pred_class = list(map(lambda x: 0 if x < 0.5 else 1, y_pred_class))\n",
    "                scores_class = custom_acc_binary(test_by_class[k][1], y_pred_class)\n",
    "                results[\"scores\"][\"test_by_class\"][\"accuracy\"][k][server_round] = scores_class[\"accuracy\"]\n",
    "                results[\"scores\"][\"test_by_class\"][\"f1s\"][k][server_round] = scores_class[\"f1s\"]\n",
    "                \n",
    "        log(INFO, f\"==>> scores: {scores}\")\n",
    "        \n",
    "        \n",
    "        return test_loss, {\"accuracy\": test_acc, \"f1s\": scores[\"f1s\"], \"FPR\": scores[\"FPR\"], \"FNR\": scores[\"FNR\"]}\n",
    "\n",
    "    return evaluate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics):\n",
    "    # print(f\"==>> weighted_average: {metrics}\")\n",
    "\n",
    "    return metrics\n",
    "    # total_examples = 0\n",
    "    # federated_metrics = {k: 0 for k in metrics[0][1].keys()}\n",
    "    # for num_examples, m in metrics:\n",
    "    #     for k, v in m.items():\n",
    "    #         federated_metrics[k] += num_examples * v\n",
    "    #     total_examples += num_examples\n",
    "    # return {k: v / total_examples for k, v in federated_metrics.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_name = \"baseline\"\n",
    "\n",
    "client_data, test, test_labels, test_by_class, input_dim = read_clients(\n",
    "    folder_path, clients_paths, dataset.label_col, dataset.class_col, dataset.class_num_col, centralities_columns, pca_columns, dataset.drop_columns, dataset.weak_columns, cfg.multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}  # a dictionary that will contain all the options and results of models\n",
    "# add all options to the results dictionary, to know what options selected for obtained results\n",
    "results[\"configuration\"] = \"2dt - baseline\"\n",
    "results[\"dtime\"] = dtime\n",
    "results[\"multi_class\"] = cfg.multi_class\n",
    "results[\"learning_rate\"] = learning_rate\n",
    "results[\"dataset_name\"] = dataset.name\n",
    "results[\"num_classes\"] = num_classes\n",
    "results[\"labels_names\"] = labels_names\n",
    "results[\"input_dim\"] = input_dim\n",
    "\n",
    "results[\"scores\"] = {}\n",
    "results[\"scores\"][\"server\"] = {}\n",
    "results[\"scores\"][\"clients\"] = {}\n",
    "results[\"scores\"][\"accuracy\"] = {}\n",
    "results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "if not cfg.multi_class:\n",
    "    results[\"scores\"][\"test_by_class\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "    for k in test_by_class.keys():\n",
    "        results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "    min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "    fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "    min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "    min_available_clients=len(client_data),  # total clients in the simulation\n",
    "    # fit_metrics_aggregation_fn = weighted_average,\n",
    "    # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "    on_fit_config_fn=get_on_fit_config(\n",
    "        cfg.config_fit\n",
    "    ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "    evaluate_fn=get_evaluate_fn(test, test_labels, input_dim, simulation_name, results, test_by_class),\n",
    ")  # a function to run on the server side to evaluate the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from math import floor\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=generate_client_fn(client_data, simulation_name, input_dim),  # a function that spawns a particular client\n",
    "    # num_clients=cfg.n_clients,  # total number of clients\n",
    "    num_clients=len(client_data),  # total number of clients\n",
    "    config=fl.server.ServerConfig(\n",
    "        num_rounds=cfg.n_rounds\n",
    "        # num_rounds=5\n",
    "    ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "    strategy=strategy,  # our strategy of choice\n",
    "    client_resources={\n",
    "        # \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "        \"num_cpus\": 1,\n",
    "        \"num_gpus\": 0.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"==>> history: {history}\")\n",
    "print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results'):\n",
    "    os.mkdir('./results')\n",
    "\n",
    "# creating the directories if they don't exist\n",
    "if not os.path.isdir('./results/{}'.format(dtime)):\n",
    "    os.mkdir('./results/{}'.format(dtime))\n",
    "\n",
    "# if not os.path.isdir('./results/{}'.format(dataset_name)):\n",
    "#     os.mkdir('./results/{}'.format(dataset_name))\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "filename = ('./results/{}/baseline.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "outfile = open(filename, 'w')\n",
    "outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    simulation_name = \"PCA\"\n",
    "    client_data, test, test_labels, test_by_class, input_dim = read_clients(\n",
    "        folder_path, clients_paths, dataset.label_col, dataset.class_col, dataset.class_num_col, centralities_columns, None, dataset.drop_columns, dataset.weak_columns, cfg.multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    results = {}  # a dictionary that will contain all the options and results of models\n",
    "    # add all options to the results dictionary, to know what options selected for obtained results\n",
    "    results[\"configuration\"] = \"2dt - PCA\"\n",
    "    results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results[\"multi_class\"] = cfg.multi_class\n",
    "    results[\"learning_rate\"] = learning_rate\n",
    "    results[\"dataset_name\"] = dataset.name\n",
    "    results[\"num_classes\"] = num_classes\n",
    "    results[\"labels_names\"] = labels_names\n",
    "    results[\"input_dim\"] = input_dim\n",
    "\n",
    "    results[\"scores\"] = {}\n",
    "    results[\"scores\"][\"server\"] = {}\n",
    "    results[\"scores\"][\"clients\"] = {}\n",
    "    results[\"scores\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "    if not cfg.multi_class:\n",
    "        results[\"scores\"][\"test_by_class\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "        for k in test_by_class.keys():\n",
    "            results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "            results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "            results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "            \n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "        min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "        fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "        min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "        min_available_clients=len(client_data),  # total clients in the simulation\n",
    "        # fit_metrics_aggregation_fn = weighted_average,\n",
    "        # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "        on_fit_config_fn=get_on_fit_config(\n",
    "            cfg.config_fit\n",
    "        ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "        evaluate_fn=get_evaluate_fn(test, test_labels, input_dim, simulation_name, results, test_by_class),\n",
    "    )  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    import multiprocessing\n",
    "    from math import floor\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=generate_client_fn(client_data, simulation_name, input_dim),  # a function that spawns a particular client\n",
    "        # num_clients=cfg.n_clients,  # total number of clients\n",
    "        num_clients=len(client_data),  # total number of clients\n",
    "        config=fl.server.ServerConfig(\n",
    "            num_rounds=cfg.n_rounds\n",
    "            # num_rounds=5\n",
    "        ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "        strategy=strategy,  # our strategy of choice\n",
    "        client_resources={\n",
    "            # \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "            \"num_cpus\": 1,\n",
    "            \"num_gpus\": 0.0,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    print(f\"==>> history: {history}\")\n",
    "    print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    filename = ('./results/{}/pca.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pca:\n",
    "    filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - DiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    simulation_name = \"digraph\"\n",
    "    client_data, test, test_labels, test_by_class, input_dim = read_clients(\n",
    "        folder_path, clients_paths, dataset.label_col, dataset.class_col, dataset.class_num_col, None, pca_columns, dataset.drop_columns, dataset.weak_columns, cfg.multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    results = {}  # a dictionary that will contain all the options and results of models\n",
    "    # add all options to the results dictionary, to know what options selected for obtained results\n",
    "    results[\"configuration\"] = \"2dt - Centralities - DiGraph\"\n",
    "    results[\"dtime\"] = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    results[\"multi_class\"] = cfg.multi_class\n",
    "    results[\"learning_rate\"] = learning_rate\n",
    "    results[\"dataset_name\"] = dataset.name\n",
    "    results[\"num_classes\"] = num_classes\n",
    "    results[\"labels_names\"] = labels_names\n",
    "    results[\"input_dim\"] = input_dim\n",
    "\n",
    "    results[\"scores\"] = {}\n",
    "    results[\"scores\"][\"server\"] = {}\n",
    "    results[\"scores\"][\"clients\"] = {}\n",
    "    results[\"scores\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "    if not cfg.multi_class:\n",
    "        results[\"scores\"][\"test_by_class\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "        for k in test_by_class.keys():\n",
    "            results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "            results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "            results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "            \n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "        min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "        fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "        min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "        min_available_clients=len(client_data),  # total clients in the simulation\n",
    "        # fit_metrics_aggregation_fn = weighted_average,\n",
    "        # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "        on_fit_config_fn=get_on_fit_config(\n",
    "            cfg.config_fit\n",
    "        ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "        evaluate_fn=get_evaluate_fn(test, test_labels, input_dim, simulation_name, results, test_by_class),\n",
    "    )  # a function to run on the server side to evaluate the global model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    import multiprocessing\n",
    "    from math import floor\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=generate_client_fn(client_data, simulation_name, input_dim),  # a function that spawns a particular client\n",
    "        # num_clients=cfg.n_clients,  # total number of clients\n",
    "        num_clients=len(client_data),  # total number of clients\n",
    "        config=fl.server.ServerConfig(\n",
    "            num_rounds=cfg.n_rounds\n",
    "            # num_rounds=5\n",
    "        ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "        strategy=strategy,  # our strategy of choice\n",
    "        client_resources={\n",
    "            # \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "            \"num_cpus\": 1,\n",
    "            \"num_gpus\": 0.0,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    print(f\"==>> history: {history}\")\n",
    "    print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    filename = ('./results/{}/digraph.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if digraph_centralities:\n",
    "    filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralities - MultiDiGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    simulation_name = \"multidigraph\"\n",
    "    client_data, test, test_labels, test_by_class, input_dim = read_clients(\n",
    "        folder_path, clients_paths, dataset.label_col, dataset.class_col, dataset.class_num_col, centralities_columns, pca_columns, dataset.drop_columns, dataset.weak_columns, cfg.multi_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    results = {}  # a dictionary that will contain all the options and results of models\n",
    "    # add all options to the results dictionary, to know what options selected for obtained results\n",
    "    results[\"configuration\"] = \"2dt - Centralities - MultiDiGraph\"\n",
    "    results[\"dtime\"] = dtime\n",
    "    results[\"multi_class\"] = cfg.multi_class\n",
    "    results[\"learning_rate\"] = learning_rate\n",
    "    results[\"dataset_name\"] = dataset.name\n",
    "    results[\"num_classes\"] = num_classes\n",
    "    results[\"labels_names\"] = labels_names\n",
    "    results[\"input_dim\"] = input_dim\n",
    "\n",
    "    results[\"scores\"] = {}\n",
    "    results[\"scores\"][\"server\"] = {}\n",
    "    results[\"scores\"][\"clients\"] = {}\n",
    "    results[\"scores\"][\"accuracy\"] = {}\n",
    "    results[\"scores\"][\"f1s\"] = {}\n",
    "\n",
    "    if not cfg.multi_class:\n",
    "        results[\"scores\"][\"test_by_class\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"accuracy\"] = {}\n",
    "        results[\"scores\"][\"test_by_class\"][\"f1s\"] = {}\n",
    "        for k in test_by_class.keys():\n",
    "            results[\"scores\"][\"test_by_class\"][\"length\"] = len(test_by_class[k][0])\n",
    "            results[\"scores\"][\"test_by_class\"][\"accuracy\"][k] = {}   \n",
    "            results[\"scores\"][\"test_by_class\"][\"f1s\"][k] = {}    \n",
    "            \n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # in simulation, since all clients are available at all times, we can just use `min_fit_clients` to control exactly how many clients we want to involve during fit\n",
    "        min_fit_clients=len(client_data),  # number of clients to sample for fit()\n",
    "        fraction_evaluate=0.0,  # similar to fraction_fit, we don't need to use this argument.\n",
    "        min_evaluate_clients=0,  # number of clients to sample for evaluate()\n",
    "        min_available_clients=len(client_data),  # total clients in the simulation\n",
    "        # fit_metrics_aggregation_fn = weighted_average,\n",
    "        # evaluate_metrics_aggregation_fn = weighted_average,\n",
    "        on_fit_config_fn=get_on_fit_config(\n",
    "            cfg.config_fit\n",
    "        ),  # a function to execute to obtain the configuration to send to the clients during fit()\n",
    "        evaluate_fn=get_evaluate_fn(test, test_labels, input_dim, simulation_name, results, test_by_class),\n",
    "    )  # a function to run on the server side to evaluate the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=generate_client_fn(client_data, simulation_name, input_dim),  # a function that spawns a particular client\n",
    "        # num_clients=cfg.n_clients,  # total number of clients\n",
    "        num_clients=len(client_data),  # total number of clients\n",
    "        config=fl.server.ServerConfig(\n",
    "            num_rounds=cfg.n_rounds\n",
    "            # num_rounds=5\n",
    "        ),  # minimal config for the server loop telling the number of rounds in FL\n",
    "        strategy=strategy,  # our strategy of choice\n",
    "        client_resources={\n",
    "            # \"num_cpus\": floor(multiprocessing.cpu_count() / len(client_data)),\n",
    "            \"num_cpus\": 1,\n",
    "            \"num_gpus\": 0.0,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    print(f\"==>> history: {history}\")\n",
    "    print(f\"==>> end of history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    filename = ('./results/{}/multidigraph.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_graph_centralities:\n",
    "    filename = ('./results/{}/results_final.json'.format(dtime))\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.writelines(json.dumps(results_final, cls=NumpyEncoder))\n",
    "    outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
